<h1 id="pytorch-doc">PyTorch Doc</h1>

<p><a href="https://pytorch.org/docs/stable/notes/modules.html">source</a></p>

<h2 id="modules">Modules</h2>

<ul>
  <li>read <a href="https://pytorch.org/docs/stable/notes/modules.html#a-simple-custom-module">A Simple Custom Module</a></li>
  <li>“Note that the module itself is callable, and that calling it invokes its <code class="language-plaintext highlighter-rouge">forward()</code> function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
    <ul>
      <li>The <mark>“forward pass”</mark> is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet).</li>
      <li>The <mark>“backward pass”</mark> <strong>computes gradients</strong> of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods.
        <ul>
          <li>PyTorch’s <strong>autograd system</strong> automatically takes care of this backward pass computation, so it is not required to manually implement a <code class="language-plaintext highlighter-rouge">backward()</code> function for each module.”</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="how-does-pytorch-create-a-computational-graph">How does PyTorch create a computational graph?</h1>

<p><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">source</a></p>

<h2 id="tensors">Tensors</h2>

<ul>
  <li>“On it’s own, <code class="language-plaintext highlighter-rouge">Tensor</code> is just like a numpy <code class="language-plaintext highlighter-rouge">ndarray</code>. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the <code class="language-plaintext highlighter-rouge">requires_grad</code> attribute of the <code class="language-plaintext highlighter-rouge">Tensor</code> to True.”</li>
  <li>“<code class="language-plaintext highlighter-rouge">requires_grad</code> is contagious. It means that when a <code class="language-plaintext highlighter-rouge">Tensor</code> is created by operating on other <code class="language-plaintext highlighter-rouge">Tensor</code>s, the <code class="language-plaintext highlighter-rouge">requires_grad</code> of the resultant <code class="language-plaintext highlighter-rouge">Tensor</code> would be set <code class="language-plaintext highlighter-rouge">True</code> given at least one of the tensors used for creation has it’s <code class="language-plaintext highlighter-rouge">requires_grad</code> set to <code class="language-plaintext highlighter-rouge">True</code>.”</li>
  <li>“Each <code class="language-plaintext highlighter-rouge">Tensor</code> has […] an attribute called <code class="language-plaintext highlighter-rouge">grad_fn</code>, which refers to the mathematical operator that creates the variable [d.h. zB., wenn die Variable <code class="language-plaintext highlighter-rouge">d</code> über <code class="language-plaintext highlighter-rouge">d = w3*b + w4*c</code> definiert ist, dann ist das <code class="language-plaintext highlighter-rouge">grad_fn</code> von <code class="language-plaintext highlighter-rouge">d</code> der Additionsoperator <code class="language-plaintext highlighter-rouge">+</code>]. If <code class="language-plaintext highlighter-rouge">requires_grad</code> is set to False, <code class="language-plaintext highlighter-rouge">grad_fn</code> would be <code class="language-plaintext highlighter-rouge">None</code>.” (kann man mit <code class="language-plaintext highlighter-rouge">print("The grad fn for a is", a.grad_fn)</code> testen!) (lies das nochmal genauer im Post!)</li>
  <li>“One can use the member function <code class="language-plaintext highlighter-rouge">is_leaf</code> to determine whether a variable is a leaf <code class="language-plaintext highlighter-rouge">Tensor</code> or not.”</li>
</ul>

<h2 id="torchnnautogradfunction-class"><code class="language-plaintext highlighter-rouge">torch.nn.Autograd.Function</code> class</h2>

<ul>
  <li>“This class has two important member functions we need to look at.”:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">forward</code>
        <ul>
          <li>“simply computes the output using it’s inputs”</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">backward</code>
        <ul>
          <li>“takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function $f$ is basically the <strong>gradient that is backpropagated to $f$ from the layers in front of it</strong> multiplied by <strong>the local gradient of the output of f with respect to it’s inputs</strong>. This is exactly what the <code class="language-plaintext highlighter-rouge">backward</code> function does.” (lies das nochmal genauer nach!)
            <ul>
              <li>Let’s again understand with our example of \(d = f(w_3b , w_4c)\)
                <ol>
                  <li><em>d</em> is our <code class="language-plaintext highlighter-rouge">Tensor</code> here. It’s <code class="language-plaintext highlighter-rouge">grad_fn</code>  is <code class="language-plaintext highlighter-rouge">&lt;ThAddBackward&gt;</code><em>.</em> This is basically the addition operation since the function that creates <em>d</em> adds inputs.</li>
                  <li>The <code class="language-plaintext highlighter-rouge">forward</code> function of the it’s <code class="language-plaintext highlighter-rouge">grad_fn</code>  receives the inputs $w_3b$ <em>and</em> $w_4c$ and adds them. This value is basically stored in the <em>d</em></li>
                  <li>The <code class="language-plaintext highlighter-rouge">backward</code> function of the <code class="language-plaintext highlighter-rouge">&lt;ThAddBackward&gt;</code>  basically takes the the <strong>incoming gradient</strong> from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from <em>L</em> to <em>d.</em> This gradient is also the gradient of <em>L</em> w.r.t to <em>d</em> and is stored in <code class="language-plaintext highlighter-rouge">grad</code>  attribute of the <code class="language-plaintext highlighter-rouge">d</code>. It can be accessed by calling <code class="language-plaintext highlighter-rouge">d.grad</code><em>.</em></li>
                  <li>It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$ and $\frac{\partial{d}}{\partial{w_3b}}$.</li>
                  <li>Then the backward function multiplies the incoming gradient with the <strong>locally computed gradients</strong> respectively and “<em><strong>sends</strong></em>” the gradients to it’s inputs by invoking the backward method of the <code class="language-plaintext highlighter-rouge">grad_fn</code> of their inputs.</li>
                  <li>For example, the <code class="language-plaintext highlighter-rouge">backward</code> function of  <code class="language-plaintext highlighter-rouge">&lt;ThAddBackward&gt;</code>  associated with <em>d</em> invokes backward function of the <code class="language-plaintext highlighter-rouge">grad_fn</code> of the $w_4*c$ (Here, $w_4*c$ is a intermediate Tensor, and it’s <code class="language-plaintext highlighter-rouge">grad_fn</code> is <code class="language-plaintext highlighter-rouge">&lt;ThMulBackward&gt;</code>. At time of invocation of the <code class="language-plaintext highlighter-rouge">backward</code> function, the gradient $\frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{w_4c}} $ is passed as the input.</li>
                  <li>Now, for the variable $w_4*c$, $\frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{w_4c}} $ becomes the incoming gradient, like $\frac{\partial{L}}{\partial{d}} $ was for $d$ in step 3 and the process repeats.</li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="how-are-pytorchs-graphs-different-from-tensorflow-graphs">How are PyTorch’s graphs different from TensorFlow graphs</h2>

<ul>
  <li>PyTorch creates something called a <strong>Dynamic Computation Graph</strong>, which means that the graph is generated on the fly.
    <ul>
      <li>in contrast to the <strong>Static Computation Graphs</strong> used by TensorFlow where the graph is declared <strong>before</strong> running the program</li>
    </ul>
  </li>
  <li>
    <p>Until the <code class="language-plaintext highlighter-rouge">forward</code> function of a Variable is called, there exists no node for the <code class="language-plaintext highlighter-rouge">Tensor</code> (it’s <code class="language-plaintext highlighter-rouge">grad_fn</code>) in the graph.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ```python
      a = torch.randn((3,3), requires_grad = True)   #No graph yet, as a is a leaf
        
      w1 = torch.randn((3,3), requires_grad = True)  #Same logic as above
        
      b = w1*a   #Graph with node `mulBackward` is created.
  ```    
</code></pre></div>    </div>
  </li>
  <li>
    <p>The graph is created as a result of <code class="language-plaintext highlighter-rouge">forward</code> function of many <em>Tensors</em> being invoked. Only then, the buffers for the non-leaf nodes are allocated for the graph and intermediate values (used for computing gradients later). When you call <code class="language-plaintext highlighter-rouge">backward</code>, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is <em>destroyed</em> (In a sense, you can't backpropagate through it, since the buffers holding values to compute the gradients are gone).</p>
  </li>
  <li>
    <p>Next time, you will call <code class="language-plaintext highlighter-rouge">forward</code> on the same set of tensors, <strong>the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.</strong></p>
  </li>
  <li>lies den Abschnitt im Post !</li>
</ul>

