<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>PyTorch - Pharath Palesuvaran</title>
<meta name="description" content="Notes on PyTorch.">


  <meta name="author" content="Pharath Palesuvaran">
  
  <meta property="article:author" content="Pharath Palesuvaran">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Pharath Palesuvaran">
<meta property="og:title" content="PyTorch">
<meta property="og:url" content="http://localhost:4000/pytorch/machine_learning/notes-pytorch/">


  <meta property="og:description" content="Notes on PyTorch.">



  <meta property="og:image" content="http://localhost:4000/assets/images/lenet.png">





  <meta property="article:published_time" content="2021-12-20T00:00:00+01:00">



  <meta property="article:modified_time" content="2021-09-23T22:00:52+02:00">




<link rel="canonical" href="http://localhost:4000/pytorch/machine_learning/notes-pytorch/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pharath Palesuvaran Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="apple-touch-icon" sizes="180x180" href="https://raw.githubusercontent.com/pharath/home/master/assets/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://raw.githubusercontent.com/pharath/home/master/assets/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://raw.githubusercontent.com/pharath/home/master/assets/favicons/favicon-16x16.png">
<link rel="manifest" href="https://raw.githubusercontent.com/pharath/home/master/assets/favicons/site.webmanifest">
<link rel="shortcut icon" href="https://raw.githubusercontent.com/pharath/home/master/assets/favicons/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="https://raw.githubusercontent.com/pharath/home/master/assets/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\(","\\)"] ],
    },
		TeX: {
    	Macros: {
				bra: ["\\langle{#1}|", 1],
				ket: ["|{#1}\\rangle", 1],
        braket: ["\\langle{#1}\\rangle", 1],
				bk: ["\\langle{#1}|{#2}|{#3}\\rangle", 3]
    	}
  	}
	});

</script>
<!--
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML' async></script>
-->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/pytorch-logo.png" alt="Pharath Palesuvaran"></a>
        
        <a class="site-title" href="/">
          Pharath Palesuvaran
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button" accesskey="s">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(0, 0, 0, 0.6), rgba(0, 0, 0, 0.6)), url('/assets/images/lenet.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          PyTorch

        
      </h1>
      
        <p class="page__lead">Notes on PyTorch.
</p>
      
      

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2021-12-20T00:00:00+01:00">December 20, 2021</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


      
      
        <p>
        
          <a href="" class="btn btn--light-outline btn--large">Some Content</a>
        
      
    </div>
  
  
    <span class="page__hero-caption">Photo credit: <a href="http://yann.lecun.com/"><strong>Yann LeCun</strong></a>
</span>
  
</div>







<div id="main" role="main">
  


  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="PyTorch">
    <meta itemprop="description" content="Notes on PyTorch.">
    <meta itemprop="datePublished" content="2021-12-20T00:00:00+01:00">
    <meta itemprop="dateModified" content="2021-09-23T22:00:52+02:00">

    <div class="page__inner-wrap">
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right ">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Contents</h4></header>
              <ul class="toc__menu"><li><a href="#pytorch-doc">PyTorch Doc</a><ul><li><a href="#modules">Modules</a></li></ul></li><li><a href="#how-does-pytorch-create-a-computational-graph">How does PyTorch create a computational graph?</a><ul><li><a href="#tensors">Tensors</a></li><li><a href="#torchnnautogradfunction-class">torch.nn.Autograd.Function class</a></li><li><a href="#how-are-pytorchs-graphs-different-from-tensorflow-graphs">How are PyTorch’s graphs different from TensorFlow graphs</a></li></ul></li><li><a href="#weight-files">Weight files</a></li></ul>

            </nav>
          </aside>
        
        <h1 id="pytorch-doc">PyTorch Doc</h1>

<p><a href="https://pytorch.org/docs/stable/notes/modules.html">source</a></p>

<h2 id="modules">Modules</h2>

<ul>
  <li>read <a href="https://pytorch.org/docs/stable/notes/modules.html#a-simple-custom-module">A Simple Custom Module</a></li>
  <li>“Note that the module itself is callable, and that calling it invokes its <code class="language-plaintext highlighter-rouge">forward()</code> function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
    <ul>
      <li>The <mark>“forward pass”</mark> is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet).</li>
      <li>The <mark>“backward pass”</mark> <strong>computes gradients</strong> of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods.
        <ul>
          <li>PyTorch’s <strong>autograd system</strong> automatically takes care of this backward pass computation, so it is not required to manually implement a <code class="language-plaintext highlighter-rouge">backward()</code> function for each module.”</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="how-does-pytorch-create-a-computational-graph">How does PyTorch create a computational graph?</h1>

<p><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">source</a></p>

<h2 id="tensors">Tensors</h2>

<ul>
  <li>“On it’s own, <code class="language-plaintext highlighter-rouge">Tensor</code> is just like a numpy <code class="language-plaintext highlighter-rouge">ndarray</code>. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the <code class="language-plaintext highlighter-rouge">requires_grad</code> attribute of the <code class="language-plaintext highlighter-rouge">Tensor</code> to True.”</li>
  <li>“<code class="language-plaintext highlighter-rouge">requires_grad</code> is contagious. It means that when a <code class="language-plaintext highlighter-rouge">Tensor</code> is created by operating on other <code class="language-plaintext highlighter-rouge">Tensor</code>s, the <code class="language-plaintext highlighter-rouge">requires_grad</code> of the resultant <code class="language-plaintext highlighter-rouge">Tensor</code> would be set <code class="language-plaintext highlighter-rouge">True</code> given at least one of the tensors used for creation has it’s <code class="language-plaintext highlighter-rouge">requires_grad</code> set to <code class="language-plaintext highlighter-rouge">True</code>.”</li>
  <li>“Each <code class="language-plaintext highlighter-rouge">Tensor</code> has […] an attribute called <code class="language-plaintext highlighter-rouge">grad_fn</code>, which refers to the mathematical operator that creates the variable [d.h. zB., wenn die Variable <code class="language-plaintext highlighter-rouge">d</code> über <code class="language-plaintext highlighter-rouge">d = w3*b + w4*c</code> definiert ist, dann ist das <code class="language-plaintext highlighter-rouge">grad_fn</code> von <code class="language-plaintext highlighter-rouge">d</code> der Additionsoperator <code class="language-plaintext highlighter-rouge">+</code>]. If <code class="language-plaintext highlighter-rouge">requires_grad</code> is set to False, <code class="language-plaintext highlighter-rouge">grad_fn</code> would be <code class="language-plaintext highlighter-rouge">None</code>.” (kann man mit <code class="language-plaintext highlighter-rouge">print("The grad fn for a is", a.grad_fn)</code> testen!) (lies das nochmal genauer im Post!)</li>
  <li>“One can use the member function <code class="language-plaintext highlighter-rouge">is_leaf</code> to determine whether a variable is a leaf <code class="language-plaintext highlighter-rouge">Tensor</code> or not.”</li>
</ul>

<h2 id="torchnnautogradfunction-class"><code class="language-plaintext highlighter-rouge">torch.nn.Autograd.Function</code> class</h2>

<ul>
  <li>“This class has two important member functions we need to look at.”:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">forward</code>
        <ul>
          <li>“simply computes the output using it’s inputs”</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">backward</code>
        <ul>
          <li>“takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function $f$ is basically the <strong>gradient that is backpropagated to $f$ from the layers in front of it</strong> multiplied by <strong>the local gradient of the output of f with respect to it’s inputs</strong>. This is exactly what the <code class="language-plaintext highlighter-rouge">backward</code> function does.” (lies das nochmal genauer nach!)
            <ul>
              <li>Let’s again understand with our example of \(d = f(w_3b , w_4c)\)
                <ol>
                  <li><em>d</em> is our <code class="language-plaintext highlighter-rouge">Tensor</code> here. It’s <code class="language-plaintext highlighter-rouge">grad_fn</code>  is <code class="language-plaintext highlighter-rouge">&lt;ThAddBackward&gt;</code><em>.</em> This is basically the addition operation since the function that creates <em>d</em> adds inputs.</li>
                  <li>The <code class="language-plaintext highlighter-rouge">forward</code> function of the it’s <code class="language-plaintext highlighter-rouge">grad_fn</code>  receives the inputs $w_3b$ <em>and</em> $w_4c$ and adds them. This value is basically stored in the <em>d</em></li>
                  <li>The <code class="language-plaintext highlighter-rouge">backward</code> function of the <code class="language-plaintext highlighter-rouge">&lt;ThAddBackward&gt;</code>  basically takes the the <strong>incoming gradient</strong> from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from <em>L</em> to <em>d.</em> This gradient is also the gradient of <em>L</em> w.r.t to <em>d</em> and is stored in <code class="language-plaintext highlighter-rouge">grad</code>  attribute of the <code class="language-plaintext highlighter-rouge">d</code>. It can be accessed by calling <code class="language-plaintext highlighter-rouge">d.grad</code><em>.</em></li>
                  <li>It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$ and $\frac{\partial{d}}{\partial{w_3b}}$.</li>
                  <li>Then the backward function multiplies the incoming gradient with the <strong>locally computed gradients</strong> respectively and “<em><strong>sends</strong></em>” the gradients to it’s inputs by invoking the backward method of the <code class="language-plaintext highlighter-rouge">grad_fn</code> of their inputs.</li>
                  <li>For example, the <code class="language-plaintext highlighter-rouge">backward</code> function of  <code class="language-plaintext highlighter-rouge">&lt;ThAddBackward&gt;</code>  associated with <em>d</em> invokes backward function of the <code class="language-plaintext highlighter-rouge">grad_fn</code> of the $w_4*c$ (Here, $w_4*c$ is a intermediate Tensor, and it’s <code class="language-plaintext highlighter-rouge">grad_fn</code> is <code class="language-plaintext highlighter-rouge">&lt;ThMulBackward&gt;</code>. At time of invocation of the <code class="language-plaintext highlighter-rouge">backward</code> function, the gradient $\frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{w_4c}} $ is passed as the input.</li>
                  <li>Now, for the variable $w_4*c$, $\frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{w_4c}} $ becomes the incoming gradient, like $\frac{\partial{L}}{\partial{d}} $ was for $d$ in step 3 and the process repeats.</li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="how-are-pytorchs-graphs-different-from-tensorflow-graphs">How are PyTorch’s graphs different from TensorFlow graphs</h2>

<ul>
  <li>PyTorch creates something called a <strong>Dynamic Computation Graph</strong>, which means that the graph is generated on the fly.
    <ul>
      <li>in contrast to the <strong>Static Computation Graphs</strong> used by TensorFlow where the graph is declared <strong>before</strong> running the program</li>
    </ul>
  </li>
  <li>
    <p>Until the <code class="language-plaintext highlighter-rouge">forward</code> function of a Variable is called, there exists no node for the <code class="language-plaintext highlighter-rouge">Tensor</code> (it’s <code class="language-plaintext highlighter-rouge">grad_fn</code>) in the graph.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ```python
      a = torch.randn((3,3), requires_grad = True)   #No graph yet, as a is a leaf
        
      w1 = torch.randn((3,3), requires_grad = True)  #Same logic as above
        
      b = w1*a   #Graph with node `mulBackward` is created.
  ```    
</code></pre></div>    </div>
  </li>
  <li>
    <p>The graph is created as a result of <code class="language-plaintext highlighter-rouge">forward</code> function of many <em>Tensors</em> being invoked. Only then, the buffers for the non-leaf nodes are allocated for the graph and intermediate values (used for computing gradients later). When you call <code class="language-plaintext highlighter-rouge">backward</code>, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is <em>destroyed</em> (In a sense, you can't backpropagate through it, since the buffers holding values to compute the gradients are gone).</p>
  </li>
  <li>
    <p>Next time, you will call <code class="language-plaintext highlighter-rouge">forward</code> on the same set of tensors, <strong>the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.</strong></p>
  </li>
  <li>lies den Abschnitt im Post !</li>
</ul>

<h1 id="weight-files">Weight files</h1>

<ul>
  <li><a href="https://stackoverflow.com/questions/59095824/what-is-the-difference-between-pt-pth-and-pwf-extentions-in-pytorch">source</a>:
    <ul>
      <li>There are no differences between the extensions that were listed: <code class="language-plaintext highlighter-rouge">.pt</code>, <code class="language-plaintext highlighter-rouge">.pth</code>, <code class="language-plaintext highlighter-rouge">.pwf</code>. One can use whatever extension (s)he wants. So, if you’re using <code class="language-plaintext highlighter-rouge">torch.save()</code> for saving models, then it by default uses python pickle (<code class="language-plaintext highlighter-rouge">pickle_module=pickle</code>) to save the objects and some metadata. Thus, you have the liberty to choose the extension you want, as long as it doesn’t cause collisions with any other standardized extensions.</li>
      <li>Having said that, it is however not recommended to use <code class="language-plaintext highlighter-rouge">.pth</code> extension when checkpointing models because it collides with Python path (<code class="language-plaintext highlighter-rouge">.pth</code>) configuration files. Because of this, I myself use <code class="language-plaintext highlighter-rouge">.pth.tar</code> or <code class="language-plaintext highlighter-rouge">.pt</code> but not <code class="language-plaintext highlighter-rouge">.pth</code>, or any other extensions.</li>
    </ul>
  </li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ml" class="page__taxonomy-item p-category" rel="tag">ml</a><span class="sep">, </span>
    
      <a href="/tags/#pytorch" class="page__taxonomy-item p-category" rel="tag">pytorch</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#machine-learning" class="page__taxonomy-item p-category" rel="tag">Machine_Learning</a><span class="sep">, </span>
    
      <a href="/categories/#pytorch" class="page__taxonomy-item p-category" rel="tag">PyTorch</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2021-09-23">September 23, 2021</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=PyTorch%20http%3A%2F%2Flocalhost%3A4000%2Fpytorch%2Fmachine_learning%2Fnotes-pytorch%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fpytorch%2Fmachine_learning%2Fnotes-pytorch%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fpytorch%2Fmachine_learning%2Fnotes-pytorch%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/lecture_notes/machine_learning/notes-computational-graphs/" class="pagination--pager" title="Computational Graphs in PyTorch
">Previous</a>
    
    
      <a href="/bachelor_thesis/link-Bachelor-Thesis/" class="pagination--pager" title="Triaxiale Schwarzschild-Modelle für elliptische Galaxien und ihre Anwendung auf NGC 4365
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You May Also Enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/mario-question.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cheatsheet/cheatsheet-markdown/" rel="permalink">Markdown Syntax Cheatsheet
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-11-28T00:00:00+01:00">November 28, 2022</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Latex

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/mario-question-block.jpeg" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/notes/notes-html/" rel="permalink">HTML Notes
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-10-10T00:00:00+02:00">October 10, 2022</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">For learning html.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/Cpp_logo.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/notes/notes-cpp/" rel="permalink">C++ Notes
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-10-06T00:00:00+02:00">October 6, 2022</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">For learning C++
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/C_logo.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cheatsheet/cheatsheet-android/" rel="permalink">Android Cheatsheet
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-10-04T00:00:00+02:00">October 4, 2022</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Useful tips for using Android
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/pharath_one" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/pharath" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="/_pages/404.md" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Pharath Palesuvaran. </div>


      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>







  </body>
</html>
