<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-15T20:54:13+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pharath Palesuvaran</title><subtitle>Deep Learning and Self-Driving Cars.</subtitle><author><name>Pharath Palesuvaran</name></author><entry><title type="html">Machine Learning (Part 2) [OLD VERSION]</title><link href="http://localhost:4000/lecture_notes/machine_learning/lecture-notes-ML-part2-old/" rel="alternate" type="text/html" title="Machine Learning (Part 2) [OLD VERSION]" /><published>2022-03-09T00:00:00+01:00</published><updated>2022-03-09T00:00:00+01:00</updated><id>http://localhost:4000/lecture_notes/machine_learning/lecture-notes-ML-part2-old</id><content type="html" xml:base="http://localhost:4000/lecture_notes/machine_learning/lecture-notes-ML-part2-old/">&lt;h1 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h1&gt;

&lt;h2 id=&quot;perceptrons-rosenblatt-1962&quot;&gt;Perceptrons (Rosenblatt 1962)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;perceptrons are &lt;strong&gt;generalized linear models&lt;/strong&gt; (“generalized” because of the activation function)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BUT&lt;/strong&gt;: Deep Neural Networks are &lt;strong&gt;nonlinear parametric models&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;more specifically: perceptrons are &lt;strong&gt;generalized linear discriminants&lt;/strong&gt; (because they map the input &lt;strong&gt;x&lt;/strong&gt; directly to a class label t in {-1,+1} [see above: “Linear models for classification”: approach 1.])&lt;/li&gt;
  &lt;li&gt;original version:
    &lt;ul&gt;
      &lt;li&gt;2-class linear discriminant&lt;/li&gt;
      &lt;li&gt;with fixed [i.e. not learned!] nonlinear transformation $\vec{\phi}(\pmb{x})$&lt;/li&gt;
      &lt;li&gt;activation function: step function&lt;/li&gt;
      &lt;li&gt;learned via minimization of “&lt;strong&gt;perceptron criterion&lt;/strong&gt;” $\Rightarrow$ SGD&lt;/li&gt;
      &lt;li&gt;exact solution guaranteed for linearly separable data set (&lt;strong&gt;Perceptron Convergence Theorem&lt;/strong&gt;)
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;BUT:&lt;/strong&gt; in practice, convergence can be slow
            &lt;ul&gt;
              &lt;li&gt;it’s hard to decide, if a problem is not linearly separable or just slowly converging!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input layer&lt;/strong&gt; is a layer, it’s not wrong to say that. &lt;a href=&quot;https://datascience.stackexchange.com/a/14033/115254&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, when calculating the &lt;strong&gt;depth&lt;/strong&gt; of a deep neural network, we only consider the layers that have tunable weights. &lt;a href=&quot;https://datascience.stackexchange.com/a/14033/115254&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automatic-differentiation&quot;&gt;Automatic Differentiation&lt;/h2&gt;

&lt;h3 id=&quot;forward-mode-vs-reverse-mode-differentiation&quot;&gt;Forward-mode vs Reverse-mode differentiation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;read &lt;a href=&quot;https://colah.github.io/posts/2015-08-Backprop/&quot;&gt;Olah&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Forward-mode differentiation&lt;/strong&gt; starts at an input to the graph and moves towards the end. At every node, it sums all the paths feeding in. Each of those paths represents one way in which the input affects that node. By adding them up, we get the total way in which the node is affected by the input, it’s derivative. […]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Reverse-mode differentiation&lt;/strong&gt;, on the other hand, starts at an output of the graph and moves towards the beginning. At each node, it merges all paths which originated at that node. […]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;When I say that reverse-mode differentiation gives us the derivative of e with respect to every node, I really do mean &lt;strong&gt;every node&lt;/strong&gt;. We get both $\frac{\partial e}{\partial a}$ and $\frac{\partial e}{\partial b}$, the derivatives of $e$ with respect to both inputs. Forward-mode differentiation gave us the derivative of our output with respect to a single input, but reverse-mode differentiation gives us all of them. […]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;When training neural networks, we think of the cost (a value describing how bad a neural network performs) as a function of the parameters (numbers describing how the network behaves). We want to calculate the derivatives of the &lt;strong&gt;cost with respect to all the parameters&lt;/strong&gt;, for use in gradient descent. Now, there’s often millions, or even tens of millions of parameters in a neural network. So, &lt;strong&gt;reverse-mode differentiation, &lt;mark&gt;called&lt;/mark&gt; backpropagation&lt;/strong&gt; [&lt;a href=&quot;#reverse_mode_accumulation&quot;&gt;more precise: reverse_mode_accumulation&lt;/a&gt;] in the context of neural networks, gives us a massive speed up!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(Are there any cases &lt;strong&gt;where forward-mode differentiation makes more sense&lt;/strong&gt;? Yes, there are! Where the reverse-mode gives the derivatives of one output with respect to all inputs, the forward-mode gives us the derivatives of all outputs with respect to one input. If one has a function with lots of outputs, forward-mode differentiation can be much, much, much faster.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;both are algorithms for efficiently computing the sum by factoring the paths. Instead of summing over all of the paths explicitly, they compute the same sum more efficiently by &lt;mark&gt;**merging paths back together at every node**&lt;/mark&gt;. In fact, &lt;strong&gt;both&lt;/strong&gt; algorithms touch each edge exactly once!
    &lt;ul&gt;
      &lt;li&gt;At each node, reverse-mode differentiation merges all paths which &lt;strong&gt;originated&lt;/strong&gt; at that node (starting at an output of the graph and moving towards the beginning)&lt;/li&gt;
      &lt;li&gt;At each node, forward-mode differentiation sums all the paths &lt;strong&gt;feeding into&lt;/strong&gt; that node (starting at the beginning and moving towards an output of the graph)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;forward-mode: apply operator $\frac{\partial}{\partial X}$&lt;/li&gt;
  &lt;li&gt;reverse-mode: apply operator $\frac{\partial Z}{\partial}$&lt;/li&gt;
  &lt;li&gt;if we have e.g. a hundred inputs, but only one output, reverse-mode differentiation gives a speed up in $\mathcal{O}(\text{# Inputs})$ compared to forward-mode differentiation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-autograd&quot;&gt;PyTorch autograd&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/pytorch_with_examples.html&quot;&gt;source: Justin Johnson&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the above examples, we had to &lt;strong&gt;manually&lt;/strong&gt; implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer (?: siehe Stichpunkt) network, but can quickly get very hairy for large complex networks.
    &lt;ul&gt;
      &lt;li&gt;?: Why “two-layer”:
        &lt;ul&gt;
          &lt;li&gt;The previous polynomial regression examples correspond to a &lt;strong&gt;single&lt;/strong&gt; layer perceptron with a fixed nonlinear transformation of the inputs (here: using polynomial basis functions), so why does Johnson say &lt;strong&gt;two&lt;/strong&gt;-layer perceptron?
            &lt;ul&gt;
              &lt;li&gt;What Johnson probably means here is that, basically, implementing backprop &lt;strong&gt;manually&lt;/strong&gt; (like in the previous polynomial regression examples) for a two-layer NN would be possible without autograd. This “two-layer network”, however, does not refer to the previous polynomial regression models!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt; computes &lt;strong&gt;all&lt;/strong&gt; gradients with only one line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;in polynomial regression example &lt;strong&gt;without&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt;:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;grad_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;grad_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;the same &lt;strong&gt;with&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt;:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;where all parameter tensors must have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad = True&lt;/code&gt; (otherwise &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt; does not know wrt which parameters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt; must be differentiated).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Thankfully, we can use &lt;strong&gt;automatic differentiation&lt;/strong&gt; to automate the computation of backward passes in neural networks. The &lt;strong&gt;autograd&lt;/strong&gt; package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a &lt;strong&gt;computational graph&lt;/strong&gt;; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.
    &lt;ul&gt;
      &lt;li&gt;auf Folie:
        &lt;ol&gt;
          &lt;li&gt;Convert NN to a computational graph
            &lt;ul&gt;
              &lt;li&gt;explanations:
                &lt;ul&gt;
                  &lt;li&gt;&lt;a href=&quot;https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/&quot;&gt;PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd&lt;/a&gt;
                    &lt;ul&gt;
                      &lt;li&gt;&lt;a href=&quot;/pytorch/machine_learning/notes-pytorch/#how-does-pytorch-create-a-computational-graph&quot;&gt;important points from this blog post&lt;/a&gt;&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/computational-graphs-in-pytorch-and-tensorflow-c25cc40bdcd1&quot;&gt;Computational graphs in PyTorch and TensorFlow&lt;/a&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Each new layer/module specifies how it affects the forward and backward passes
            &lt;ul&gt;
              &lt;li&gt;auf nächster Folie: “Each module is defined by
                &lt;ul&gt;
                  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module.fprop(&lt;/code&gt;$x$&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;)&lt;/code&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module.bprop(&lt;/code&gt;$\frac{\partial E}{\partial y}$&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;)&lt;/code&gt;
                    &lt;ul&gt;
                      &lt;li&gt;computes the gradients of the cost wrt. the inputs $x$ given the gradient wrt. the outputs $y$&lt;/li&gt;
                      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module.bprop()&lt;/code&gt; ist in PyTorch wegen dem Autograd System nicht notwendig (vgl. &lt;a href=&quot;/pytorch/machine_learning/notes-pytorch/#modules&quot;&gt;aus PyTorch Doc&lt;/a&gt;)&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Linear&lt;/code&gt; specifies that it will apply a linear transformation $y=xA^T+b$ to the incoming data during the forward pass (each module has a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward()&lt;/code&gt; method, see e.g. &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear&quot;&gt;source nn.Linear&lt;/a&gt;)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Apply reverse-mode differentiation
            &lt;ul&gt;
              &lt;li&gt;i.e. call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; is a Tensor that has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.requires_grad=True&lt;/code&gt; then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.grad&lt;/code&gt; is another Tensor holding the gradient of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; with respect to some scalar value.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# -*- coding: utf-8 -*-
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# Create Tensors to hold input and outputs.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For this example, the output y is a linear function of (x, x^2, x^3), so
# we can consider it as a linear layer neural network. Let's prepare the
# tensor (x, x^2, x^3).
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape
# (3,), for this case, broadcasting semantics will apply to obtain a tensor
# of shape (2000, 3)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Use the nn package to define our model as a sequence of layers. nn.Sequential
# is a Module which contains other Modules, and applies them in sequence to
# produce its output. The Linear Module computes output from input using a
# linear function, and holds internal Tensors for its weight and bias.
# The Flatten layer flatens the output of the linear layer to a 1D tensor,
# to match the shape of `y`.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The nn package also contains definitions of popular loss functions; in this
# case we will use Mean Squared Error (MSE) as our loss function.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MSELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Forward pass: compute predicted y by passing x to the model. Module objects
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# override the __call__ operator so you can call them like functions. When
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# doing so you pass a Tensor of input data to the Module and it produces
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# a Tensor of output data.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Compute and print loss. We pass Tensors containing the predicted and true
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# values of y, and the loss function returns a Tensor containing the
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# loss.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Zero the gradients before running the backward pass.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Backward pass: compute gradient of the loss with respect to all the learnable
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# parameters of the model. Internally, the parameters of each Module are stored
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# in Tensors with requires_grad=True, so this call will compute gradients for
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# all learnable parameters in the model.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Update the weights using gradient descent. Each parameter is a Tensor, so
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# we can access its gradients like we did before.
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# You can access the first layer of `model` like accessing the first item of a list
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# For linear layer, its parameters are stored as `weight` and `bias`.
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Result: y = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; + &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; x + &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; x^2 + &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; x^3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;forward-propagation&quot;&gt;Forward Propagation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;inputs:
    &lt;ul&gt;
      &lt;li&gt;depth $l$&lt;/li&gt;
      &lt;li&gt;$l$ weight matrices of the model $\mathbf{W}^{(i)}$&lt;/li&gt;
      &lt;li&gt;$l$ biases of the model $\mathbf{b}^{(i)}$&lt;/li&gt;
      &lt;li&gt;input $\mathbf{x}$ (here: only one for simplicity)&lt;/li&gt;
      &lt;li&gt;target $\mathbf{y}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;outputs:
    &lt;ul&gt;
      &lt;li&gt;output $\hat{\mathbf{y}}$&lt;/li&gt;
      &lt;li&gt;cost function $J$&lt;/li&gt;
      &lt;li&gt;input of unit $j$: $\mathbf{a}_j^{(k)}$ for all $j$&lt;/li&gt;
      &lt;li&gt;output of unit $j$: $\mathbf{h}_j^{(k)}$ for all $j$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;backprop&quot;&gt;Backprop&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;inputs:
    &lt;ul&gt;
      &lt;li&gt;depth l&lt;/li&gt;
      &lt;li&gt;l weight matrices of the model $\mathbf{W}^{(i)}$&lt;/li&gt;
      &lt;li&gt;l biases of the model $\mathbf{b}^{(i)}$&lt;/li&gt;
      &lt;li&gt;outputs of Forward Propagation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;outputs:
    &lt;ul&gt;
      &lt;li&gt;gradients w.r.t. all weights and biases $\nabla_{\mathbf{W}^{(k)}}J$ and $\nabla_{\mathbf{b}^{(k)}}J$
        &lt;ul&gt;
          &lt;li&gt;also computes all $\nabla_{\mathbf{a}^{(k)}}J$ and $\nabla_{\mathbf{h}^{(k)}}J$ in the process
            &lt;ul&gt;
              &lt;li&gt;$\nabla_{\mathbf{a}^{(k)}}J$ can be interpreted as an indication of how each layer’s output should change to reduce error
                &lt;ul&gt;
                  &lt;li&gt;es gibt ein $\nabla_{\mathbf{a}^{(k)}}J$ pro layer k: jede unit in layer k entspricht einer Komponente von $\nabla_{\mathbf{a}^{(k)}}J$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;refers only to the &lt;strong&gt;method used to compute all necessary gradients&lt;/strong&gt;, whereas another algorithm (e.g. SGD) is used to perform &lt;strong&gt;learning&lt;/strong&gt; using these gradients!
    &lt;ul&gt;
      &lt;li&gt;“however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent” &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;&gt;source&lt;/a&gt;
 	&amp;gt; &lt;a name=&quot;reverse_mode_accumulation&quot;&gt;&lt;/a&gt;“More generally, the field of &lt;strong&gt;automatic differentiation&lt;/strong&gt; is concerned with how to compute derivatives algorithmically. The back-propagation algorithm described here is only one approach to automatic differentiation. It is a special case of a broader class of techniques called &lt;strong&gt;reverse mode accumulation&lt;/strong&gt;.” (Goodfellow, Bengio)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;“layer below builds upon (gradient) result of layer above” (basically, chain rule)
    &lt;ul&gt;
      &lt;li&gt;this is why it’s called “backprop”&lt;/li&gt;
      &lt;li&gt;“propagates the gradient backwards through the layers”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;“performs on the order of one &lt;strong&gt;Jacobian product&lt;/strong&gt; per node in the graph” (Goodfellow, Bengio)
    &lt;ul&gt;
      &lt;li&gt;This can be seen from the fact that Backprop visits each edge (of the computational graph for this problem) only once&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;“[…] the amount of computation required for performing the back-propagation &lt;strong&gt;scales linearly with the number of edges&lt;/strong&gt; in $\mathcal{G}$, where the computation &lt;strong&gt;for each edge&lt;/strong&gt; corresponds to computing
    &lt;ul&gt;
      &lt;li&gt;a partial derivative (of one node with respect to one of its parents) as well as performing&lt;/li&gt;
      &lt;li&gt;one multiplication and&lt;/li&gt;
      &lt;li&gt;one addition.” (Goodfellow, Bengio)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-graphs&quot;&gt;Computational Graphs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;the following texts from &lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt; describe the same graphs as Olah is describing in his &lt;a href=&quot;https://colah.github.io/posts/2015-08-Backprop/&quot;&gt;blog post&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;“That algorithm specifies the &lt;strong&gt;forward propagation&lt;/strong&gt; computation, which we could put in a graph $\mathcal{G}$. In order to perform &lt;strong&gt;back-propagation&lt;/strong&gt;, we can construct a computational graph that depends on $\mathcal{G}$ and adds to it an extra set of nodes. These form a &lt;strong&gt;subgraph&lt;/strong&gt; $\mathcal{B}$ with one node per node of $\mathcal{G}$. Computation in $\mathcal{B}$ proceeds in exactly the reverse of the order of computation in $\mathcal{B}$, and each node of $\mathcal{B}$ computes the derivative $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ associated with the &lt;strong&gt;forward graph&lt;/strong&gt; node $u^{(i)}$.” (Goodfellow, Bengio)&lt;/li&gt;
      &lt;li&gt;“The subgraph $\mathcal{B}$ contains exactly one edge for each edge from node $u^{(j)}$ to node $u^{(i)}$ of $\mathcal{G}$.” (Goodfellow, Bengio)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;a computer programming method
    &lt;ul&gt;
      &lt;li&gt;though, in literature one often finds the plural form “dynamic programming methods”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner
    &lt;ul&gt;
      &lt;li&gt;if this “breaking down” is possible for a problem, then the problem is said to have &lt;strong&gt;optimal substructure&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;example-fibonacci-sequence&quot;&gt;Example: Fibonacci sequence&lt;/h4&gt;

&lt;p&gt;source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming#Fibonacci_sequence&quot;&gt;https://en.wikipedia.org/wiki/Dynamic_programming#Fibonacci_sequence&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;→&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;→&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;−&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;−&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;This technique of saving values that have already been calculated is called &lt;strong&gt;memoization&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The function requires only $\mathcal{O}(n)$ time instead of &lt;strong&gt;exponential time&lt;/strong&gt; (but requires $\mathcal{O}(n)$ space)
    &lt;ul&gt;
      &lt;li&gt;i.e. the number of common subexpressions is reduced &lt;strong&gt;without regard to memory&lt;/strong&gt;!&lt;/li&gt;
      &lt;li&gt;note: sometimes recalculating instead of storing can be a good decision, &lt;strong&gt;if memory is limited&lt;/strong&gt;!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;relation-to-backprop&quot;&gt;Relation to Backprop&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Backprop stores the $y_i^{(k-1)}$ during the forward pass and re-uses it during the backward pass to calculate $\frac{\partial E}{\partial w_{ji}^{(k-1)}}=y_i^{(k-1)}\frac{\partial E}{\partial w_{ji}^{(k-1)}}$ (memoization, Dynamic Programming)&lt;/li&gt;
  &lt;li&gt;During the backward pass Backprop visits each edge only once (see above) and gradients that have already been calculated are saved in memory (cf. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_table[u[i]]&lt;/code&gt; in Algo 6.2 or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g&lt;/code&gt; in Algo 6.4 Goodfellow, Bengio)! (memoization, Dynamic Programming)
    &lt;ul&gt;
      &lt;li&gt;this is analogous to the Fibonacci Sequence Algo’s map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; (see above) which saves the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fib(n − 1) + fib(n − 2)&lt;/code&gt; that have already been calculated in memory&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(cf. Figure 6.9 in Goodfellow, Bengio) Back-propagation avoids the exponential explosion in &lt;strong&gt;repeated subexpressions&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;similar to the Fibonacci example “the back-propagation algorithm is designed to reduce the number of common subexpressions &lt;strong&gt;without regard to memory&lt;/strong&gt;.” (Goodfellow, Bengio)&lt;/li&gt;
  &lt;li&gt;“When the memory required to store the value of these expressions is low, the back-propagation approach of equation 6.52 &lt;img src=&quot;/assets/images/goodfellow_ml/Goodf_6_50-6_53.png&quot; alt=&quot;6.52&quot; /&gt; is clearly preferable because of its reduced runtime. However, equation 6.53 is also a valid implementation of the chain rule, and is useful &lt;strong&gt;when memory is limited&lt;/strong&gt;.” (Goodfellow, Bengio)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;implementing-softmax-correctly&quot;&gt;Implementing Softmax Correctly&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Problem: Exponentials get very big and can have very different magnitudes
    &lt;ul&gt;
      &lt;li&gt;Solution:
        &lt;ul&gt;
          &lt;li&gt;Evaluate $\ln{(\sum_{j=1}^K\exp{(\mathbf{w}_j^\top\mathbf{x})})}$ in the denominator &lt;strong&gt;before&lt;/strong&gt; calculating the fraction&lt;/li&gt;
          &lt;li&gt;since $\text{softmax}(\mathbf{a} + \mathbf{b}) = \text{softmax}(\mathbf{a})$ for all $\mathbf{b}\in\mathbb{R}^D$, one can subtract the largest $\mathbf{w}_j$ from the others
            &lt;ul&gt;
              &lt;li&gt;(entspricht $\mathbf{a}=\mathbf{w}_j^\top\mathbf{x}$ und $\mathbf{b}=\mathbf{w}_M^\top\mathbf{x}$ bzw. Kürzen des Bruches mit $\exp{(\mathbf{w}_M^\top\mathbf{x})}$, wobei $\mathbf{w}_M$ das größte weight ist)&lt;/li&gt;
              &lt;li&gt;(egal, ob $\mathbf{b}$ von $\mathbf{x}$ abhängt oder nicht!)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mlp-in-numpy-from-scratch&quot;&gt;MLP in numpy from scratch&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;see &lt;a href=&quot;https://htmlpreview.github.io/?https://github.com/pharath/home/blob/master/_posts_html/notebooks_in_html/Expl_NN_in_numpy_copy.html&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stochastic-learning-vs-batch-learning&quot;&gt;Stochastic Learning vs Batch Learning&lt;/h2&gt;

&lt;p&gt;source: LeCun et al. “Efficient BackProp”&lt;/p&gt;

&lt;h3 id=&quot;sgd&quot;&gt;SGD&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Pros:
    &lt;ul&gt;
      &lt;li&gt;is usually much faster than batch learning
        &lt;ul&gt;
          &lt;li&gt;consider large redundant data set
            &lt;ul&gt;
              &lt;li&gt;example: training set of size 1000 is inadvertently composed of 10 identical copies of a set with 100 samples&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;also often results in better solutions because of the noise in the updates
        &lt;ul&gt;
          &lt;li&gt;because the noise present in the updates can result in the weights jumping into the basin of another, possibly deeper, local minimum. This has been demonstrated in certain simplified cases&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;can be used for tracking changes
        &lt;ul&gt;
          &lt;li&gt;useful when the function being modeled is changing over time&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons:
    &lt;ul&gt;
      &lt;li&gt;noise also prevents full convergence to the minimum
        &lt;ul&gt;
          &lt;li&gt;Instead of converging to the exact minimum, the convergence stalls out due to the &lt;strong&gt;weight fluctuations&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;size of the fluctuations depend on the degree of noise of the stochastic updates:
            &lt;ul&gt;
              &lt;li&gt;The variance of the fluctuations around the local minimum is proportional to the learning rate $\eta$&lt;/li&gt;
              &lt;li&gt;So in order &lt;strong&gt;to reduce the fluctuations&lt;/strong&gt; we can either
                &lt;ul&gt;
                  &lt;li&gt;decrease (anneal) the learning rate or&lt;/li&gt;
                  &lt;li&gt;have an adaptive batch size.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-gd&quot;&gt;Batch GD&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Pros:
    &lt;ul&gt;
      &lt;li&gt;Conditions of convergence are well understood.&lt;/li&gt;
      &lt;li&gt;Many acceleration techniques (e.g. conjugate gradient) only operate in batch learning.
   	- Theoretical analysis of the weight dynamics and convergence rates are simpler&lt;/li&gt;
      &lt;li&gt;one is able to use second order methods to speed the learning process
        &lt;ul&gt;
          &lt;li&gt;Second order methods speed learning by estimating not just the gradient but also the curvature of the cost surface. Given the curvature, one can estimate the approximate location of the actual minimum.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons:
    &lt;ul&gt;
      &lt;li&gt;redundancy can make batch learning much slower than on-line&lt;/li&gt;
      &lt;li&gt;often results in worse solutions because of the absence of noise in the updates
        &lt;ul&gt;
          &lt;li&gt;will discover the minimum of whatever basin the weights are initially placed&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;changes go undetected and we obtain rather bad results since we are likely to average over several rules&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mini-batch-gd&quot;&gt;Mini-batch GD&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Another method to remove noise [in SGD] is to use “mini-batches”, that is, start with a small batch size and increase the size as training proceeds.
    &lt;ul&gt;
      &lt;li&gt;However, deciding the rate at which to increase the batch size and which inputs to include in the small batches is as difficult as determining the proper learning rate. &lt;strong&gt;Effectively the size of the learning rate in stochastic learning corresponds to the respective size of the mini batch.&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Note also that the problem of removing the noise in the data may be less critical than one thinks because of generalization. &lt;strong&gt;Overtraining may occur long before the noise regime is even reached.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;shuffling-the-examples&quot;&gt;Shuffling the Examples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/Expl_NN_in_numpy.ipynb&quot;&gt;Expl_NN_in_numpy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/MLP_in_numpy.ipynb&quot;&gt;MLP_in_numpy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/MLP_selbst_versucht.ipynb&quot;&gt;MLP_selbst_versucht&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/WofuerIst__name__gut.ipynb&quot;&gt;WofuerIst__name__gut&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Lecture_Notes" /><category term="Machine_Learning" /><category term="lecture_notes" /><category term="ml" /><summary type="html">[OLD VERSION!] Notes on Machine Learning theory. Based on C. M. Bishop, &quot;Pattern Recognition and Machine Learning&quot; (2011) and Goodfellow, Bengio, Courville, &quot;Deep Learning&quot;.</summary></entry><entry><title type="html">New Inspirations for Machine Learning Methods</title><link href="http://localhost:4000/notes/machine_learning/notes-Neuroscience/" rel="alternate" type="text/html" title="New Inspirations for Machine Learning Methods" /><published>2022-02-26T00:00:00+01:00</published><updated>2022-02-26T00:00:00+01:00</updated><id>http://localhost:4000/notes/machine_learning/notes-Neuroscience</id><content type="html" xml:base="http://localhost:4000/notes/machine_learning/notes-Neuroscience/">&lt;ul&gt;
  &lt;li&gt;Bengio &lt;a href=&quot;https://www.youtube.com/watch?v=ve4A1XSmw0c&quot;&gt;youtube&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;I’d like to see more work done that would really be helpful for the program I’ve been talking about is regarding memory because an attention because the the kind of memory that we’ve put in you on that these days I mean like the extended memory neural nets like neural Turing machines and and and all those they’re not the kind of memory that you find in human brains really it doesn’t look very plausible so it looks like a lot of our memory and the way that we select like attention is is more of a process of the dynamic coming from the dynamics of the circuit which leads to sort of concentration of activity in a few selected places rather than copying information in some memory bank and then retrieving it&lt;/li&gt;
      &lt;li&gt;TODO: yeah I mean another area the intersection with biology of course is you you mentioned sample efficiency and and you know natural selection is&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Notes" /><category term="Machine_Learning" /><category term="notes" /><category term="ml" /><summary type="html">Notes on some inspirations for Machine Learning methods. Based on opinions from leading scientists and also some of my own opinions.</summary></entry><entry><title type="html">Carla Simulator</title><link href="http://localhost:4000/carla/simulator/self_driving/notes-Carla/" rel="alternate" type="text/html" title="Carla Simulator" /><published>2022-02-17T00:00:00+01:00</published><updated>2022-02-17T00:00:00+01:00</updated><id>http://localhost:4000/carla/simulator/self_driving/notes-Carla</id><content type="html" xml:base="http://localhost:4000/carla/simulator/self_driving/notes-Carla/">&lt;h1 id=&quot;download&quot;&gt;Download&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/carla-simulator/carla/blob/master/Docs/download.md&quot;&gt;release list and doc list&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;package contains
        &lt;ul&gt;
          &lt;li&gt;a precompiled version of the simulator,&lt;/li&gt;
          &lt;li&gt;the Python API module and&lt;/li&gt;
          &lt;li&gt;some scripts to be used as examples.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;install&quot;&gt;Install&lt;/h1&gt;

&lt;h2 id=&quot;install-client-library&quot;&gt;Install Client library&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip3 install --upgrade pip&lt;/code&gt; (because pip3 version 20.3 or higher is required)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip3 install carla==0.9.12&lt;/code&gt;, when you use Carla version 0.9.12.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;install-dependencies-for-example-scripts&quot;&gt;Install dependencies for example scripts&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CARLA_0.9.12/PythonAPI/carla: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip3 install -r carla/requirements.txt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;CARLA_0.9.12/PythonAPI/examples: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip3 install -r examples/requirements.txt&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ros_bridge&quot;&gt;ROS_bridge&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;after installation: `[bridge-1] ModuleNotFoundError: No module named ‘derived_object_msgs’
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt install ros-foxy-derived-object-msgs&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;imitation-learning&quot;&gt;Imitation Learning&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;download Carla 0.8.2&lt;/li&gt;
  &lt;li&gt;copy folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PythonClient/carla&lt;/code&gt; from Carla 0.8.2 into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ws_170222/imitation-learning&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install numpy scipy tensorflow-gpu==1.1 pillow&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Carla" /><category term="Simulator" /><category term="Self_Driving" /><category term="carla" /><category term="simulator" /><category term="self_driving" /><summary type="html">Notes on the Carla Simulator</summary></entry><entry><title type="html">Machine Learning (Part 3)</title><link href="http://localhost:4000/lecture_notes/machine_learning/computer_vision/lecture-notes-ML-part3/" rel="alternate" type="text/html" title="Machine Learning (Part 3)" /><published>2022-01-27T00:00:00+01:00</published><updated>2022-01-27T00:00:00+01:00</updated><id>http://localhost:4000/lecture_notes/machine_learning/computer_vision/lecture-notes-ML-part3</id><content type="html" xml:base="http://localhost:4000/lecture_notes/machine_learning/computer_vision/lecture-notes-ML-part3/">&lt;h1 id=&quot;cnn-concept&quot;&gt;CNN Concept&lt;/h1&gt;

&lt;h2 id=&quot;good-overviews&quot;&gt;Good Overviews&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network#Receptive_field&quot;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-do-cnns-work&quot;&gt;Why do CNNs work?&lt;/h2&gt;

&lt;h3 id=&quot;sparse-connectivity&quot;&gt;Sparse Connectivity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;as opposed to &lt;strong&gt;Full&lt;/strong&gt; Connectivity&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Convolutional networks, however, typically have &lt;strong&gt;sparse interactions&lt;/strong&gt; (also referred to as &lt;strong&gt;sparse connectivity&lt;/strong&gt; or &lt;strong&gt;sparse weights&lt;/strong&gt;). This is accomplished by making the kernel smaller than the input.
        &lt;ul&gt;
          &lt;li&gt;This means that we need to store fewer parameters, which both
            &lt;ul&gt;
              &lt;li&gt;reduces the &lt;strong&gt;memory requirements&lt;/strong&gt; of the model and&lt;/li&gt;
              &lt;li&gt;improves its &lt;strong&gt;statistical efficiency&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;It also means that computing the output requires &lt;strong&gt;fewer operations&lt;/strong&gt;.&lt;/li&gt;
          &lt;li&gt;These improvements in efficiency are usually quite large.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;parameter-sharing&quot;&gt;Parameter Sharing&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: increases bias of the model, but not the variance&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters &lt;strong&gt;for every location&lt;/strong&gt;, we learn only one set.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;efficiency-of-the-convolution-operation&quot;&gt;Efficiency of the Convolution Operation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;sparse connectivity&lt;/strong&gt; and &lt;strong&gt;parameter sharing&lt;/strong&gt; can dramatically improve the efficiency of a linear function for detecting edges in an image
        &lt;ul&gt;
          &lt;li&gt;[matrix multiplication vs convolution:]
            &lt;ul&gt;
              &lt;li&gt;Of course, most of the entries of the matrix would be zero. If we stored only the nonzero entries of the matrix, then both matrix multiplication and convolution would require the same number of &lt;strong&gt;floating point operations&lt;/strong&gt; to compute.&lt;/li&gt;
              &lt;li&gt;Convolution is an extremely efficient way of describing transformations that apply the same linear transformation of a small, local region across the entire input.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;translational-equivariance-via-parameter-sharing&quot;&gt;Translational Equivariance (via Parameter Sharing)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;achieved by convolution operation (aka “parameter sharing across locations”)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
        &lt;ul&gt;
          &lt;li&gt;In the case of convolution, the particular form of &lt;strong&gt;parameter sharing causes&lt;/strong&gt; the layer to have a property called equivariance to translation.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;convolution creates a 2-D map of where certain features appear in the input. If we move the object in the input, &lt;strong&gt;its representation&lt;/strong&gt; will move the same amount in the output.&lt;/li&gt;
      &lt;li&gt;This is useful for when we know that some function of a small number of neighboring pixels is useful when applied to multiple input locations.
        &lt;ul&gt;
          &lt;li&gt;For &lt;strong&gt;example&lt;/strong&gt;, when processing images, it is useful to detect edges in the first layer of a convolutional network. The same edges appear more or less everywhere in the image, so it is practical to share parameters across the entire image.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;[Caveat:] In some cases, we may &lt;strong&gt;not&lt;/strong&gt; wish to share parameters across the entire image.
        &lt;ul&gt;
          &lt;li&gt;For &lt;strong&gt;example&lt;/strong&gt;, if we are processing images that are cropped to be centered on an individual’s face, we probably want to extract different features at different locations—the part of the network processing the top of the face needs to look for eyebrows, while the part of the network processing the bottom of the face needs to look for a chin.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Convolution is &lt;strong&gt;not naturally equivariant&lt;/strong&gt; to some other transformations, such as changes in the &lt;strong&gt;scale&lt;/strong&gt; or &lt;strong&gt;rotation&lt;/strong&gt; of an image. Other mechanisms are necessary for handling these kinds of transformations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo&quot;&gt;source&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;because the convolution operator &lt;strong&gt;commutes&lt;/strong&gt; wrt translation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/translational-invariance-vs-translational-equivariance-f9fbc8fca63a&quot;&gt;source&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Translational Equivariance or just equivariance is a very important property of the convolutional neural networks where the position of the object in the image should not be fixed in order for it to be detected by the CNN. This simply means that &lt;strong&gt;if the input changes, the output also changes&lt;/strong&gt;. To be precise, a function $f(x)$ is said to be equivariant to a function g if $f(g(x)) = g(f(x))$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Equivariant_map&quot;&gt;Wiki: Equivariant Map&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;That is, applying a symmetry transformation and then computing the function produces the same result as computing the function and then applying the transformation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reduction-in-flexibility-of-model&quot;&gt;Reduction in Flexibility of Model&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;source&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;This [translational equivariance] means that &lt;strong&gt;translating the input to a convolutional layer will result in translating the output&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;[“flexibility reduction is good” and &lt;strong&gt;this is also why CNNs work well&lt;/strong&gt;]:
        &lt;ul&gt;
          &lt;li&gt;There is a &lt;strong&gt;downside to complete flexibility&lt;/strong&gt;. While we know that we can learn our target function, there’s also a whole universe of incorrect functions that look exactly the same on our training data. If we’re totally flexible, our model could learn any one of these functions, and once we move away from the training data, we might fail to generalise. For this reason, it’s often desirable to restrict our flexibility a little. If we can identify a smaller class of functions which still contains our target, and build an architecture which only learns functions in this class, we rule out many wrong answers while still allowing our model enough flexibility to learn the right answer. This might make a big difference to the necessary amount of training data or, given the same training data, make the difference between a highly successful model and one that performs very poorly.&lt;/li&gt;
          &lt;li&gt;A famous success story for this reduction in flexibility is the &lt;strong&gt;CNN&lt;/strong&gt;. The &lt;strong&gt;convolutional structure of the CNN encodes the translation symmetries of image data&lt;/strong&gt;. We’ve given up total flexibility and can no longer learn functions which lack translation symmetry, but we don’t lose any useful flexibility, because we know that our target function does have translation symmetry. In return, we have a much smaller universe of functions to explore, and we’ve reduced the number of parameters that we need to train.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;extend-this-idea&quot;&gt;Extend this idea&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;take this idea further: extending translation equivariance of CNNs to further symmetry classes $\Rightarrow$ “Group Equivariant CNNs” [&lt;a href=&quot;https://arxiv.org/pdf/1602.07576.pdf&quot;&gt;Cohen, Welling 2016&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;translational-invariance-via-pooling&quot;&gt;Translational invariance (via Pooling)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;achieved by pooling operation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;popular pooling functions&lt;/strong&gt; include
        &lt;ul&gt;
          &lt;li&gt;the average of a rectangular neighborhood,&lt;/li&gt;
          &lt;li&gt;the L2 norm of a rectangular neighborhood, or&lt;/li&gt;
          &lt;li&gt;a weighted average based on the distance from the central pixel.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;pooling helps to make the representation become approximately invariant to small translations of the input.&lt;/li&gt;
      &lt;li&gt;Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change.&lt;/li&gt;
      &lt;li&gt;Invariance to local translation can be a very useful property if we care more about whether some feature is present than exactly where it is.
        &lt;ul&gt;
          &lt;li&gt;For example, when determining whether an image contains a face, we need not know the location of the eyes with pixel-perfect accuracy, we just need to know that there is an eye on the left side of the face and an eye on the right side of the face.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;[Caveat:] In other contexts, it is more important to preserve the location of a feature.
        &lt;ul&gt;
          &lt;li&gt;For example, if we want to find a corner defined by two edges meeting at a specific orientation, we need to preserve the location of the edges well enough to test whether they meet.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The use of pooling can be viewed as adding an &lt;strong&gt;infinitely strong prior&lt;/strong&gt; that the function the layer learns must be invariant to small translations. &lt;strong&gt;When&lt;/strong&gt; this assumption is correct, it can greatly improve the statistical efficiency of the network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pooling&quot;&gt;Pooling&lt;/h3&gt;

&lt;h4 id=&quot;how-to-set-size-of-pools&quot;&gt;How to set size of pools?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;larger pooling region $\Rightarrow$ more invariant to translation, but poorer generalization
    &lt;ul&gt;
      &lt;li&gt;notice, units in the deeper layers may indirectly interact with a larger portion of the input&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;in practice, best to pool slowly (via few stacks of conv-pooling layers)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/chapter/10.1007%2F978-3-030-66151-9_17&quot;&gt;Mouton, Myburgh, Davel 2020&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;CNNs are not invariant to translation! Reason:
        &lt;ul&gt;
          &lt;li&gt;This lack of invariance is attributed to
            &lt;ul&gt;
              &lt;li&gt;the &lt;strong&gt;use of stride&lt;/strong&gt; which subsamples the input, resulting in a &lt;strong&gt;loss of information&lt;/strong&gt;,&lt;/li&gt;
              &lt;li&gt;and &lt;strong&gt;fully connected layers&lt;/strong&gt; which lack spatial reasoning.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;“&lt;strong&gt;local homogeneity&lt;/strong&gt;” (dataset-specific characteristic):
        &lt;ul&gt;
          &lt;li&gt;determines relationship between &lt;strong&gt;pooling kernel size&lt;/strong&gt; and &lt;strong&gt;stride&lt;/strong&gt; required for translation invariance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learning-invariance-to-other-transformations&quot;&gt;Learning Invariance to other Transformations&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;[pooling over different filters (instead of pooling spatially over the image) allows &lt;strong&gt;learning rotation invariance&lt;/strong&gt;] Pooling over spatial regions produces invariance to translation, but if we pool &lt;strong&gt;over the outputs of separately parametrized convolutions&lt;/strong&gt; [i.e. different filters], the features can learn which transformations to become invariant to (see figure 9.9).
        &lt;ul&gt;
          &lt;li&gt;This principle is leveraged by maxout networks (Goodfellow et al., 2013a) and other convolutional networks.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;strided-pooling&quot;&gt;Strided Pooling&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;[advantages of &lt;strong&gt;strided&lt;/strong&gt; pooling] Because pooling summarizes the responses over a whole neighborhood, it is possible to use fewer pooling units than detector units, by reporting summary statistics for pooling regions spaced $k$ pixels apart rather than $1$ pixel apart.
        &lt;ul&gt;
          &lt;li&gt;See figure 9.10 for an example.&lt;/li&gt;
          &lt;li&gt;This &lt;strong&gt;improves the computational efficiency&lt;/strong&gt; of the network because the next layer has roughly $k$ times fewer inputs to process.&lt;/li&gt;
          &lt;li&gt;When the &lt;strong&gt;number of parameters&lt;/strong&gt; in the next layer is a function of its input size (such as when the next layer is fully connected and based on matrix multiplication) this reduction in the input size can also result in &lt;strong&gt;improved statistical efficiency and reduced memory requirements&lt;/strong&gt; for storing the parameters.
            &lt;ul&gt;
              &lt;li&gt;less parameters $\Rightarrow$ reduces overfitting&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;nonlinear-downsampling-method&quot;&gt;Nonlinear Downsampling Method&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;[pooling as a nonlinear downsampling method for a following classification layer] For many tasks, pooling is &lt;strong&gt;essential for handling inputs of varying size&lt;/strong&gt;.
        &lt;ul&gt;
          &lt;li&gt;For example, if we want to classify images of variable size, the &lt;strong&gt;input to the classification layer must have a fixed size&lt;/strong&gt;.
            &lt;ul&gt;
              &lt;li&gt;This is usually accomplished by varying the size of an offset between pooling regions so that the &lt;strong&gt;classification layer always receives the same number of summary statistics regardless of the input size&lt;/strong&gt;.
                &lt;ul&gt;
                  &lt;li&gt;For example, the final pooling layer of the network may be defined to output four sets of summary statistics, one for each quadrant of an image, regardless of the image size.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;nonlinearities&quot;&gt;Nonlinearities&lt;/h3&gt;

&lt;h4 id=&quot;what-do-relu-layers-accomplish-&quot;&gt;What do ReLU layers accomplish ?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Piece-wise linear tiling: mapping is locally linear.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1402.1869.pdf&quot;&gt;Montufar et al.  “On the number of linear regions of DNNs” arXiv 2014&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;ReLU layers do &lt;strong&gt;local linear approximation&lt;/strong&gt;.
        &lt;ul&gt;
          &lt;li&gt;Number of planes grows exponentially with number of hidden units.
            &lt;ul&gt;
              &lt;li&gt;Multiple  layers  yield  exponential savings in number of parameters (parameter sharing).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-reasons-ranzato&quot;&gt;Other Reasons (Ranzato)&lt;/h3&gt;

&lt;p&gt;[&lt;a href=&quot;https://ranzato.github.io/publications/ranzato_deeplearn17_lec1_vision.pdf&quot;&gt;source&lt;/a&gt;]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/CV/whyconvnetswork.png&quot; alt=&quot;whyconvnetswork&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conv-layer-properties&quot;&gt;Conv Layer properties&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;key points of Karpathy slide below:
    &lt;ul&gt;
      &lt;li&gt;output size: $(\frac{W-F+2P}{S}+1) \times (\frac{H-F+2P}{S}+1) \times K$&lt;/li&gt;
      &lt;li&gt;number of parameters: $(F\cdot F\cdot D_{input}+1)\cdot K$
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: for FC layers: number of parameters: $(c \cdot p)+c$
            &lt;ul&gt;
              &lt;li&gt;where
                &lt;ul&gt;
                  &lt;li&gt;current layer neurons $c$&lt;/li&gt;
                  &lt;li&gt;previous layer neurons $p$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ranzato slides:
    &lt;ul&gt;
      &lt;li&gt;computational cost per conv layer: $(F\cdot F\cdot D_{input}+1)\cdot (\frac{W-F+2P}{S}+1)\cdot (\frac{H-F+2P}{S}+1)\cdot K$
        &lt;ul&gt;
          &lt;li&gt;[das ist &lt;strong&gt;nicht&lt;/strong&gt; “output size” mal “number of parameters”! Sonst wäre $K$ doppelt, weil $K$ in beiden enthalten ist!]
            &lt;ul&gt;
              &lt;li&gt;[cost für EINEN Kernel: $(F\cdot F\cdot D_{input}+1)\cdot (\frac{W-F+2P}{S}+1)\cdot (\frac{H-F+2P}{S}+1)$]
  &lt;img src=&quot;/assets/images/conv_layer_params_cs231n_karpathy.png&quot; alt=&quot;conv_layer_params_cs231n_karpathy.png&quot; /&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-many-feature-maps-per-conv-layer&quot;&gt;How many feature maps (per conv layer)?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Ranzato slides:
    &lt;ul&gt;
      &lt;li&gt;Usually, there are more output feature maps than input feature maps.&lt;/li&gt;
      &lt;li&gt;Convolutional layers can increase the number of hidden units by big factors (and are expensive to compute).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-the-size-of-the-filters&quot;&gt;What’s the size of the filters?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The size of the filters has to match the size/scale of the patterns we want to detect (task dependent)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-is-the-size-of-f-always-an-odd-number&quot;&gt;Why is the size of $F$ always an odd number?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;usually $1$, $3$, $5$, $7$ and sometimes $11$&lt;/li&gt;
  &lt;li&gt;you can use filters with even numbers, but it is not common&lt;/li&gt;
  &lt;li&gt;the lowest people use is $3$, mainly for convenience
    &lt;ul&gt;
      &lt;li&gt;you want to apply the filter around a well-defined position that “exists” in the input volume (see traditional image processing)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-the-receptive-field-of-a-filter&quot;&gt;What’s the receptive field of a filter?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$\text{RF}&lt;em&gt;i=(\text{RF}&lt;/em&gt;{i-1}-1)S_i+F_i$
    &lt;ul&gt;
      &lt;li&gt;$F_i$: Kernel size&lt;/li&gt;
      &lt;li&gt;$S_i$: Stride&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;padding&quot;&gt;Padding&lt;/h2&gt;

&lt;h3 id=&quot;why-pad-with-zeros-&quot;&gt;Why pad with zeros ?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;idea: you do not want the outer padded pixels to contribute to the dot product
    &lt;ul&gt;
      &lt;li&gt;[well, this is engineering: i.e. if you have some reason to use other paddings, just try it]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;but you can also use other padding techniques, but it is less common, in practice&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;valid-vs-same-padding&quot;&gt;VALID vs SAME padding&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://iq.opengenus.org/same-and-valid-padding/&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The general meaning of SAME and VALID padding are:
        &lt;ul&gt;
          &lt;li&gt;SAME: Expects padding to be such that the input and output is of same size (provided stride=1) (pad value = kernel size)&lt;/li&gt;
          &lt;li&gt;VALID: Padding value is set to 0&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: SAME padding is different in different frameworks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-the-function-of-1x1-filters&quot;&gt;What’s the function of 1x1 filters?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Channel-wise pooling (aka cross-channel downsampling, cross-channel pooling)
    &lt;ul&gt;
      &lt;li&gt;$\Rightarrow$ dimensionality reduction (in depth), while &lt;strong&gt;introducing nonlinearity&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;these methods introduce &lt;strong&gt;additional hyperparameters&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;usually done in a separate CPU (or GPU) thread, so that &lt;strong&gt;training&lt;/strong&gt; and &lt;strong&gt;augmentation&lt;/strong&gt; can be done &lt;strong&gt;in parallel&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;common-examples&quot;&gt;Common examples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;cropping&lt;/li&gt;
  &lt;li&gt;zooming&lt;/li&gt;
  &lt;li&gt;flipping&lt;/li&gt;
  &lt;li&gt;mirroring&lt;/li&gt;
  &lt;li&gt;Color PCA
    &lt;ul&gt;
      &lt;li&gt;for each pixel an Eigenspace is computed that highlights the expected color variations and then within those color variations you can adjust the colors of the entire image&lt;/li&gt;
      &lt;li&gt;AlexNet paper:
  &lt;img src=&quot;/home/assets/images/ML_part3/ColorPCA_AlexNet_1.png&quot; alt=&quot;ColorPCA_AlexNet_1.png&quot; /&gt;
  &lt;img src=&quot;/home/assets/images/ML_part3/ColorPCA_AlexNet_2.png&quot; alt=&quot;ColorPCA_AlexNet_2.png&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;jittering
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://paperswithcode.com/method/colorjitter&quot;&gt;ColorJitter&lt;/a&gt; is a type of image data augmentation where we randomly change the brightness, contrast and saturation of an image.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;used less often, computationally expensive:
    &lt;ul&gt;
      &lt;li&gt;rotation&lt;/li&gt;
      &lt;li&gt;shearing&lt;/li&gt;
      &lt;li&gt;local warping&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;test-time-augmentation-tta&quot;&gt;Test-time augmentation (TTA)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;apply &lt;strong&gt;cropping and flipping&lt;/strong&gt; also at test time&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ColorPCA&lt;/strong&gt; at test time can improve accuracy another $1\%$, but runtime will increase!&lt;/li&gt;
  &lt;li&gt;My question: Is &lt;strong&gt;zooming&lt;/strong&gt; at test time not necessary because CNNs are invariant to translations?
    &lt;ul&gt;
      &lt;li&gt;convolutions are not naturally invariant to other transformations such as rotation and &lt;strong&gt;scale [zoom]&lt;/strong&gt; (see above and Goodfellow_2016)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Does TTA make sense? &lt;a href=&quot;https://cs230.stanford.edu/files/cs230exam_fall18_soln.pdf&quot;&gt;Stanford exam&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Both answers are right.&lt;/li&gt;
      &lt;li&gt;If &lt;strong&gt;no&lt;/strong&gt;, then explain that we want to test on real data only.&lt;/li&gt;
      &lt;li&gt;If &lt;strong&gt;yes&lt;/strong&gt;, then explain in which situation doing data augmentation on test set might make sense (e.g. as an ensemble approach in image classifiers).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;cnn-datasets&quot;&gt;CNN Datasets&lt;/h1&gt;

&lt;h2 id=&quot;imagenet-dataset-vs-pascal-dataset&quot;&gt;ImageNet Dataset vs PASCAL Dataset&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;PASCAL&lt;/strong&gt;: classify “bird”, “cat”, … etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ImageNet&lt;/strong&gt;: more finegrained classes:
    &lt;ul&gt;
      &lt;li&gt;e.g.
        &lt;ul&gt;
          &lt;li&gt;instead of “bird” you have to classify “flamingo”, “cock”, “ruffed grouse”, “quail”, … etc.&lt;/li&gt;
          &lt;li&gt;instead of “cat” you have to classify “Egyptian cat”, “Persian cat”, “Siamese cat”, “tabby”, … etc.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;cnn-architectures&quot;&gt;CNN Architectures&lt;/h1&gt;

&lt;h2 id=&quot;alexnet-vs-lenet&quot;&gt;AlexNet vs LeNet&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;architecture:
    &lt;ul&gt;
      &lt;li&gt;conv&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;normalization (local response normalization)&lt;/li&gt;
      &lt;li&gt;max pool&lt;/li&gt;
      &lt;li&gt;conv&lt;/li&gt;
      &lt;li&gt;ReLU&lt;/li&gt;
      &lt;li&gt;normalization&lt;/li&gt;
      &lt;li&gt;max pool&lt;/li&gt;
      &lt;li&gt;a few more conv layers with ReLU,&lt;/li&gt;
      &lt;li&gt;max pool (after 5. conv)&lt;/li&gt;
      &lt;li&gt;several FC layers with ReLU&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;similar to LeNet&lt;/li&gt;
  &lt;li&gt;AlexNet has more layers&lt;/li&gt;
  &lt;li&gt;60 million parameters&lt;/li&gt;
  &lt;li&gt;first use of ReLU&lt;/li&gt;
  &lt;li&gt;local response normalization layers (not used anymore)&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;GPU acceleration (CUDA Kernel by Krizhevsky)&lt;/li&gt;
  &lt;li&gt;more data&lt;/li&gt;
  &lt;li&gt;Momentum&lt;/li&gt;
  &lt;li&gt;learning rate reduction (divide by 10 on plateaus)
    &lt;ul&gt;
      &lt;li&gt;paper:
        &lt;ul&gt;
          &lt;li&gt;[…] divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and reduced three times prior to termination […]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ensembling&lt;/li&gt;
  &lt;li&gt;weight decay
    &lt;ul&gt;
      &lt;li&gt;paper:
        &lt;ul&gt;
          &lt;li&gt;We trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and &lt;strong&gt;weight decay&lt;/strong&gt; of 0.0005. We found that this small amount of weight decay was important for the model to learn. In other words, weight decay here is not merely a regularizer: it reduces the model’s &lt;strong&gt;training&lt;/strong&gt; error.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;efficient&lt;/strong&gt; data augmentation
    &lt;ol&gt;
      &lt;li&gt;image translations and horizontal reflections&lt;/li&gt;
      &lt;li&gt;Color PCA
        &lt;ul&gt;
          &lt;li&gt;We employ &lt;strong&gt;two distinct forms&lt;/strong&gt; of data augmentation, both of which allow transformed images to be produced from the original images &lt;strong&gt;with very little computation&lt;/strong&gt;, so the transformed images &lt;strong&gt;do not need to be stored on disk&lt;/strong&gt;. In our implementation, the transformed images are generated in Python code on the CPU while the GPU is training on the previous batch of images. So these &lt;strong&gt;data augmentation schemes are, in effect, computationally free&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
        &lt;ul&gt;
          &lt;li&gt;[i.e. no additional memory, only little additional computation].&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;zfnet-vs-alexnet&quot;&gt;ZFNet vs AlexNet&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Zeiler, Fergus&lt;/li&gt;
  &lt;li&gt;basically, same idea&lt;/li&gt;
  &lt;li&gt;better hyperparameters, e.g.
    &lt;ul&gt;
      &lt;li&gt;different stride size&lt;/li&gt;
      &lt;li&gt;different numbers of filters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://paperswithcode.com/method/zfnet&quot;&gt;source&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;ZFNet is a classic convolutional neural network. The design was motivated by visualizing intermediate feature layers and the operation of the classifier. Compared to AlexNet, the filter sizes are reduced and the stride of the convolutions are reduced.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vggnet-vs-alexnet&quot;&gt;VGGNet vs AlexNet&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Simonyan, Zisserman (Oxford)&lt;/li&gt;
  &lt;li&gt;mainly used: VGG16 and VGG19&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;deeper with smaller filters&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;$3\times 3$ filters all the way
        &lt;ul&gt;
          &lt;li&gt;(1) &lt;strong&gt;deeper&lt;/strong&gt; (3 layers instead of 1),&lt;/li&gt;
          &lt;li&gt;(2) &lt;strong&gt;more nonlinearities&lt;/strong&gt; and&lt;/li&gt;
          &lt;li&gt;(3) &lt;strong&gt;fewer parameters&lt;/strong&gt; for same &lt;strong&gt;effective receptive field&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;higher total memory usage (most memory in early layers)
    &lt;ul&gt;
      &lt;li&gt;need to store numbers (e.g. activations) during forward pass because you will need them during backprop&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;roughly twice as many parameters (most parameters in later FC layers)&lt;/li&gt;
  &lt;li&gt;try to &lt;strong&gt;maintain a constant level of compute through layers&lt;/strong&gt; [Leibe Part 17 Folie 23: “same complexity per layer”]:
    &lt;ul&gt;
      &lt;li&gt;downsample spatially and simultaneously increase depth as you go deeper
        &lt;ul&gt;
          &lt;li&gt;Leibe:
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;Heuristic&lt;/strong&gt;: double the $\text{#}(\text{filter channels})$ whenever &lt;strong&gt;pooling&lt;/strong&gt; halves the dimensions
                &lt;ul&gt;
                  &lt;li&gt;saves computation by working on smaller input&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;similar training procedure as AlexNet&lt;/li&gt;
  &lt;li&gt;no local response normalization&lt;/li&gt;
  &lt;li&gt;ensembling (like AlexNet)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;googlenet-vs-alexnet&quot;&gt;GoogleNet vs AlexNet&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Christian Szegedy, Dragomir Anguelov&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;deeper&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;22 layers
        &lt;ul&gt;
          &lt;li&gt;[inception module zählt als 2 layer, ansonsten zähle alle Conv layer; zähle sozusagen alle “blauen Kästchen”]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;computationally more efficient (because no FC layers)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;no FC layers&lt;/strong&gt; (except for one for the classifier)
    &lt;ul&gt;
      &lt;li&gt;fewer parameters (12x less than AlexNet)&lt;/li&gt;
      &lt;li&gt;computationally more efficient&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;use of “local network topology” aka “network within a network” aka &lt;strong&gt;inception module&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;idea&lt;/strong&gt;: filters with different sizes, will handle better multiple object scales&lt;/li&gt;
      &lt;li&gt;applying several different kinds of filter operations in parallel
        &lt;ul&gt;
          &lt;li&gt;1x1,&lt;/li&gt;
          &lt;li&gt;3x3,&lt;/li&gt;
          &lt;li&gt;5x5,&lt;/li&gt;
          &lt;li&gt;3x3 max pooling  (to summarize the content of the previous layer)
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: this max pooling layer uses SAME padding (i.e. input size=output size), so that the concatenation with the other outputs is possible&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;concatenate all outputs depth-wise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;use of &lt;strong&gt;bottleneck layers&lt;/strong&gt; aka “one-by-one convolutions”
    &lt;ul&gt;
      &lt;li&gt;project feature maps to lower (depth) dimension before convolutional operations&lt;/li&gt;
      &lt;li&gt;compared to an inception module &lt;strong&gt;without&lt;/strong&gt; bottleneck layers:
        &lt;ul&gt;
          &lt;li&gt;reduces total number of ops (i.e. computational complexity)&lt;/li&gt;
          &lt;li&gt;prevents depth from blowing up after each inception module&lt;/li&gt;
          &lt;li&gt;less parameters&lt;/li&gt;
          &lt;li&gt;more nonlinearities&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;architecture:
    &lt;ul&gt;
      &lt;li&gt;stem network (similar to AlexNet)
        &lt;ul&gt;
          &lt;li&gt;Conv - MaxPool - LocalRespNorm - Conv - Conv - LocalRespNorm - MaxPool&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;9 inception modules all stacked on top of each other&lt;/li&gt;
      &lt;li&gt;extra stems
        &lt;ul&gt;
          &lt;li&gt;auxiliary classification outputs for training the lower layers
            &lt;ul&gt;
              &lt;li&gt;inject additional gradients/signal at earlier layers (in deeper NNs early layers get smaller gradients)&lt;/li&gt;
              &lt;li&gt;GoogleNet paper:
                &lt;ul&gt;
                  &lt;li&gt;Given the relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern. One interesting insight is that the strong performance of relatively shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative.&lt;/li&gt;
                  &lt;li&gt;&lt;strong&gt;By adding auxiliary classifiers&lt;/strong&gt; connected to these intermediate layers, we would expect
                    &lt;ul&gt;
                      &lt;li&gt;to encourage discrimination in the lower stages in the classifier,&lt;/li&gt;
                      &lt;li&gt;increase the gradient signal that gets propagated back, and&lt;/li&gt;
                      &lt;li&gt;provide additional regularization.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;These classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules.&lt;/li&gt;
                  &lt;li&gt;&lt;strong&gt;&lt;mark&gt;During training&lt;/mark&gt;, their loss gets added to the total loss of the network with a discount weight&lt;/strong&gt; (the losses of the auxiliary classifiers were weighted by 0.3).&lt;/li&gt;
                  &lt;li&gt;&lt;strong&gt;&lt;mark&gt;At inference time&lt;/mark&gt;, these auxiliary networks are discarded.&lt;/strong&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Average Pooling
        &lt;ul&gt;
          &lt;li&gt;GoogleNet paper:
            &lt;ul&gt;
              &lt;li&gt;The use of average pooling before the classifier is based on [12], although our implementation differs in that we use an extra linear layer. This &lt;strong&gt;enables adapting and fine-tuning our networks for other label sets easily&lt;/strong&gt;, but it is mostly convenience and we do not expect it to have a major effect. &lt;strong&gt;It was found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%&lt;/strong&gt;, however the use of dropout remained essential even after removing the fully connected layers.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;classifier (FC layer - Softmax)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;resnet&quot;&gt;ResNet&lt;/h2&gt;

&lt;h3 id=&quot;architecture&quot;&gt;Architecture&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;For more than 50 layers (ResNet-50+), use two &lt;strong&gt;bottleneck layers&lt;/strong&gt; (i.e. “1x1 - 3x3 - 1x1” instead of “3x3 - 3x3”) to improve efficiency (similar to GoogleNet)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;idea&lt;/strong&gt;: project the depth down (so that it is less expensive) and back up via the 1x1 convs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;batch normalization after every conv layer,&lt;/li&gt;
  &lt;li&gt;they use Xavier initialization with an extra scaling factor that they introduced to improve the initialization,&lt;/li&gt;
  &lt;li&gt;trained with SGD + momentum,&lt;/li&gt;
  &lt;li&gt;they use a similar &lt;strong&gt;learning rate&lt;/strong&gt; type of schedule where you decay your learning rate when your validation error plateaus [similar to AlexNet, but starts with 10 times larger learning rate]
    &lt;ul&gt;
      &lt;li&gt;the learning rate starts from 0.1 and is divided by 10 when the error plateaus&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Mini batch size 256,&lt;/li&gt;
  &lt;li&gt;a little bit of weight decay and&lt;/li&gt;
  &lt;li&gt;no drop out&lt;/li&gt;
  &lt;li&gt;double the number of filters and simultaneously downsample spatially using stride two (like VGGNet)&lt;/li&gt;
  &lt;li&gt;no FC layers (like GoogleNet)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;he-et-al&quot;&gt;He et al&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Plain nets: stacking $3\times 3$ Conv layers
    &lt;ul&gt;
      &lt;li&gt;56-layer net has higher &lt;strong&gt;training&lt;/strong&gt; error than 20-layer net
        &lt;ul&gt;
          &lt;li&gt;not caused by overfitting
            &lt;ul&gt;
              &lt;li&gt;when you are overfitting you would expect to have a very low training error rate and just bad test error&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;unlikely to be caused by vanishing gradients
            &lt;ul&gt;
              &lt;li&gt;plain nets are trained with BN, which ensures that forward propageted signals have non-zero variances
                &lt;ul&gt;
                  &lt;li&gt;i.e. forward signals do not vanish!&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;He et al verify that the gradients exhibit healthy norms
                &lt;ul&gt;
                  &lt;li&gt;i.e. backward signals do not vanish!&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;the solver works to some extent
            &lt;ul&gt;
              &lt;li&gt;“In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that &lt;strong&gt;the solver works&lt;/strong&gt; to some extent.”&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;deeper model has a richer solution space which should allow it to find better solutions (i.e. it should perform at least as well as a shallower model)
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;solution by construction&lt;/strong&gt;:
                &lt;ul&gt;
                  &lt;li&gt;copy the original layers from the shallower model and just set the extra layers as identity mappings
                    &lt;ul&gt;
                      &lt;li&gt;this model should be at least as well as the shallower model&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;this motivates the ResNet design:
                    &lt;ul&gt;
                      &lt;li&gt;fit a residual mapping instead of a direct mapping, so that it is &lt;strong&gt;easier for the deep NN to learn an identity mapping on unused layers&lt;/strong&gt; (“learn some Delta/residual” instead of “learn an arbitrary mapping”)
                        &lt;ul&gt;
                          &lt;li&gt;Why is learning a residual easier than learning the direct mapping?
                            &lt;ul&gt;
                              &lt;li&gt;This is not proven. It’s just He’s hypothesis.&lt;/li&gt;
                            &lt;/ul&gt;
                          &lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                      &lt;li&gt;so we can get something close to this “&lt;strong&gt;solution by construction&lt;/strong&gt;” that we had earlier&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;reason&lt;/strong&gt;: optimization difficulties
            &lt;ul&gt;
              &lt;li&gt;“deep plain nets may have &lt;strong&gt;exponentially low convergence rates&lt;/strong&gt;”
                &lt;ul&gt;
                  &lt;li&gt;d.h. der Solver funktioniert, aber er braucht einfach nur so lange, dass es so aussieht als würde er nicht konvergieren&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;veit-et-al&quot;&gt;Veit et al&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;“depth is good” (cf. He et al) is &lt;strong&gt;not the full explanation&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;because, if this were true, 1202 layers should have been better than 110 layers
        &lt;ul&gt;
          &lt;li&gt;however, accuracy stagnates above about 150 layers&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1605.06431.pdf&quot;&gt;Veit et al, 2016&lt;/a&gt; did some experiments on ImageNet and CIFAR-10&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternative-view-of-resnets&quot;&gt;Alternative View of ResNets&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Unraveling ResNets:
    &lt;ul&gt;
      &lt;li&gt;ResNets &lt;strong&gt;can be viewed as a collection of shorter paths through different subsets&lt;/strong&gt; of the layers&lt;/li&gt;
      &lt;li&gt;see “unraveled view” of a ResNet in slides&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;deleting-layers&quot;&gt;Deleting Layers&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Deleting layers:
    &lt;ul&gt;
      &lt;li&gt;VGG-Net:
        &lt;ul&gt;
          &lt;li&gt;when deleting a layer in &lt;strong&gt;VGG-Net&lt;/strong&gt;, it breaks down completely&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;ResNets:
        &lt;ul&gt;
          &lt;li&gt;in &lt;strong&gt;ResNets&lt;/strong&gt; deleting has almost no effect (except for pooling layers)
            &lt;ul&gt;
              &lt;li&gt;“something important” seems to happen in pooling layers&lt;/li&gt;
              &lt;li&gt;see “pooling layer spike” in slides&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;deleting an increasing number of layers increases the error smoothly&lt;/li&gt;
          &lt;li&gt;i.e. &lt;strong&gt;paths in a ResNet do not strongly depend on each other&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;gradient-contributions&quot;&gt;Gradient Contributions&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Gradient contribution of each path:
    &lt;ul&gt;
      &lt;li&gt;plot: “Pfadlänge” (= Anz. layers im Pfad) vs “Anz. Pfade mit dieser Pfadlänge” ist &lt;strong&gt;binomialverteilt&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;plot: “total gradient magnitude per path length”: Großteil des Gesamtgradientbetrages ist auf Pfade mit Länge 5-17 verteilt (das sind nur $0.45\%$ aller Pfade!)
            &lt;ul&gt;
              &lt;li&gt;d.h. &lt;strong&gt;die längeren Pfade tragen kaum zum learning bei!&lt;/strong&gt;
                &lt;ul&gt;
                  &lt;li&gt;man kann auch sagen “effectively only 5-17 active modules” (weil ResNet also effektiv wie ein 17 layer NN ist, oder genauer: ein ensemble von mehreren 17 layer NNs)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;only a small portion ($\lt 1\%$) of all paths&lt;/strong&gt; in the NN are used for passing the gradients
        &lt;ul&gt;
          &lt;li&gt;effectively &lt;strong&gt;only shallow paths&lt;/strong&gt; with 5-17 modules are used
            &lt;ul&gt;
              &lt;li&gt;i.e. it remains a hard problem to pass gradients through weight layers and apparently the optimum is in the range 5-17 weight layers&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;this &lt;strong&gt;explains why deleting layers has almost no effect&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;deleting only affects a subset of paths and the shorter paths are less likely to be affected than the longer paths&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;distribution of path lengths follows a Binomial distribution
        &lt;ul&gt;
          &lt;li&gt;where $\text{path length} = \text{#(weight layers along the path)}$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;resnet-as-ensemble-method&quot;&gt;ResNet as Ensemble Method&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;new interpretation of ResNet:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;ResNet can be viewed as an ensemble method&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;they create an ensemble of relatively &lt;strong&gt;shallow paths&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;making ResNet deeper increases the ensemble size
            &lt;ul&gt;
              &lt;li&gt;Recall ensemble learning: this &lt;strong&gt;explains why the accuracy stagnates&lt;/strong&gt; above about 150 layers or so
                &lt;ul&gt;
                  &lt;li&gt;there is only so much more that a new model (here: path) can add to the ensemble model accuracy&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;similarly &lt;strong&gt;Dropout&lt;/strong&gt; can be viewed as an ensemble method
            &lt;ul&gt;
              &lt;li&gt;however, deleting connections using &lt;strong&gt;Dropout can improve the performance of ResNet&lt;/strong&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;excluding longer paths does not negatively affect the results&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;cnn-training&quot;&gt;CNN Training&lt;/h1&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;This is often done in medical imaging and &lt;strong&gt;in situations where there is very little training data available&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;method&quot;&gt;Method&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;pretrain the network&lt;/li&gt;
  &lt;li&gt;fine-tuning:
    &lt;ul&gt;
      &lt;li&gt;little data available:
        &lt;ul&gt;
          &lt;li&gt;swap only the Softmax layer [d.h. letzter FC-1000 und Softmax] at the end and leave everything else unchanged&lt;/li&gt;
          &lt;li&gt;fine-tune this final Softmax layer based on the new data&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;medium sized data set available:
        &lt;ul&gt;
          &lt;li&gt;Option 1: retrain a &lt;strong&gt;bigger portion&lt;/strong&gt; of the network&lt;/li&gt;
          &lt;li&gt;Option 2: retrain the &lt;strong&gt;full&lt;/strong&gt; network, but use the &lt;strong&gt;old weights as initialization&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;restrict the step size, so that you allow only small changes in the early layers&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;additional-hyperparameters&quot;&gt;Additional Hyperparameters&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cs230.stanford.edu/files/cs230exam_fall18_soln.pdf&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The parameters you would need to choose are: 
  1) How many layers of the original network to keep. 
  2) How many new layers to introduce 
  3) How many of the layers of the original network would you want to keep frozen while fine tuning.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;joint-vs-separate-training&quot;&gt;Joint vs Separate training&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;[from Johnson, Classification &amp;amp; Localization:]
    &lt;ul&gt;
      &lt;li&gt;Why not train (1) AlexNet + Classification and then separately (2) AlexNet + Localization and merge those two then ?&lt;a name=&quot;multitask_hyperparam_problem&quot;&gt;&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;whenever you’re doing transfer learning you &lt;strong&gt;always get better performance if you fine tune the whole system jointly&lt;/strong&gt; because there’s probably some &lt;strong&gt;mismatch between the features&lt;/strong&gt;,
            &lt;ul&gt;
              &lt;li&gt;if you train on ImageNet and&lt;/li&gt;
              &lt;li&gt;then you use that network for your data set&lt;/li&gt;
              &lt;li&gt;you’re going to get better performance on your data set if you can also change the network.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;cnn-tasks&quot;&gt;CNN Tasks&lt;/h1&gt;

&lt;h2 id=&quot;classification--localization&quot;&gt;Classification &amp;amp; Localization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;C&amp;amp;L vs Object Detection:
    &lt;ul&gt;
      &lt;li&gt;in C&amp;amp;L the &lt;strong&gt;number of predicted bounding boxes is known ahead of time&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;in Object Detection (e.g. YOLO) we do not know how many objects will be predicted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-1&quot;&gt;Architecture&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AlexNet&lt;/li&gt;
  &lt;li&gt;2 FC layers:
    &lt;ul&gt;
      &lt;li&gt;class scores&lt;/li&gt;
      &lt;li&gt;$H\times  W\times x\times y$ bounding box&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;multi-task loss
    &lt;ul&gt;
      &lt;li&gt;(1) Softmax Loss and (2) Regression loss (e.g. L2, L1, …)&lt;/li&gt;
      &lt;li&gt;training images have &lt;strong&gt;2 labels&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;multi-task-learning&quot;&gt;Multi-task Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CS231n, Johnson:
    &lt;ul&gt;
      &lt;li&gt;take a &lt;strong&gt;weighted sum of these two different loss functions&lt;/strong&gt; to give our final scalar loss.&lt;/li&gt;
      &lt;li&gt;And then you’ll take your gradients with respect to this weighted sum of the two losses.&lt;/li&gt;
      &lt;li&gt;this &lt;strong&gt;weighting parameter&lt;/strong&gt; is a &lt;strong&gt;hyperparameter&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;but &lt;strong&gt;it’s kind of different from some of the other hyperparameters&lt;/strong&gt; that we’ve seen so far in the past right because this weighting hyperparameter actually changes the value of the loss function.&lt;/li&gt;
          &lt;li&gt;why not train both tasks separately (see &lt;a href=&quot;#multitask_hyperparam_problem&quot;&gt;multitask_hyperparam_problem&lt;/a&gt;), so that we do not have this weighting hyperparameter in the first place ?
            &lt;ul&gt;
              &lt;li&gt;training &lt;strong&gt;with&lt;/strong&gt; multi-task loss always performs better than training separate tasks&lt;/li&gt;
              &lt;li&gt;however, a trick in practice (similar to this idea):
                &lt;ul&gt;
                  &lt;li&gt;train heads separately (as suggested in the question) until convergence
                    &lt;ul&gt;
                      &lt;li&gt;AlexNet fixed&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;train whole system jointly&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;in practice, you kind of need to &lt;strong&gt;take it on a case by case basis&lt;/strong&gt; for exactly the problem you’re solving [d.h. muss man für jedes Problem neu überlegen, es gibt keine Regel]
        &lt;ul&gt;
          &lt;li&gt;[Johnson recommendation:] but my &lt;strong&gt;general strategy&lt;/strong&gt; for this is to have some other metric of performance that you care about other than the actual loss value which then you actually use that final performance metric to make your cross validation choices rather than looking at the value of the loss to make those choices.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;human-pose-estimation&quot;&gt;Human Pose Estimation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;via multi-task learning:
    &lt;ul&gt;
      &lt;li&gt;same framework as Classification &amp;amp; Localization&lt;/li&gt;
      &lt;li&gt;14 joint positions&lt;/li&gt;
      &lt;li&gt;regression loss on &lt;strong&gt;each&lt;/strong&gt; of those 14 joints (multi-task loss)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;via FCN (see &lt;a href=&quot;#fcn_human_pose&quot;&gt;fcn_human_pose&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;vs C &amp;amp; L:
    &lt;ul&gt;
      &lt;li&gt;you do not know ahead of time how many objects will be predicted&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PASCAL VOC (considered too easy nowadays)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: &lt;strong&gt;sliding window approach&lt;/strong&gt; is &lt;strong&gt;computationally intractable&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;i.e.
        &lt;ul&gt;
          &lt;li&gt;(1) Crop&lt;/li&gt;
          &lt;li&gt;(2) Classify with CNN&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;use region proposals (aka &lt;strong&gt;ROIs&lt;/strong&gt;) $\Rightarrow$ Region-based methods
        &lt;ul&gt;
          &lt;li&gt;hierzu gehören sowohl R-CNN methods als auch single-shot methods&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;variables:
    &lt;ul&gt;
      &lt;li&gt;base networks:
        &lt;ul&gt;
          &lt;li&gt;VGG&lt;/li&gt;
          &lt;li&gt;ResNet&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;metastrategy
        &lt;ul&gt;
          &lt;li&gt;region-based&lt;/li&gt;
          &lt;li&gt;single shot&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;hyperparameters
        &lt;ul&gt;
          &lt;li&gt;image size&lt;/li&gt;
          &lt;li&gt;number of region proposals to be used&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Comparison of different methods:
    &lt;ul&gt;
      &lt;li&gt;Faster R-CNN style methods: higher accuracy, slower&lt;/li&gt;
      &lt;li&gt;Single-Shot methods: less accurate, faster&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;r-cnn-methods&quot;&gt;R-CNN methods&lt;/h3&gt;

&lt;h4 id=&quot;r-cnn&quot;&gt;R-CNN&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Ross Girshick, Jeff Donahue, Jitendra Malik [&lt;a href=&quot;https://arxiv.org/pdf/1311.2524.pdf&quot;&gt;paper&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;(1) Crop using traditional region proposals (aka &lt;strong&gt;ROIs&lt;/strong&gt;)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Selective Search&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;speed: order of ~1000 proposals/sec&lt;/li&gt;
          &lt;li&gt;high recall&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(2) Warp to fixed size&lt;/li&gt;
  &lt;li&gt;(3) Classify with AlexNet CNN (multi-task)
    &lt;ul&gt;
      &lt;li&gt;classifier: (binary) SVM&lt;strong&gt;s&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;für jede class ein separates binary SVM (e.g. “Katze/keine Katze”), weil “sometimes you might wanna have &lt;strong&gt;one region have multiple positives&lt;/strong&gt;, be able to output “yes” on multiple classes for the same image region. And one way they do that is by training separate binary SVMs for each class” Johnson 2016 CS231n&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;regressor: correction to the bounding box given by Selective Search&lt;/li&gt;
      &lt;li&gt;zu (3):
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1311.2524.pdf&quot;&gt;paper&lt;/a&gt;:&lt;a name=&quot;RCNN_training&quot;&gt;&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;pre-training: train AlexNet on ILSVRC2012
                &lt;ul&gt;
                  &lt;li&gt;&lt;strong&gt;ohne&lt;/strong&gt; bounding-box labels (also kein multi-task training)&lt;/li&gt;
                  &lt;li&gt;using Caffe (was merged into PyTorch)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;fine-tuning:
                &lt;ul&gt;
                  &lt;li&gt;&lt;strong&gt;mit&lt;/strong&gt; bounding-box labels (also multi-task training)&lt;/li&gt;
                  &lt;li&gt;we continue stochastic gradient descent (SGD) training of &lt;strong&gt;the CNN parameters&lt;/strong&gt; [also nicht nur die SVMs] using only warped region proposals.&lt;/li&gt;
                  &lt;li&gt;Aside from replacing the CNN’s ImageNet-specific 1000-way classification layer with a randomly initialized (N + 1)-way classification layer (where N is the number of object classes, plus 1 for background), the CNN architecture is unchanged.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;problems&quot;&gt;Problems&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;at test time: slow (because forward passes for each of the ~1000 ROIs)&lt;/li&gt;
  &lt;li&gt;at training time: super slow (many forward and backward passes)&lt;/li&gt;
  &lt;li&gt;fixed region proposals, not learned&lt;/li&gt;
  &lt;li&gt;complex &lt;strong&gt;transfer learning&lt;/strong&gt; pipeline (cf. &lt;a href=&quot;#RCNN_training&quot;&gt;RCNN_training&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;CNN and FC layers are learned (sort of) separately “post-hoc”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;fast-r-cnn&quot;&gt;Fast R-CNN&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Ross Girshick&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;idea&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;end-to-end (aka jointly) trainable system&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;shared computation&lt;/strong&gt;&lt;a name=&quot;Fast_RCNN_shared_computation&quot;&gt;&lt;/a&gt;: sharing all this computation [of conv layers] between different “feature map ROI proposals” for an image
        &lt;ul&gt;
          &lt;li&gt;[d.h. nur ein forward pass / image statt 2000 / image].&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(1) Selective Search, but do not crop yet&lt;/li&gt;
  &lt;li&gt;(2) Conv Layers (AlexNet [CaffeNet from R-CNN paper], CNN-M-1024 [&lt;a href=&quot;https://arxiv.org/pdf/1405.3531.pdf&quot;&gt;paper&lt;/a&gt;], VGG16)&lt;/li&gt;
  &lt;li&gt;(3) Crop feature map obtained in (2) using ROIs obtained in (1)&lt;/li&gt;
  &lt;li&gt;(4) Warp to fixed size &lt;strong&gt;via ROI pooling layer&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;teile ROI in grid cells auf&lt;/li&gt;
      &lt;li&gt;max pooling auf jede grid cell, &lt;strong&gt;sodass Output die passende Größe für das FC layer hat&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(5) FC layers
    &lt;ul&gt;
      &lt;li&gt;Classifier&lt;/li&gt;
      &lt;li&gt;Regressor&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;comparison-with-r-cnn&quot;&gt;Comparison with R-CNN&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;at training time fast R-CNN is something like 10 times faster to train
    &lt;ul&gt;
      &lt;li&gt;because of &lt;a href=&quot;#Fast_RCNN_shared_computation&quot;&gt;Fast_RCNN_shared_computation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;at test time its computation time is actually dominated [“bottlenecked”] by computing region proposals&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;problems-1&quot;&gt;Problems&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;still relies on fixed region proposals from &lt;strong&gt;Selective Search&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Selective Search is a bottleneck&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;faster-r-cnn&quot;&gt;Faster R-CNN&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun&lt;/li&gt;
  &lt;li&gt;$\sim 7-18$ fps
    &lt;ul&gt;
      &lt;li&gt;YOLO paper:
        &lt;ul&gt;
          &lt;li&gt;The recent Faster R-CNN replaces selective search with a neural network to propose bounding boxes, similar to Szegedy et al. [8] In our tests, their most accurate model achieves 7 fps while a smaller, less accurate one runs at 18 fps. The VGG-16 version of Faster R-CNN is 10 mAP higher but is also 6 times slower than YOLO. The Zeiler-Fergus Faster R-CNN is only 2.5 times slower than YOLO but is also less accurate.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;detects small objects in groups well (better than YOLO) since it uses nine anchors per grid cell&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;idea&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;make the network itself predict its own region proposals
        &lt;ul&gt;
          &lt;li&gt;use &lt;strong&gt;region proposal network (RPN)&lt;/strong&gt; which works on top of those convolutional features and predicts its own region proposals inside the network&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;4 losses&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ACHTUNG:
    &lt;ul&gt;
      &lt;li&gt;wurde in 4 &lt;strong&gt;aufeinander aufbauenden&lt;/strong&gt; Schritten trainiert (in denen 4 mal die Architektur geändert wurde) (s. unten “4-Step Alternating Training”)&lt;/li&gt;
      &lt;li&gt;“joint training” auch möglich und training schneller, aber ist nur eine Näherung (da bestimmte Gradienten schwer zu berücksichtigen sind und deshalb ignoriert werden)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;2 modules [source: &lt;a href=&quot;https://arxiv.org/pdf/1506.01497.pdf&quot;&gt;paper&lt;/a&gt;]:
    &lt;ul&gt;
      &lt;li&gt;&lt;mark&gt;(module 1) RPN&lt;/mark&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;mark&gt;input&lt;/mark&gt;: image (of any size)&lt;/li&gt;
          &lt;li&gt;(1) ZFNet or VGG-16&lt;/li&gt;
          &lt;li&gt;(2) 3 × 3 sliding window over feature map of last &lt;strong&gt;shared&lt;/strong&gt; conv layer [shared by RPN and Fast R-CNN]
            &lt;ul&gt;
              &lt;li&gt;Each sliding window is mapped to a lower-dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;(3) This feature is fed into &lt;strong&gt;two sibling fully-connected layers&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;a box-regression layer (reg) and&lt;/li&gt;
              &lt;li&gt;a box-classification layer (cls).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;mark&gt;2 outputs&lt;/mark&gt;:
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;binary classification&lt;/strong&gt; ($2\cdot9$ scores)
                &lt;ul&gt;
                  &lt;li&gt;nur “object or not object” (in paper: “&lt;strong&gt;objectness&lt;/strong&gt;”, d.h. verschiedene Objekte werden hier &lt;strong&gt;nicht&lt;/strong&gt; unterschieden, sondern nur ihre Existenz gemessen)&lt;/li&gt;
                  &lt;li&gt;$2$ scores, weil sie Softmax benutzen statt logistic regression. Mit logistic regression wären es nur $9$ scores statt $2\cdot 9$!&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;anchor box regression&lt;/strong&gt; ($4\cdot9$ coordinates)
                &lt;ul&gt;
                  &lt;li&gt;maximum number of possible proposals for each sliding-window location: 9 boxes (3 aspect ratios and 3 scales)
                    &lt;ul&gt;
                      &lt;li&gt;For a convolutional feature map of a size $W × H$ (typically $\sim 2\,400$), there are $WHk$ anchors in total.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;translation invariant
                    &lt;ul&gt;
                      &lt;li&gt;“If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location”&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;Multi-scale
                    &lt;ul&gt;
                      &lt;li&gt;“The design of multi-scale anchors is a key component for sharing features without extra cost for addressing scales”&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;mark&gt;(module 2) Fast R-CNN&lt;/mark&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;3-ways-for-training&quot;&gt;3 Ways for Training&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;3 ways for training:
    &lt;ul&gt;
      &lt;li&gt;alternating training
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;4-Step Alternating Training.&lt;/strong&gt; In this paper, we adopt a pragmatic 4-step training algorithm to &lt;strong&gt;learn shared features&lt;/strong&gt; via alternating optimization.
            &lt;ul&gt;
              &lt;li&gt;(1) In the first step, we &lt;strong&gt;train the RPN&lt;/strong&gt; as described in Section 3.1.3. This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task.&lt;/li&gt;
              &lt;li&gt;(2) In the second step, we train a separate detection network by &lt;strong&gt;Fast R-CNN using the proposals generated by the step-1 RPN&lt;/strong&gt;. This detection network is also initialized by the ImageNet-pre-trained model. At this point the two networks do not share convolutional layers.&lt;/li&gt;
              &lt;li&gt;(3) In the third step, we &lt;strong&gt;use the detector network to initialize RPN training&lt;/strong&gt;, but we &lt;strong&gt;fix the shared convolutional layers&lt;/strong&gt; and only &lt;strong&gt;fine-tune the layers unique to RPN&lt;/strong&gt;. Now the two networks share convolutional layers.&lt;/li&gt;
              &lt;li&gt;(4) Finally, keeping the shared convolutional layers fixed, we &lt;strong&gt;fine-tune the unique layers of Fast R-CNN&lt;/strong&gt;. As such, both networks share the same convolutional layers and form a unified network.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;A similar alternating training can be run for more iterations, but we have observed negligible improvements.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Approximate joint training (“just works, though”, why approximate: &lt;a href=&quot;https://stackoverflow.com/a/68729051&quot;&gt;see here&lt;/a&gt;)
        &lt;ul&gt;
          &lt;li&gt;But this solution ignores the derivative w.r.t. the proposal boxes’ coordinates that are also network responses, so is approximate.&lt;/li&gt;
          &lt;li&gt;In our experiments, we have empirically found this solver produces &lt;strong&gt;close results&lt;/strong&gt;, yet &lt;strong&gt;reduces the training time&lt;/strong&gt; by about 25-50% comparing with alternating training.&lt;/li&gt;
          &lt;li&gt;This solver is included in our &lt;strong&gt;released Python code&lt;/strong&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Non-approximate joint training
        &lt;ul&gt;
          &lt;li&gt;This is a nontrivial problem and a solution can be given by an “RoI warping” layer as developed in [15], which is beyond the scope of this paper.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;mask-r-cnn&quot;&gt;Mask R-CNN&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;used for
    &lt;ul&gt;
      &lt;li&gt;Object Detection
        &lt;ul&gt;
          &lt;li&gt;by using Faster R-CNN&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#Instance_Segmentation&quot;&gt;Instance Segmentation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;by using an &lt;strong&gt;additional head&lt;/strong&gt; that predicts a segmentation mask for each region proposal&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Pose Estimation
        &lt;ul&gt;
          &lt;li&gt;by using an &lt;strong&gt;additional head&lt;/strong&gt; that predicts the coordinates of the joints of the instance&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;speed: $\sim 5$ fps&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;single-shot-methods&quot;&gt;Single-Shot methods&lt;/h3&gt;

&lt;h4 id=&quot;yolo&quot;&gt;YOLO&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi&lt;/li&gt;
  &lt;li&gt;$\sim 45$ fps (“Fast YOLO”: $\sim 155$ fps)&lt;/li&gt;
  &lt;li&gt;$7\times 7\times (5B+C)$ tensor
    &lt;ul&gt;
      &lt;li&gt;also nur C conditional class probabilities pro grid cell und &lt;strong&gt;nicht&lt;/strong&gt; pro box!
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Grund&lt;/strong&gt;: at test time werden diese conditional class probabilities mit den confidence scores der jeweiligen box multipliziert, sodass man die “class-specific confidence scores for each box” kriegt
            &lt;ul&gt;
              &lt;li&gt;“These scores encode both the probability of that class appearing in the box and how well the predicted box fits the object.”&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;problems&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;has problems with detecting small objects in groups
        &lt;ul&gt;
          &lt;li&gt;because YOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;does not generalize well when objects in the image have rare aspect ratios&lt;/li&gt;
      &lt;li&gt;loss function treats errors the same in small bounding boxes versus large bounding boxes
        &lt;ul&gt;
          &lt;li&gt;small error in a small box has a much greater effect on IOU&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;YOLO struggles to localize objects correctly.
        &lt;ul&gt;
          &lt;li&gt;Localization errors account for more of YOLO’s errors than all other sources combined.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pros&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;produces fewer false positives on “background” than Faster R-CNN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;confidence-score&quot;&gt;Confidence Score&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/iou-a-better-detection-evaluation-metric-45a511185be1&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The confidence score indicates
        &lt;ol&gt;
          &lt;li&gt;how sure the model is that the box contains an object [$=P(\text{“box contains object”})$] and also&lt;/li&gt;
          &lt;li&gt;how accurate it thinks the box is that [the model] predicts [$= \text{IoU}$].&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;The confidence score can be calculated using the formula:
        &lt;ul&gt;
          &lt;li&gt;$C = P(\text{“box contains object”}) \cdot \text{IoU}$
            &lt;ul&gt;
              &lt;li&gt;[Formel stimmt s. paper]&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$\text{IoU}$: Intersection over Union between the predicted box and the ground truth.&lt;/li&gt;
      &lt;li&gt;If no object exists in a cell, its confidence score should be $0$.&lt;/li&gt;
      &lt;li&gt;a certain confidence (or only the IoU) will be used as threshold for “&lt;strong&gt;Non-max suppression&lt;/strong&gt;” (see here)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;comparison-with-r-cnn-1&quot;&gt;Comparison with R-CNN&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;YOLO paper:
    &lt;ul&gt;
      &lt;li&gt;YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features.&lt;/li&gt;
      &lt;li&gt;However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object&lt;/li&gt;
      &lt;li&gt;produces fewer false positives on “background” than Faster R-CNN
        &lt;ul&gt;
          &lt;li&gt;YOLO makes far fewer background mistakes than Fast R-CNN.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;combine YOLO and Fast R-CNN&lt;/strong&gt;:
            &lt;ul&gt;
              &lt;li&gt;By using YOLO to eliminate background detections from Fast R-CNN we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;YOLO struggles to localize objects correctly. Localization errors account for more of YOLO’s errors than all other sources combined.&lt;/li&gt;
      &lt;li&gt;Fast R-CNN makes much fewer localization errors but far more background errors.
        &lt;ul&gt;
          &lt;li&gt;13.6% of its top detections are false positives that do not contain any objects.&lt;/li&gt;
          &lt;li&gt;Fast R-CNN is almost 3x more likely to predict background detections than YOLO.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ssd&quot;&gt;SSD&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Solution for YOLOs “small objects in groups” problem&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-segmentation&quot;&gt;Semantic Segmentation&lt;/h2&gt;

&lt;h3 id=&quot;semantic-vs-instance-segmentation&quot;&gt;Semantic vs Instance Segmentation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;semantic&lt;/strong&gt; segmentation does not differentiate instances
    &lt;ul&gt;
      &lt;li&gt;e.g. “cow 1” and “cow 2” are just “cow”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;instance&lt;/strong&gt; segmentation does&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;naive-idea-1-sliding-window-approach&quot;&gt;Naive Idea 1: Sliding Window approach&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;extract patch, run patch through AlexNet and classify center pixel&lt;/li&gt;
  &lt;li&gt;in theory, this would work&lt;/li&gt;
  &lt;li&gt;very inefficient
    &lt;ul&gt;
      &lt;li&gt;no reuse of shared features between overlapping patches&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;does not scale with input size&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;naive-idea-2-fcn-which-preserves-the-input-size&quot;&gt;Naive Idea 2: FCN which preserves the input size&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;in order to take into account a large receptive field of the input image while computing features at different NN layers one needs
    &lt;ul&gt;
      &lt;li&gt;either &lt;strong&gt;large filters&lt;/strong&gt; (&lt;mark&gt;which is done here&lt;/mark&gt;)&lt;/li&gt;
      &lt;li&gt;or &lt;strong&gt;many different layers&lt;/strong&gt; to get a sufficiently large receptive field&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;too expensive&lt;/li&gt;
  &lt;li&gt;produces $C\times H\times W$ output ($C=#(\text{classes})$) &lt;strong&gt;like Shelhamer FCN&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;or $H\times W$ segmented image (which is the $\argmax$ of the $C\times H\times W$ output)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fcn-long-shelhamer-darrell&quot;&gt;FCN (Long, Shelhamer, Darrell)&lt;/h3&gt;

&lt;h4 id=&quot;overview&quot;&gt;Overview&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;paper discussion: &lt;a href=&quot;https://www.youtube.com/watch?v=S4kgpV9PbRk&amp;amp;list=PLkZG1WV2RkYJm6FtNO29AxM64Nib1-eIX&amp;amp;index=47&amp;amp;t=754s&quot;&gt;part1&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=dildx4MDj9Y&quot;&gt;part2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;paper chapter 4:
    &lt;ul&gt;
      &lt;li&gt;We &lt;strong&gt;cast ILSVRC classifiers into FCNs&lt;/strong&gt; and augment them for dense prediction with in-network upsampling and a pixelwise loss.&lt;/li&gt;
      &lt;li&gt;We train for segmentation &lt;strong&gt;by fine-tuning&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Next, we build a novel &lt;strong&gt;skip architecture&lt;/strong&gt; that combines coarse, semantic and local, appearance information to refine prediction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;segmentation training data is more expensive than classification data
    &lt;ul&gt;
      &lt;li&gt;less data available
        &lt;ul&gt;
          &lt;li&gt;use transfer learning
            &lt;ul&gt;
              &lt;li&gt;in paper: ImageNet NN: AlexNet, VGG-16, GoogleNet&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;goal&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;process arbitrarily sized inputs&lt;/li&gt;
      &lt;li&gt;reuse shared features between overlapping patches&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;used for
    &lt;ul&gt;
      &lt;li&gt;Semantic Segmentation&lt;/li&gt;
      &lt;li&gt;Human Pose estimation&lt;a name=&quot;fcn_human_pose&quot;&gt;&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;heatmap for each joint’s position
            &lt;ul&gt;
              &lt;li&gt;e.g. right ankle, right knee, right hip, … etc.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;produce a &lt;strong&gt;heatmap&lt;/strong&gt; of class labels (i.e. a $(C\times H\times W)$ tensor, where $C$ is the number of classes and $H\times W$ are the input image dimensions) for each pixel in the input image&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.stackexchange.com/questions/21810/what-is-a-fully-convolution-network&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;A &lt;strong&gt;fully convolutional network (FCN)&lt;/strong&gt; is a neural network that only performs convolution (and subsampling or upsampling) operations. Equivalently, an FCN is a CNN without fully connected layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;advantages&quot;&gt;Advantages&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-are-the-advantages-of-Fully-Convolutional-Networks-over-CNNs&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Variable input image size&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;If you don’t have any fully connected layer in your network, you can apply the network to images of virtually any size. Because only the fully connected layer expects inputs of a certain size, which is why in architectures like AlexNet, you must provide input images of a certain size (224x224).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Preserve spatial information&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Fully connected layer generally causes loss of spatial information - because its “fully connected”: all output neurons are connected to all input neurons. This kind of architecture can’t be used for segmentation, if you are working in a huge space of possibilities (e.g. unconstrained real images &lt;a href=&quot;#Long_Shelhamer&quot;&gt;Long_Shelhamer&lt;/a&gt;).
            &lt;ul&gt;
              &lt;li&gt;[&lt;strong&gt;FC layers for segmentation&lt;/strong&gt;] Although fully connected layers can still do segmentation if you are restricted to a relatively smaller space e.g. a handful of object categories with limited visual variation, such that the FC activations may act as a sufficient statistic for those images [&lt;a href=&quot;#Kulkarni_Whitney&quot;&gt;Kulkarni_Whitney&lt;/a&gt;, &lt;a href=&quot;#Dosovitskiy_Springenberg&quot;&gt;Dosovitskiy_Springenberg&lt;/a&gt;]. In the latter case, the FC activations are enough to encode both the object type and its spatial arrangement. Whether one or the other happens depends upon the capacity of the FC layer as well as the loss function.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Long_Shelhamer&quot;&gt;Long_Shelhamer&lt;/a&gt; paper chapter 3.1:
    &lt;ul&gt;
      &lt;li&gt;The &lt;strong&gt;spatial output maps&lt;/strong&gt; of these convolutionalized models make them a &lt;strong&gt;natural choice for&lt;/strong&gt; dense problems like &lt;strong&gt;semantic segmentation&lt;/strong&gt;. With ground truth available at every output cell, both the forward and backward passes are straightforward, and both take advantage of the inherent computational efficiency (and aggressive optimization) of convolution.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convolutionalization&quot;&gt;Convolutionalization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=XdsmlBGOK-k&quot;&gt;Andrew Ng lecture&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;FCNs can be viewed as performing a sliding-window classification and produce a heatmap of output scores for each class&lt;/li&gt;
      &lt;li&gt;“&lt;strong&gt;convolutionalization&lt;/strong&gt; of FC layers makes FCNs &lt;strong&gt;more efficient than standard CNNs&lt;/strong&gt;&lt;a name=&quot;FCN_efficiency&quot;&gt;&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;computations are reused between the sliding windows
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#Long_Shelhamer&quot;&gt;Long_Shelhamer&lt;/a&gt; paper chapter 3.1:
                &lt;ul&gt;
                  &lt;li&gt;Furthermore, while the resulting maps are equivalent to the evaluation of the original net on particular input patches, the &lt;strong&gt;computation is highly amortized over the overlapping regions&lt;/strong&gt; of those patches.
                    &lt;ul&gt;
                      &lt;li&gt;For example, while &lt;strong&gt;AlexNet&lt;/strong&gt; takes 1.2 ms (on a typical GPU) to infer the classification scores of a 227×227 image, the &lt;strong&gt;fully convolutional net&lt;/strong&gt; takes 22 ms to produce a 10×10 grid of outputs from a 500×500 image, which is more than 5 times faster than the naive approach [footnote with technical details].&lt;/li&gt;
                      &lt;li&gt;The corresponding &lt;strong&gt;backward times&lt;/strong&gt; for the AlexNet example are 2.4 ms for a single image and 37 ms for a fully convolutional 10 × 10 output map, resulting in a speedup similar to that of the forward pass.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Long_Shelhamer&quot;&gt;Long_Shelhamer&lt;/a&gt; paper chapter 4.1:
    &lt;ul&gt;
      &lt;li&gt;We begin by convolutionalizing proven classification architectures as in Section 3.&lt;/li&gt;
      &lt;li&gt;We consider
        &lt;ul&gt;
          &lt;li&gt;the AlexNet architecture [19] that won ILSVRC12,&lt;/li&gt;
          &lt;li&gt;as well as the VGG nets [31] and&lt;/li&gt;
          &lt;li&gt;the GoogLeNet [32] which did exceptionally well in ILSVRC14.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;We pick the VGG 16-layer net, which we found to be equivalent to the 19-layer net on this task.&lt;/li&gt;
      &lt;li&gt;For GoogLeNet, we use only the final loss layer, and improve performance by discarding the final average pooling layer.&lt;/li&gt;
      &lt;li&gt;We &lt;strong&gt;decapitate each net&lt;/strong&gt; by discarding the final classifier layer, and &lt;strong&gt;convert all fully connected layers to convolutions&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;We append a 1 × 1 convolution with &lt;strong&gt;channel dimension 21&lt;/strong&gt; to predict scores for each of the PASCAL classes (including background) at each of the coarse output locations,&lt;/li&gt;
      &lt;li&gt;followed by a &lt;strong&gt;deconvolution layer&lt;/strong&gt; to &lt;strong&gt;bilinearly upsample&lt;/strong&gt; the coarse outputs to pixel-dense outputs as described in Section 3.3.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.stackexchange.com/questions/21810/what-is-a-fully-convolution-network&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The typical &lt;strong&gt;convolutional neural network (CNN)&lt;/strong&gt; is &lt;strong&gt;not fully convolutional&lt;/strong&gt; because it often contains &lt;strong&gt;fully connected layers&lt;/strong&gt; too (which do not perform the convolution operation), which are &lt;strong&gt;parameter-rich&lt;/strong&gt;, in the sense that they have many parameters (compared to their equivalent convolution layers),
        &lt;ul&gt;
          &lt;li&gt;although the fully connected layers &lt;a href=&quot;https://arxiv.org/pdf/1411.4038.pdf&quot;&gt;can also be viewed as convolutions with kernels that cover the entire input regions&lt;/a&gt;, which is the main idea behind converting a CNN to an FCN.
            &lt;ul&gt;
              &lt;li&gt;See &lt;a href=&quot;https://www.youtube.com/watch?v=XdsmlBGOK-k&quot;&gt;this video&lt;/a&gt; by Andrew Ng that explains how to convert a fully connected layer to a convolutional layer.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learnable-upsampling&quot;&gt;Learnable Upsampling&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;deconvolution (aka fractionally strided convolution, transpose convolution, backwards convolution, etc.)&lt;/li&gt;
  &lt;li&gt;paper chapter 3.3&lt;/li&gt;
  &lt;li&gt;Johnson CS231n 2016:
    &lt;ul&gt;
      &lt;li&gt;this is equivalent to convolution operation from the backward pass&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;skip-connections&quot;&gt;Skip Connections&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;paper chapter 4.2 [im Prinzip das, was Johnson sagt]&lt;/li&gt;
  &lt;li&gt;Johnson CS231n 2016:
    &lt;ul&gt;
      &lt;li&gt;so they actually don’t use only just these pool5 features&lt;/li&gt;
      &lt;li&gt;they actually &lt;strong&gt;use the convolutional features from different layers&lt;/strong&gt; in the network which sort of exist at different scales&lt;/li&gt;
      &lt;li&gt;so you can imagine that once you’re in the pool4 layer of AlexNet that’s actually a bigger feature map than the pool5 and pool3 is even bigger than pool4&lt;/li&gt;
      &lt;li&gt;so the intuition is that these lower convolutional layers might actually help you &lt;strong&gt;capture finer grained structure in the input image&lt;/strong&gt; since they have a &lt;strong&gt;smaller receptive field&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;take these different convolutional feature maps and apply a &lt;strong&gt;separate learned upsampling to each of these feature maps&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;then combine them all to produce the final output&lt;/li&gt;
      &lt;li&gt;[paper results:]
        &lt;ul&gt;
          &lt;li&gt;in their results they show that adding these skip connections tends to help a lot with these low-level details&lt;/li&gt;
          &lt;li&gt;so over here on the left these are the results that only use these pool5 outputs&lt;/li&gt;
          &lt;li&gt;you can see that it’s sort of gotten the rough idea of a person on a bicycle but it’s kind of blobby and missing a lot of the fine details around the edges&lt;/li&gt;
          &lt;li&gt;but then, when you add in these skip connections from these lower convolutional layers, that gives you a lot more fine-grained information about the spatial locations of things in the image&lt;/li&gt;
          &lt;li&gt;so, adding those skip connections from the lower layers really helps you clean up the boundaries in some cases for these outputs&lt;/li&gt;
          &lt;li&gt;&lt;img src=&quot;/home/assets/images/ML_part3/effect_of_fcn_skip_connections.png&quot; alt=&quot;effect_of_fcn_skip_connections.png&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;alternative-view-of-fc-layers&quot;&gt;Alternative View of FC layers&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Long_Shelhamer&quot;&gt;Long_Shelhamer&lt;/a&gt; paper chapter 3.1:
    &lt;ul&gt;
      &lt;li&gt;Typical recognition nets, including &lt;strong&gt;LeNet&lt;/strong&gt; [21], &lt;strong&gt;AlexNet&lt;/strong&gt; [20], and its &lt;strong&gt;deeper successors&lt;/strong&gt; [31, 32], ostensibly take fixed-sized inputs and &lt;strong&gt;produce non-spatial outputs&lt;/strong&gt;.
        &lt;ul&gt;
          &lt;li&gt;The fully connected layers of these nets have fixed dimensions and throw away spatial coordinates.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;However, these &lt;strong&gt;fully connected layers can also be viewed as convolutions with kernels that cover their entire input regions&lt;/strong&gt;. Doing so casts them into fully convolutional networks that take input of any size and output classification maps. This transformation is illustrated in Figure 2.
        &lt;ul&gt;
          &lt;li&gt;Figure 2: Transforming fully connected layers into convolution layers enables a classification net to &lt;strong&gt;output a heatmap&lt;/strong&gt;. Adding layers and a &lt;strong&gt;spatial loss&lt;/strong&gt; (as in Figure 1) produces an &lt;strong&gt;efficient&lt;/strong&gt; machine for end-to-end dense learning.
            &lt;ul&gt;
              &lt;li&gt;[&lt;strong&gt;spatial loss&lt;/strong&gt;:] If the loss function is a &lt;strong&gt;sum over the spatial dimensions&lt;/strong&gt; of the final layer, $l(\mathbf{x}, \theta) = \sum_{ij}l^\prime(\mathbf{x}_{ij}, \theta)$, its gradient will be a sum over the gradients of each of its spatial components. Thus stochastic gradient descent on $l$ computed on whole images will be the same as stochastic gradient descent on $l^\prime$, taking all of the final layer receptive fields as a minibatch.&lt;/li&gt;
              &lt;li&gt;[&lt;strong&gt;efficient&lt;/strong&gt;:] When these receptive fields overlap significantly, both feedforward computation and backpropagation are much more efficient when computed layer-by-layer over an entire image instead of independently patch-by-patch (cf. &lt;a href=&quot;#FCN_efficiency&quot;&gt;FCN_efficiency&lt;/a&gt;).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[can be thought of as performing a sliding-window classification producing a heatmap &lt;strong&gt;per class&lt;/strong&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/ML_part3/fcn_heatmap.png&quot; alt=&quot;fcn_heatmap.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;encoder-decoder-architecture&quot;&gt;Encoder-Decoder Architecture&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;used for
    &lt;ul&gt;
      &lt;li&gt;Semantic Segmentation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;problem&lt;/strong&gt;: FCN output has low resolution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;solution&lt;/strong&gt;: perform upsampling to get back to the desired resolution
    &lt;ul&gt;
      &lt;li&gt;use skip connections to &lt;strong&gt;preserve higher resolution information&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;learnable-upsampling-1&quot;&gt;Learnable Upsampling&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Unpooling
    &lt;ul&gt;
      &lt;li&gt;Nearest-Neighbor Unpooling&lt;/li&gt;
      &lt;li&gt;“Bed of Nails” Unpooling&lt;/li&gt;
      &lt;li&gt;Max Unpooling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transpose Convolution&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;skip-connections-1&quot;&gt;Skip Connections&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;problem&lt;/strong&gt;: downsampling loses high-resolution information&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;solution&lt;/strong&gt;: use skip connections to preserve this information&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;instance-segmentation&quot;&gt;Instance Segmentation&lt;a name=&quot;Instance_Segmentation&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Instance Segmentation:
    &lt;ul&gt;
      &lt;li&gt;paperswithcode:
        &lt;ul&gt;
          &lt;li&gt;“Instance segmentation is the task of detecting and delineating [einzeichnen] each distinct object of interest appearing in an image.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pipeline approaches that look a bit like Object Detection methods&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;embeddings-in-vision-and-siamese-networks&quot;&gt;Embeddings in Vision and Siamese Networks&lt;/h2&gt;

&lt;h3 id=&quot;digression-n-gram-model&quot;&gt;Digression: N-gram model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Number of N-grams in a sentence with X words: $X-(N-1)$&lt;/li&gt;
  &lt;li&gt;text and speech data: “corpora”&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;word-probability-markov-assumption&quot;&gt;Word Probability (Markov Assumption)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;probability of word $W_T$ given the history of &lt;strong&gt;all&lt;/strong&gt; previous words (Chain rule):
    &lt;ul&gt;
      &lt;li&gt;$P(W_T\vert W_1,\ldots,W_{T-1})=\frac{P(W_1,\ldots,W_T)}{P(W_1,\ldots,W_{T-1})}$
        &lt;ul&gt;
          &lt;li&gt;&lt;img src=&quot;/home/assets/images/ML_part3/markov_assumption.png&quot; alt=&quot;markov_assumption.png&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;problem&lt;/strong&gt;: we cannot keep track of complete history of all previous words
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;solution&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;use &lt;strong&gt;Markov assumption&lt;/strong&gt;:
            &lt;ul&gt;
              &lt;li&gt;$P(W_T\vert W_1,\ldots,W_{T-1})\approx P(W_T\vert W_{k-N+1},\ldots,W_{T-1})$
                &lt;ul&gt;
                  &lt;li&gt;d.h. nur die $k-N+1$ letzten Wörter benutzen, z.B. für $N=2$ (bigram): $P(W_k\vert W_{k-1},\ldots,W_1)\approx P(W_k\vert W_{k-1})$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;problems-2&quot;&gt;Problems&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;problem 1: &lt;strong&gt;scalability&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;possible combinations and thus also the required data increases &lt;strong&gt;exponentially&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;problem 2: &lt;strong&gt;partial observability&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;probability is not zero just because the count is zero
        &lt;ul&gt;
          &lt;li&gt;Solution 1: need to back-off to $N-1$-grams when the count for $N$-gram is too small
            &lt;ul&gt;
              &lt;li&gt;see &lt;a href=&quot;#katz_smoothing&quot;&gt;katz_smoothing&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Solution 2: use &lt;a href=&quot;#smoothing&quot;&gt;smoothing&lt;/a&gt; (compensate for uneven sampling frequencies)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;smoothing-methods&quot;&gt;Smoothing Methods&lt;a name=&quot;smoothing&quot;&gt;&lt;/a&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;add-one smoothing
    &lt;ul&gt;
      &lt;li&gt;Laplace’s, Lidstone’s and Jeffreys-Perks’ laws&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Witten-Bell smoothing&lt;/li&gt;
  &lt;li&gt;Good-Turing estimation&lt;/li&gt;
  &lt;li&gt;Katz smoothing (Back-off Estimator)&lt;a name=&quot;katz_smoothing&quot;&gt;&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;if N-gram not found, then back-off to N-1-gram&lt;/li&gt;
      &lt;li&gt;if N-1-gram not found, then back-off to N-2-gram&lt;/li&gt;
      &lt;li&gt;… etc.&lt;/li&gt;
      &lt;li&gt;until you reach a model that has non-zero count&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Linear Interpolation
    &lt;ul&gt;
      &lt;li&gt;use a weighted sum of unigram, bigram and trigram probabilities
        &lt;ul&gt;
          &lt;li&gt;because each model may contain useful information&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kneser-Ney smoothing
    &lt;ul&gt;
      &lt;li&gt;Wiki: “It is widely considered the most effective method of smoothing due to its use of absolute discounting by subtracting a fixed value from the probability’s lower order terms to omit n-grams with lower frequencies. This approach has been considered equally effective for both higher and lower order n-grams.”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;digression-word-embeddings-and-nplm&quot;&gt;Digression: Word Embeddings and NPLM&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;watch:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=jQTuRnjJzBU&quot;&gt;Word Representation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yXV_Torwzyc&quot;&gt;Learning word embeddings&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;neural-probabilistic-language-model-bengio-2003&quot;&gt;Neural Probabilistic Language Model (Bengio 2003)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Neural Probabilistic Language Model (Bengio 2003)
    &lt;ul&gt;
      &lt;li&gt;[aka NPLM, NNLM]&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wqUgMFMnlzI&quot;&gt;Hinton explanation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;for each $\mathbf{x}$ only one row of $\mathbf{W}_{V\times d}$ is needed
        &lt;ul&gt;
          &lt;li&gt;$\mathbf{W}_{V\times d}$ is effectively a &lt;strong&gt;look-up table&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;$V\approx 1\text{M}$ (size of the vocabulary, i.e. $\mathbf{x}$ is a one-hot encoded 1M-dimensional vector!)&lt;/li&gt;
          &lt;li&gt;$d\in (50,300)$ (feature vector size)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Hinton:
        &lt;ul&gt;
          &lt;li&gt;one extra refinement that makes it work better is to use &lt;strong&gt;skip-layer connections&lt;/strong&gt; that go straight from the input words to the output words because the individual input words are individually quite informative about what the output words might be.&lt;/li&gt;
          &lt;li&gt;Bengio’s model initially performed only slightly worse than n-gram models, but combined with a tri-gram model it performed much better.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;learning-a-word-embedding&quot;&gt;Learning a Word Embedding&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Ways to &lt;strong&gt;learn a word embedding&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;word2vec [Mikolov, Jeffrey Dean (Google) 2013]
        &lt;ul&gt;
          &lt;li&gt;paper: [beide sind designed um &lt;strong&gt;complexity zu minimieren&lt;/strong&gt;; beide haben &lt;strong&gt;keine nonlinearity&lt;/strong&gt; im hidden layer, um sie besser mit großen Datensätzen trainieren zu können]
            &lt;ul&gt;
              &lt;li&gt;In this section, we propose two new model architectures for learning distributed representations of words that try &lt;strong&gt;to minimize computational complexity&lt;/strong&gt;.&lt;/li&gt;
              &lt;li&gt;The main observation from the previous section was that &lt;strong&gt;most of the complexity is caused by the non-linear hidden layer&lt;/strong&gt; in the model.&lt;/li&gt;
              &lt;li&gt;While this is what makes neural networks so attractive, we decided to explore &lt;strong&gt;simpler models&lt;/strong&gt; that might not be able to represent the data as precisely as neural networks, but &lt;strong&gt;can possibly be trained on much more data efficiently&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;CBOW [“bag” because &lt;strong&gt;order&lt;/strong&gt; of context words does &lt;strong&gt;not&lt;/strong&gt; matter]
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;predict&lt;/strong&gt;: word&lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;input&lt;/strong&gt;: context (words around the current word)&lt;/li&gt;
              &lt;li&gt;hidden layer has &lt;strong&gt;no nonlinearity&lt;/strong&gt;&lt;/li&gt;
              &lt;li&gt;embedding vectors of context words are &lt;strong&gt;averaged&lt;/strong&gt; (SUM)
                &lt;ul&gt;
                  &lt;li&gt;“projection layer is shared for all words (not just the projection matrix)”&lt;/li&gt;
                  &lt;li&gt;Leibe: Summing the encoding vectors for all words encourages the network to learn orthogonal embedding vectors for different words&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;skip-gram
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;predict&lt;/strong&gt;: context ($R$ words before and $R$ words after the current word)
                &lt;ul&gt;
                  &lt;li&gt;$R$ is chosen randomly within $[1,C]$, where $C$ is the maximum word distance&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;input&lt;/strong&gt;: word&lt;/li&gt;
              &lt;li&gt;gives less weight to more distant words
                &lt;ul&gt;
                  &lt;li&gt;words that are close to a target word have a chance of being drawn more often $\Rightarrow$ i.e. these words would implicitly get a higher weight in this prediction task&lt;/li&gt;
                  &lt;li&gt;paper:
                    &lt;ul&gt;
                      &lt;li&gt;We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity.&lt;/li&gt;
                      &lt;li&gt;Since the more &lt;strong&gt;distant words are usually less related to the current word&lt;/strong&gt; than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;GloVe&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;analogy-questions&quot;&gt;Analogy Questions&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Analogy questions:
    &lt;ul&gt;
      &lt;li&gt;Vektordifferenz $\mathbf{a}-\mathbf{b}$ geht “zu $\mathbf{a}$ hin”&lt;/li&gt;
      &lt;li&gt;search the &lt;strong&gt;embedding vector space&lt;/strong&gt; for the word closest to the result &lt;strong&gt;using the cosine distance&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;projection of embedding vectors to 2D space via &lt;strong&gt;t-SNE&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;wiki: t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton,&lt;a href=&quot;https://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738&quot;&gt;1&lt;/a&gt; where Laurens van der Maaten proposed the t-distributed variant.&lt;a href=&quot;http://www.deeplearningbook.org&quot;&gt;2&lt;/a&gt; It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;types of analogies:
        &lt;ul&gt;
          &lt;li&gt;semantic (use &lt;strong&gt;skip-gram&lt;/strong&gt;):
            &lt;ul&gt;
              &lt;li&gt;Athens - Greece $\approx$ Oslo - Norway&lt;/li&gt;
              &lt;li&gt;brother - sister $\approx$ grandson - granddaughter&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;syntactic (use &lt;strong&gt;CBOW&lt;/strong&gt;):
            &lt;ul&gt;
              &lt;li&gt;great - greater $\approx$ tough - tougher&lt;/li&gt;
              &lt;li&gt;Switzerland - Swiss $\approx$ Cambodia - Cambodian&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;implementation-1&quot;&gt;Implementation&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;problem&lt;/strong&gt;: calculating the denominator of the softmax activation is very expensive
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;reason&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;i.e. normalization over 100k-1M outputs, sum over the entire vocab size in the denominator&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Solution 1&lt;/strong&gt;: use &lt;strong&gt;Hierarchical Softmax&lt;/strong&gt; [&lt;a href=&quot;https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf&quot;&gt;Morin, Bengio 2005&lt;/a&gt;][&lt;a href=&quot;https://arxiv.org/pdf/1411.2738.pdf&quot;&gt;3rd party paper&lt;/a&gt;] instead of standard Softmax:
        &lt;ul&gt;
          &lt;li&gt;watch: &lt;a href=&quot;https://www.youtube.com/watch?v=3eoX_waysy4&amp;amp;list=PLhWB2ZsrULv-wEM8JDKA1zk8_2Lc88I-s&amp;amp;index=6&quot;&gt;Andrew Ng&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;learn a binary search tree
            &lt;ul&gt;
              &lt;li&gt;[each leaf is one word]
                &lt;ul&gt;
                  &lt;li&gt;[path to the leaf determines the &lt;strong&gt;probability of the word being the output word&lt;/strong&gt;]&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;factorize “probability of a word being the output word” as a &lt;strong&gt;product of node probabilities along the path&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;learn a &lt;strong&gt;linear decision function at each node&lt;/strong&gt; for deciding for left or right child node&lt;/li&gt;
          &lt;li&gt;computational cost: $\mathcal{O}(\log(V))$ (instead of $\mathcal{O}(V)$ for standard Softmax)&lt;/li&gt;
          &lt;li&gt;[&lt;a href=&quot;https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf&quot;&gt;Morin, Bengio 2005&lt;/a&gt;]
            &lt;ul&gt;
              &lt;li&gt;The objective of this paper is thus to propose a much faster variant of the neural probabilistic language model&lt;/li&gt;
              &lt;li&gt;The basic idea is to form a hierarchical description of a word as a sequence of $\mathcal{O}(\log \vert V\vert)$ decisions, and to learn to take these probabilistic decisions instead of directly predicting each word’s probability. [$\vert V\vert$ is the number of words in the vocabulary $V$]&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Solution 2&lt;/strong&gt;: use &lt;strong&gt;negative sampling&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;nplmnnlm-problems&quot;&gt;NPLM/NNLM problems&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;problem&lt;/strong&gt;: Hinton: [last hidden layer must not be too big or too small, it’s hard to find a compromise]
    &lt;ul&gt;
      &lt;li&gt;each unit in the last hidden layer has 100k outgoing weights
        &lt;ul&gt;
          &lt;li&gt;therefore, we can only afford to have a few units there before we start overfitting [&lt;strong&gt;hidden unit Anzahl erhöhen&lt;/strong&gt;]
            &lt;ul&gt;
              &lt;li&gt;unless we have enough training data&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;we could make the last hidden layer small, but then it’s hard to get the 100k probabilities right [&lt;strong&gt;hidden unit Anzahl vermindern&lt;/strong&gt;]
            &lt;ul&gt;
              &lt;li&gt;the small probabilities are often relevant&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;siamese-networks&quot;&gt;Siamese Networks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Siamese networks&lt;/strong&gt; (aka Triplet Loss NN)
    &lt;ul&gt;
      &lt;li&gt;similar idea to word embeddings
        &lt;ul&gt;
          &lt;li&gt;learn an embedding network that preserves (semantic) similarity between inputs&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Learning
        &lt;ol&gt;
          &lt;li&gt;with triplet loss
            &lt;ul&gt;
              &lt;li&gt;learn an embedding that groups the &lt;strong&gt;positive&lt;/strong&gt; closer to the &lt;strong&gt;anchor&lt;/strong&gt; than the &lt;strong&gt;negative&lt;/strong&gt;&lt;/li&gt;
              &lt;li&gt;$d(a,p)+\alpha\lt d(a,n)$ $\Rightarrow$ $L_{tri}=\sum_{a,p,n}\max(\alpha+D_{a,p}-D_{a,n},0)$
                &lt;ul&gt;
                  &lt;li&gt;$\alpha$: margin
                    &lt;ul&gt;
                      &lt;li&gt;to avoid trivial solution of the inequality (e.g. same output for all images)&lt;/li&gt;
                      &lt;li&gt;$\alpha$ determines how different the inputs have to be in order to satisfy the inequality (similar to SVM margin)&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;problem&lt;/strong&gt;: most triplets are uninformative
                &lt;ul&gt;
                  &lt;li&gt;we want &lt;strong&gt;medium-hard&lt;/strong&gt; triplets because using too hard triplets is like focussing on outliers (which do not help learning)&lt;/li&gt;
                  &lt;li&gt;&lt;strong&gt;solution&lt;/strong&gt;: use hard triplet mining
                    &lt;ul&gt;
                      &lt;li&gt;process dataset to find hard triplets&lt;/li&gt;
                      &lt;li&gt;use those for learning&lt;/li&gt;
                      &lt;li&gt;iterate&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;used in e.g. Google FaceNet&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;with contrastive loss&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;apps:
        &lt;ul&gt;
          &lt;li&gt;patch matching&lt;/li&gt;
          &lt;li&gt;face recognition&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;rnn&quot;&gt;RNN&lt;/h1&gt;

&lt;h2 id=&quot;architecture-2&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/karpathy_rnn.png&quot; alt=&quot;karpathy_rnn.png&quot; /&gt;&lt;a name=&quot;karpathy_rnn&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{h}_0$ is also &lt;strong&gt;learned&lt;/strong&gt; like the other parameters&lt;/li&gt;
  &lt;li&gt;weights $\mathbf{W}_{hh}$ between the hidden units are &lt;strong&gt;shared&lt;/strong&gt; between temporal layers&lt;/li&gt;
  &lt;li&gt;connection matrices: $\mathbf{W}&lt;em&gt;{xh}$, $\mathbf{W}&lt;/em&gt;{hy}$ and $\mathbf{W}_{hh}$&lt;/li&gt;
  &lt;li&gt;powerful because
    &lt;ul&gt;
      &lt;li&gt;distributed hidden state
        &lt;ul&gt;
          &lt;li&gt;allows to store information about the past efficiently&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;nonlinearities
        &lt;ul&gt;
          &lt;li&gt;hidden states can be updated in complicated ways&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;RNNs are Turing complete
        &lt;ul&gt;
          &lt;li&gt;given enough neurons and time RNNs can compute anything that can be computed by a computer&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hinton:
    &lt;ul&gt;
      &lt;li&gt;just a feedforward net that keeps reusing the same weights in each (temporal) layer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bptt&quot;&gt;BPTT&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;BPTT equations:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;key point&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;in $\frac{\partial E}{\partial w_{ij}}$ the &lt;strong&gt;(temporal) error propagation term&lt;/strong&gt; $\frac{\partial h_t}{\partial h_k}$ will either go to zero or explode (depending on the largest singular value of $\mathbf{W}_{rec}$) for $k\ll t$ (i.e. for &lt;strong&gt;long term contributions&lt;/strong&gt;)
            &lt;ul&gt;
              &lt;li&gt;vanishing gradient proof (cf. below in Pascanu paper): $\frac{\partial h_t}{\partial h_k}$ is bounded from above by $\eta^{t-k}$ which goes to 0 exponentially fast with $t-k$ and, therefore, long term contributions go to zero, too&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Bengio_1994 and &lt;a href=&quot;https://proceedings.mlr.press/v28/pascanu13.pdf&quot;&gt;Pascanu_2013&lt;/a&gt;:&lt;br /&gt;
  &lt;img src=&quot;/home/assets/images/bengio94_2.png&quot; alt=&quot;bengio94_2.png&quot; /&gt;
  &lt;img src=&quot;/home/assets/images/bengio94_3.png&quot; alt=&quot;bengio94_3.png&quot; /&gt;
  &lt;img src=&quot;/home/assets/images/bengio94_1.png&quot; alt=&quot;bengio94_1.png&quot; /&gt;
  &lt;img src=&quot;/home/assets/images/bengio94_4.png&quot; alt=&quot;bengio94_4.png&quot; /&gt;
  &lt;img src=&quot;/home/assets/images/bengio94_5.png&quot; alt=&quot;bengio94_5.png&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;i.e. this is only valid for &lt;strong&gt;long term contributions&lt;/strong&gt; ($k\ll t$)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cliffs-and-exploding-gradients&quot;&gt;Cliffs and Exploding Gradients&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;if the largest Eigenvalue of the weight matrix $W_{hh}$ is $\gt 1$, the gradient will explode (s. &lt;a href=&quot;#karpathy_rnn&quot;&gt;Karpathy code&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;source: &lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/goodfellow_ml/RNN_cliff.png&quot; alt=&quot;RNN_cliff.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/goodfellow_ml/RNN_cliff_gradient_clipping.png&quot; alt=&quot;RNN_cliff_gradient_clipping.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;cliff structures are an &lt;strong&gt;example of the &lt;mark&gt;exploding&lt;/mark&gt; gradient phenomenon&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Recurrent networks&lt;/strong&gt; use the same matrix $W$ at each time step, but &lt;strong&gt;feedforward networks&lt;/strong&gt; do not,
        &lt;ul&gt;
          &lt;li&gt;so even very deep feedforward networks can largely avoid the vanishing and exploding gradient problem&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;solution-gradient-clipping&quot;&gt;Solution: Gradient Clipping&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;rescale gradients to a fixed size
    &lt;ul&gt;
      &lt;li&gt;“if the gradient is larger than a threshold, clip it to that threshold”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;relus&quot;&gt;ReLUs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1504.00941.pdf&quot;&gt;Le, Hinton 2015&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;At first sight, ReLUs seem inappropriate for RNNs&lt;/strong&gt; because they can have very large outputs so they might be expected to be &lt;strong&gt;far more likely to explode&lt;/strong&gt; than units that have bounded values&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vanishing-gradient-problem&quot;&gt;Vanishing Gradient Problem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;if the largest Eigenvalue of the weight matrix $W_{hh}$ is $\lt 1$, the gradient will vanish (s. &lt;a href=&quot;#karpathy_rnn&quot;&gt;Karpathy code&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Goodfellow:
    &lt;ul&gt;
      &lt;li&gt;gradients through such a [RNN] graph are also scaled according to $\text{diag}(\vec{\lambda})^t$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Vanishing gradients&lt;/strong&gt; make it difficult to know which direction the parameters should move to improve the cost function, while &lt;strong&gt;exploding gradients&lt;/strong&gt; can make learning unstable.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Leibe:
    &lt;ul&gt;
      &lt;li&gt;they severely restrict the dependencies the RNN can learn
        &lt;ul&gt;
          &lt;li&gt;e.g. in language models:
            &lt;ul&gt;
              &lt;li&gt;words from time steps far away (&lt;strong&gt;long-range dependencies&lt;/strong&gt;) are not taken into consideration when training to predict the next word&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;problem gets more severe the deeper the network is&lt;/li&gt;
      &lt;li&gt;harder problem than exploding gradients because it can be very hard to diagnose that vanishing gradients occur
        &lt;ul&gt;
          &lt;li&gt;you just see that learning gets stuck&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;long-range-dependencies&quot;&gt;Long-range Dependencies&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1504.00941.pdf&quot;&gt;Le, Hinton 2015&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;[&lt;strong&gt;key point&lt;/strong&gt;:]
        &lt;ul&gt;
          &lt;li&gt;[use ReLU and initialize $\mathbf{W}_{hh}$ to be the identity matrix (and biases to be zero)]
            &lt;ul&gt;
              &lt;li&gt;[this way RNNs can perform as well as LSTMs and learn long-range dependencies]&lt;/li&gt;
              &lt;li&gt;[Leibe: to propagate the gradients with a constant factor]&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;In this paper, we demonstrate that, with the right initialization of the weights, &lt;strong&gt;&lt;mark&gt;RNNs&lt;/mark&gt;&lt;/strong&gt; composed of rectified linear units are relatively easy to train and are &lt;strong&gt;&lt;mark&gt;good at modeling long-range dependencies&lt;/mark&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Their &lt;strong&gt;&lt;mark&gt;performance&lt;/mark&gt;&lt;/strong&gt; on test data is &lt;strong&gt;&lt;mark&gt;comparable with LSTMs&lt;/mark&gt;&lt;/strong&gt;, both for toy problems involving very long-range temporal structures and for real tasks like predicting the next word in a very large corpus of text.&lt;/li&gt;
      &lt;li&gt;[method:]
        &lt;ul&gt;
          &lt;li&gt;We &lt;strong&gt;initialize&lt;/strong&gt; the recurrent weight matrix to be the &lt;strong&gt;identity matrix&lt;/strong&gt; and biases to be zero.
            &lt;ul&gt;
              &lt;li&gt;This means that each new hidden state vector is obtained by simply copying the previous hidden vector then adding on the effect of the current inputs and replacing all negative states by zero.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;In the absence of input&lt;/strong&gt;, an RNN that is composed of ReLUs and initialized with the identity matrix (which we call an &lt;strong&gt;IRNN&lt;/strong&gt;) just &lt;strong&gt;stays in the same state indefinitely&lt;/strong&gt;.&lt;/li&gt;
          &lt;li&gt;The identity initialization has the very desirable property that when the &lt;strong&gt;error derivatives for the hidden units&lt;/strong&gt; are backpropagated through time they &lt;strong&gt;remain constant&lt;/strong&gt; provided no extra error-derivatives are added.
            &lt;ul&gt;
              &lt;li&gt;This is the &lt;strong&gt;same behavior as LSTMs&lt;/strong&gt; when their forget gates are set so that there is no decay and it &lt;strong&gt;makes it easy to learn very long-range temporal dependencies&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;apps&quot;&gt;Apps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Predicting the next word (e.g. Google search suggestions)&lt;/li&gt;
  &lt;li&gt;Machine translation (Sutskever 2014)&lt;/li&gt;
  &lt;li&gt;Character-Level Language Model
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;task&lt;/strong&gt;: model the probability distribution of the next character in the sequence
        &lt;ul&gt;
          &lt;li&gt;advantage RNN: RNN can learn varying amount of context&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Karpathy:
        &lt;ul&gt;
          &lt;li&gt;min-char-rnn,&lt;/li&gt;
          &lt;li&gt;generating Shakespearesque texts (3-layer RNN),&lt;/li&gt;
          &lt;li&gt;generate Wikipedia text (LSTM),&lt;/li&gt;
          &lt;li&gt;generate algebraic geometry text/LaTeX code (multilayer LSTM),&lt;/li&gt;
          &lt;li&gt;generate Linux source code/C code (3-layer LSTM)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;image captioning
    &lt;ul&gt;
      &lt;li&gt;use CNN to define $h_0$&lt;/li&gt;
      &lt;li&gt;use RNN to produce a caption&lt;/li&gt;
      &lt;li&gt;data: e.g. Microsoft COCO&lt;/li&gt;
      &lt;li&gt;variant: “image to story”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;video to text description&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;units-with-a-gating-mechanism&quot;&gt;Units with a Gating Mechanism&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;basic idea:
    &lt;ul&gt;
      &lt;li&gt;use more sophisticated units that implement a gating mechanism, such as a &lt;strong&gt;long short-term memory (LSTM) unit&lt;/strong&gt; and a recently proposed &lt;strong&gt;gated recurrent unit (GRU)&lt;/strong&gt; instead of more traditional recurrent units such as &lt;strong&gt;tanh units&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Johnson: [LSTMs are] designed to help alleviate this problem of vanishing and exploding gradients [of RNNs]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm&quot;&gt;LSTM&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Olah&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1504.00941.pdf&quot;&gt;Le, Hinton&lt;/a&gt;:
            &lt;ul&gt;
              &lt;li&gt;at the time [LSTMs were invented], the important issue was to find any scheme that could learn long-range dependencies rather than to find the minimal or optimal scheme.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Leibe:
    &lt;ul&gt;
      &lt;li&gt;with standard RNNs you can learn temporal dependencies of up to 10 time steps, with LSTMs you can maintain temporal connections of up to a 100 time steps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;four&lt;/strong&gt; gates: i, f, o and g&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Olah&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Consider e.g. a &lt;strong&gt;language model&lt;/strong&gt; trying to predict the next word based on all the previous ones&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;f&lt;/strong&gt;: decide what information we’re going to throw away from the cell state
        &lt;ul&gt;
          &lt;li&gt;In such a [language model] problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;i, g&lt;/strong&gt;: decide what new information we’re going to store in the cell state
        &lt;ul&gt;
          &lt;li&gt;we’d want to add the gender of the new subject [$x_t$] to the cell state, to replace the old one we’re forgetting&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;update&lt;/strong&gt; the old cell state, $C_{t−1}$, into the new cell state $C_t$
        &lt;ul&gt;
          &lt;li&gt;this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps [&lt;strong&gt;steps f and i, g&lt;/strong&gt;]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;o&lt;/strong&gt;: decide what we’re going to output
        &lt;ul&gt;
          &lt;li&gt;since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optimization&quot;&gt;Optimization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1504.00941.pdf&quot;&gt;Le, Hinton&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;it is observed that setting a higher initial forget gate bias for LSTMs can give better results for long term dependency problems. We therefore also performed a grid search for the initial forget gate bias in LSTMs from the set {1.0, 4.0, 10.0, 20.0}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gru&quot;&gt;GRU&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Olah&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;Simpler model than LSTM
        &lt;ul&gt;
          &lt;li&gt;$z_t$: combine forget and input gate into a single &lt;strong&gt;update gate&lt;/strong&gt; $z_t$&lt;/li&gt;
          &lt;li&gt;$r_t$: reset gate $r_t$ (functional form like update gate $z_t$)&lt;/li&gt;
          &lt;li&gt;merges cell state $C_{t-1}$ and hidden state $h_{t-1}$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Empirical results
    &lt;ul&gt;
      &lt;li&gt;performance similar to LSTM&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;But&lt;/strong&gt;: fewer parameters than LSTM (need less training data)
        &lt;ul&gt;
          &lt;li&gt;however, GRUs have more complex mechanisms that are harder to learn than LSTMs
            &lt;ul&gt;
              &lt;li&gt;therefore, performance similar to LSTMs&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Effects (see equations for $\tilde{h}_t$ and $h_t$)
    &lt;ul&gt;
      &lt;li&gt;reset: if $r_t$ is close to zero, ignore previous hidden state $h_{t-1}$&lt;/li&gt;
      &lt;li&gt;update: if $z_t$ is close to zero, $h_{t}\approx h_{t-1}$
        &lt;ul&gt;
          &lt;li&gt;i.e. update gate $z_t$ controls how much of the past state $h_{t-1}$ should matter now&lt;/li&gt;
          &lt;li&gt;in this way information can be copied through many time steps
            &lt;ul&gt;
              &lt;li&gt;i.e. less vanishing gradients&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;typical learned behaviors (cf. “&lt;strong&gt;GRU circuit&lt;/strong&gt;” visualization)
    &lt;ul&gt;
      &lt;li&gt;units with &lt;strong&gt;short-term&lt;/strong&gt; dependencies often have an active reset gate $r_t=1$&lt;/li&gt;
      &lt;li&gt;units with &lt;strong&gt;long-term&lt;/strong&gt; dependencies often have an inactive update gate $z_t=0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;REFERENCES&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a name=&quot;Bishop_2006&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738&quot;&gt;Bishop, Christopher M., &lt;em&gt;Pattern Recognition and Machine Learning (Information Science and Statistics)&lt;/em&gt; (2006), Springer-Verlag, Berlin, Heidelberg, 0387310738.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;Goodfellow_2016&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.deeplearningbook.org&quot;&gt;Ian J. Goodfellow and Yoshua Bengio and Aaron Courville, &lt;em&gt;Deep Learning&lt;/em&gt; (2016), MIT Press, Cambridge, MA, USA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;Long_Shelhamer&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf&quot;&gt;Long, Shelhamer, &lt;em&gt;FCN for semantic segmentation&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;Kulkarni_Whitney&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf&quot;&gt;Tejas D. Kulkarni, William F. Whitney, Pushmeet Kohli, Joshua B. Tenenbaum, &lt;em&gt;Deep Convolutional Inverse Graphics Network&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;Dosovitskiy_Springenberg&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.semanticscholar.org/paper/Learning-to-Generate-Chairs-Tables-and-Cars-with-Dosovitskiy-Springenberg/e47e988e6d96b876bcab8ca8e2275a6d73a3f7e8/pdf&quot;&gt;Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko, Thomas Brox, &lt;em&gt;Learning to Generate Chairs, Tables and Cars with Convolutional Networks&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Lecture_Notes" /><category term="Machine_Learning" /><category term="Computer_Vision" /><category term="lecture_notes" /><category term="ml" /><category term="cv" /><summary type="html">Notes on Computer Vision theory and NLP. Based on Goodfellow, Bengio &quot;Deep Learning&quot;, Stanford CS231n and RWTH Aachen University Machine Learning</summary></entry><entry><title type="html">ROS Cheatsheet</title><link href="http://localhost:4000/cheatsheet/cheatsheet-ros/" rel="alternate" type="text/html" title="ROS Cheatsheet" /><published>2022-01-22T00:00:00+01:00</published><updated>2022-01-22T00:00:00+01:00</updated><id>http://localhost:4000/cheatsheet/cheatsheet-ros</id><content type="html" xml:base="http://localhost:4000/cheatsheet/cheatsheet-ros/">&lt;h1 id=&quot;roslaunch&quot;&gt;roslaunch&lt;/h1&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;roslaunch --ros-args /path/to/launchfile.launch&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Display command-line arguments for this launch file&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;launch&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- ros_args.launch --&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;arg&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;foo&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;default=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;true&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;doc=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I pity the foo'.&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;arg&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bar&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;doc=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Someone walks into this.&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;arg&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;baz&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;default=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;false&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;arg&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;nop&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;arg&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;fix&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/launch&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$&amp;gt;&lt;/span&gt; roslaunch &lt;span class=&quot;nt&quot;&gt;--ros-args&lt;/span&gt; ros_args.launch
Required Arguments:
  bar: Someone walks into this.
  nop: undocumented
Optional Arguments:
  baz &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;default &lt;span class=&quot;s2&quot;&gt;&quot;false&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: undocumented
  foo &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;default &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: I pity the foo&lt;span class=&quot;s1&quot;&gt;'.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;image_view&quot;&gt;image_view&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Display the images of an image topic:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rosrun image_view image_view image:&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/sensor/camera1/image_raw
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;some-open-source-packages&quot;&gt;some open source packages&lt;/h1&gt;

&lt;h2 id=&quot;opencv_apps&quot;&gt;opencv_apps&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sudo apt install ros-noetic-opencv-apps&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;roslaunch opencv_apps hough_lines.launch image:&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/sensor/camera1/image_raw
&lt;span class=&quot;c&quot;&gt;# view all arguments of a node in the launch file&lt;/span&gt;
roslaunch &lt;span class=&quot;nt&quot;&gt;--args&lt;/span&gt; hough_lines /opt/ros/noetic/share/opencv_apps/launch/hough_lines.launch
&lt;span class=&quot;c&quot;&gt;# view all arguments of the launch file&lt;/span&gt;
roslaunch &lt;span class=&quot;nt&quot;&gt;--ros-args&lt;/span&gt; /opt/ros/noetic/share/opencv_apps/launch/hough_lines.launch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Cheatsheet" /><category term="ros" /><category term="cheatsheet" /><summary type="html">roslaunch roslaunch --ros-args /path/to/launchfile.launch Display command-line arguments for this launch file &amp;lt;launch&amp;gt; &amp;lt;!-- ros_args.launch --&amp;gt; &amp;lt;arg name=&quot;foo&quot; default=&quot;true&quot; doc=&quot;I pity the foo'.&quot;/&amp;gt; &amp;lt;arg name=&quot;bar&quot; doc=&quot;Someone walks into this.&quot;/&amp;gt; &amp;lt;arg name=&quot;baz&quot; default=&quot;false&quot;/&amp;gt; &amp;lt;arg name=&quot;nop&quot;/&amp;gt; &amp;lt;arg name=&quot;fix&quot; value=&quot;true&quot;/&amp;gt; &amp;lt;/launch&amp;gt; $&amp;gt; roslaunch --ros-args ros_args.launch Required Arguments: bar: Someone walks into this. nop: undocumented Optional Arguments: baz (default &quot;false&quot;): undocumented foo (default &quot;true&quot;): I pity the foo'. image_view Display the images of an image topic: rosrun image_view image_view image:=/sensor/camera1/image_raw some open source packages opencv_apps sudo apt install ros-noetic-opencv-apps roslaunch opencv_apps hough_lines.launch image:=/sensor/camera1/image_raw # view all arguments of a node in the launch file roslaunch --args hough_lines /opt/ros/noetic/share/opencv_apps/launch/hough_lines.launch # view all arguments of the launch file roslaunch --ros-args /opt/ros/noetic/share/opencv_apps/launch/hough_lines.launch</summary></entry><entry><title type="html">Machine Learning (Part 2)</title><link href="http://localhost:4000/lecture_notes/machine_learning/lecture-notes-ML-part2/" rel="alternate" type="text/html" title="Machine Learning (Part 2)" /><published>2022-01-17T00:00:00+01:00</published><updated>2022-01-17T00:00:00+01:00</updated><id>http://localhost:4000/lecture_notes/machine_learning/lecture-notes-ML-part2</id><content type="html" xml:base="http://localhost:4000/lecture_notes/machine_learning/lecture-notes-ML-part2/">&lt;h1 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h1&gt;

&lt;h2 id=&quot;perceptrons-rosenblatt-1962&quot;&gt;Perceptrons (Rosenblatt 1962)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;perceptrons (SLPs) are &lt;strong&gt;generalized linear models&lt;/strong&gt; (“generalized” because of the activation function)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;BUT&lt;/strong&gt;: Deep Neural Networks (MLPs) are &lt;strong&gt;nonlinear parametric models&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;more specifically: perceptrons are &lt;strong&gt;generalized linear discriminants&lt;/strong&gt; (because they map the input &lt;strong&gt;x&lt;/strong&gt; directly to a class label t in {-1,+1} [see above: “Linear models for classification”: approach 1.])&lt;/li&gt;
  &lt;li&gt;original version:
    &lt;ul&gt;
      &lt;li&gt;2-class linear discriminant
        &lt;ul&gt;
          &lt;li&gt;special case: SLP&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;with &lt;mark&gt;fixed [= not learned]&lt;/mark&gt; nonlinear transformation $\vec{\phi}(\pmb{x})$
        &lt;ul&gt;
          &lt;li&gt;for MLPs: learning the $w_{kj}$ to the hidden units corresponds to learning $\vec{\phi}(\pmb{x})$&lt;/li&gt;
          &lt;li&gt;Note: with a suitable $\vec{\phi}(\pmb{x})$ this perceptron &lt;strong&gt;can&lt;/strong&gt; learn an XOR function!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;activation function: step function&lt;/li&gt;
      &lt;li&gt;learned via minimization of “&lt;strong&gt;perceptron criterion&lt;/strong&gt;” $\Rightarrow$ SGD &lt;strong&gt;of quadratic error&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Perceptron Convergence Theorem&lt;/strong&gt;: exact solution in a finite number of steps guaranteed for linearly separable data set
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;BUT:&lt;/strong&gt; in practice, convergence can be slow
            &lt;ul&gt;
              &lt;li&gt;it’s hard to decide, if a problem is not linearly separable or just slowly converging!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;terminology&quot;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Input layer&lt;/strong&gt; is a layer, it’s not wrong to say that. &lt;a href=&quot;https://datascience.stackexchange.com/a/14033/115254&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;However, when calculating the &lt;strong&gt;depth&lt;/strong&gt; of a deep neural network, we only consider the layers that have tunable weights. &lt;a href=&quot;https://datascience.stackexchange.com/a/14033/115254&quot;&gt;source&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;follows the direction of the &lt;strong&gt;negative&lt;/strong&gt; gradient
    &lt;ul&gt;
      &lt;li&gt;gradients $\vec{\nabla}f$ point towards the direction of steepest &lt;strong&gt;ascent&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;computational-differentiation-methods&quot;&gt;Computational Differentiation Methods&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Automatic Differentiation&lt;/li&gt;
  &lt;li&gt;Symbolic Differentiation (Computer Algebra System (CAS))
    &lt;ul&gt;
      &lt;li&gt;use a method to &lt;strong&gt;represent mathematical data&lt;/strong&gt; in a computer
        &lt;ul&gt;
          &lt;li&gt;data representation:
            &lt;ul&gt;
              &lt;li&gt;numbers
                &lt;ul&gt;
                  &lt;li&gt;efficient implementation of the arithmetic operations:
                    &lt;ul&gt;
                      &lt;li&gt;GMP library (&lt;em&gt;de facto&lt;/em&gt; standard)&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;variables&lt;/li&gt;
              &lt;li&gt;expressions&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;e.g.
        &lt;ul&gt;
          &lt;li&gt;Mathematica (using GMP)&lt;/li&gt;
          &lt;li&gt;Maple (using GMP)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Numerical Differentiation
    &lt;ul&gt;
      &lt;li&gt;e.g.
        &lt;ul&gt;
          &lt;li&gt;Finite Differences method&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;automatic-differentiation&quot;&gt;Automatic Differentiation&lt;/h2&gt;

&lt;h3 id=&quot;forward-mode-vs-reverse-mode-differentiation&quot;&gt;Forward-mode vs Reverse-mode differentiation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;read &lt;a href=&quot;https://colah.github.io/posts/2015-08-Backprop/&quot;&gt;Olah&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Forward-mode differentiation&lt;/strong&gt; starts at an input to the graph and moves towards the end. At every node, it sums all the paths feeding in. Each of those paths represents one way in which the input affects that node. By adding them up, we get the total way in which the node is affected by the input, it’s derivative. […]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Reverse-mode differentiation&lt;/strong&gt;, on the other hand, starts at an output of the graph and moves towards the beginning. At each node, it merges all paths which originated at that node. […]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;When I say that reverse-mode differentiation gives us the derivative of e with respect to every node, I really do mean &lt;strong&gt;every node&lt;/strong&gt;. We get both $\frac{\partial e}{\partial a}$ and $\frac{\partial e}{\partial b}$, the derivatives of $e$ with respect to both inputs. Forward-mode differentiation gave us the derivative of our output with respect to a single input, but reverse-mode differentiation gives us all of them. […]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;When training neural networks, we think of the cost (a value describing how bad a neural network performs) as a function of the parameters (numbers describing how the network behaves). We want to calculate the derivatives of the &lt;strong&gt;cost with respect to all the parameters&lt;/strong&gt;, for use in gradient descent. Now, there’s often millions, or even tens of millions of parameters in a neural network. So, &lt;strong&gt;reverse-mode differentiation, &lt;mark&gt;called&lt;/mark&gt; backpropagation&lt;/strong&gt; [&lt;a href=&quot;#reverse_mode_accumulation&quot;&gt;more precise: reverse_mode_accumulation&lt;/a&gt;] in the context of neural networks, gives us a massive speed up!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(Are there any cases &lt;strong&gt;where forward-mode differentiation makes more sense&lt;/strong&gt;? Yes, there are! Where the reverse-mode gives the derivatives of one output with respect to all inputs, the forward-mode gives us the derivatives of all outputs with respect to one input. If one has a function with lots of outputs, forward-mode differentiation can be much, much, much faster.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;both are algorithms for efficiently computing the sum by factoring the paths. Instead of summing over all of the paths explicitly, they compute the same sum more efficiently by &lt;mark&gt;**merging paths back together at every node**&lt;/mark&gt;. In fact, &lt;strong&gt;both&lt;/strong&gt; algorithms touch each edge exactly once!
    &lt;ul&gt;
      &lt;li&gt;At each node, reverse-mode differentiation merges all paths which &lt;strong&gt;originated&lt;/strong&gt; at that node (starting at an output of the graph and moving towards the beginning)&lt;/li&gt;
      &lt;li&gt;At each node, forward-mode differentiation sums all the paths &lt;strong&gt;feeding into&lt;/strong&gt; that node (starting at the beginning and moving towards an output of the graph)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;forward-mode: apply operator $\frac{\partial}{\partial X}$&lt;/li&gt;
  &lt;li&gt;reverse-mode: apply operator $\frac{\partial Z}{\partial}$&lt;/li&gt;
  &lt;li&gt;if we have e.g. a hundred inputs, but only one output, reverse-mode differentiation gives a speed up in $\mathcal{O}(\text{# Inputs})$ compared to forward-mode differentiation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pytorch-autograd&quot;&gt;PyTorch autograd&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/pytorch_with_examples.html&quot;&gt;source: Justin Johnson&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the above examples, we had to &lt;strong&gt;manually&lt;/strong&gt; implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer (?: siehe Stichpunkt) network, but can quickly get very hairy for large complex networks.
    &lt;ul&gt;
      &lt;li&gt;?: Why “two-layer”:
        &lt;ul&gt;
          &lt;li&gt;The previous polynomial regression examples correspond to a &lt;strong&gt;single&lt;/strong&gt; layer perceptron with a fixed nonlinear transformation of the inputs (here: using polynomial basis functions), so why does Johnson say &lt;strong&gt;two&lt;/strong&gt;-layer perceptron?
            &lt;ul&gt;
              &lt;li&gt;What Johnson probably means here is that, basically, implementing backprop &lt;strong&gt;manually&lt;/strong&gt; (like in the previous polynomial regression examples) for a two-layer NN would be possible without autograd. This “two-layer network”, however, does not refer to the previous polynomial regression models!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt; computes &lt;strong&gt;all&lt;/strong&gt; gradients with only one line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;in polynomial regression example &lt;strong&gt;without&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt;:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;grad_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;grad_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;grad_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;grad_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;the same &lt;strong&gt;with&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt;:
        &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;p&gt;where all parameter tensors must have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad = True&lt;/code&gt; (otherwise &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;autograd&lt;/code&gt; does not know wrt which parameters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt; must be differentiated).&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Thankfully, we can use &lt;strong&gt;automatic differentiation&lt;/strong&gt; to automate the computation of backward passes in neural networks. The &lt;strong&gt;autograd&lt;/strong&gt; package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a &lt;strong&gt;computational graph&lt;/strong&gt;; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.
    &lt;ul&gt;
      &lt;li&gt;auf Folie:
        &lt;ol&gt;
          &lt;li&gt;Convert NN to a computational graph
            &lt;ul&gt;
              &lt;li&gt;explanations:
                &lt;ul&gt;
                  &lt;li&gt;&lt;a href=&quot;https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/&quot;&gt;PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd&lt;/a&gt;
                    &lt;ul&gt;
                      &lt;li&gt;&lt;a href=&quot;/pytorch/machine_learning/notes-pytorch/#how-does-pytorch-create-a-computational-graph&quot;&gt;important points from this blog post&lt;/a&gt;&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/computational-graphs-in-pytorch-and-tensorflow-c25cc40bdcd1&quot;&gt;Computational graphs in PyTorch and TensorFlow&lt;/a&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Each new layer/module specifies how it affects the forward and backward passes
            &lt;ul&gt;
              &lt;li&gt;auf nächster Folie: “Each module is defined by
                &lt;ul&gt;
                  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module.fprop(&lt;/code&gt;$x$&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;)&lt;/code&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module.bprop(&lt;/code&gt;$\frac{\partial E}{\partial y}$&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;)&lt;/code&gt;
                    &lt;ul&gt;
                      &lt;li&gt;computes the gradients of the cost wrt. the inputs $x$ given the gradient wrt. the outputs $y$&lt;/li&gt;
                      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;module.bprop()&lt;/code&gt; ist in PyTorch wegen dem Autograd System nicht notwendig (vgl. &lt;a href=&quot;/pytorch/machine_learning/notes-pytorch/#modules&quot;&gt;aus PyTorch Doc&lt;/a&gt;)&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Linear&lt;/code&gt; specifies that it will apply a linear transformation $y=xA^T+b$ to the incoming data during the forward pass (each module has a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward()&lt;/code&gt; method, see e.g. &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear&quot;&gt;source nn.Linear&lt;/a&gt;)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Apply reverse-mode differentiation
            &lt;ul&gt;
              &lt;li&gt;i.e. call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss.backward()&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; is a Tensor that has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.requires_grad=True&lt;/code&gt; then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x.grad&lt;/code&gt; is another Tensor holding the gradient of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; with respect to some scalar value.
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c1&quot;&gt;# -*- coding: utf-8 -*-
&lt;/span&gt;  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
  &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;


  &lt;span class=&quot;c1&quot;&gt;# Create Tensors to hold input and outputs.
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# For this example, the output y is a linear function of (x, x^2, x^3), so
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# we can consider it as a linear layer neural network. Let's prepare the
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# tensor (x, x^2, x^3).
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (3,), for this case, broadcasting semantics will apply to obtain a tensor
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# of shape (2000, 3)
&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# Use the nn package to define our model as a sequence of layers. nn.Sequential
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# is a Module which contains other Modules, and applies them in sequence to
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# produce its output. The Linear Module computes output from input using a
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# linear function, and holds internal Tensors for its weight and bias.
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# The Flatten layer flatens the output of the linear layer to a 1D tensor,
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# to match the shape of `y`.
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# The nn package also contains definitions of popular loss functions; in this
&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# case we will use Mean Squared Error (MSE) as our loss function.
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MSELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

      &lt;span class=&quot;c1&quot;&gt;# Forward pass: compute predicted y by passing x to the model. Module objects
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# override the __call__ operator so you can call them like functions. When
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# doing so you pass a Tensor of input data to the Module and it produces
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# a Tensor of output data.
&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

      &lt;span class=&quot;c1&quot;&gt;# Compute and print loss. We pass Tensors containing the predicted and true
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# values of y, and the loss function returns a Tensor containing the
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# loss.
&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

      &lt;span class=&quot;c1&quot;&gt;# Zero the gradients before running the backward pass.
&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

      &lt;span class=&quot;c1&quot;&gt;# Backward pass: compute gradient of the loss with respect to all the learnable
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# parameters of the model. Internally, the parameters of each Module are stored
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# in Tensors with requires_grad=True, so this call will compute gradients for
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# all learnable parameters in the model.
&lt;/span&gt;      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

      &lt;span class=&quot;c1&quot;&gt;# Update the weights using gradient descent. Each parameter is a Tensor, so
&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# we can access its gradients like we did before.
&lt;/span&gt;      &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# You can access the first layer of `model` like accessing the first item of a list
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# For linear layer, its parameters are stored as `weight` and `bias`.
&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Result: y = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; + &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; x + &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; x^2 + &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; x^3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forward-propagation&quot;&gt;Forward Propagation&lt;/h2&gt;

&lt;h3 id=&quot;inputsoutputs&quot;&gt;Inputs/Outputs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;inputs:
    &lt;ul&gt;
      &lt;li&gt;depth $l$&lt;/li&gt;
      &lt;li&gt;$l$ weight matrices of the model $\mathbf{W}^{(i)}$&lt;/li&gt;
      &lt;li&gt;$l$ biases of the model $\mathbf{b}^{(i)}$&lt;/li&gt;
      &lt;li&gt;input $\mathbf{x}$ (here: only one for simplicity)&lt;/li&gt;
      &lt;li&gt;target $\mathbf{y}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;outputs:
    &lt;ul&gt;
      &lt;li&gt;output $\hat{\mathbf{y}}$&lt;/li&gt;
      &lt;li&gt;cost function $J$&lt;/li&gt;
      &lt;li&gt;input of unit $j$: $\mathbf{a}_j^{(k)}$ for all $j$&lt;/li&gt;
      &lt;li&gt;output of unit $j$: $\mathbf{h}_j^{(k)}$ for all $j$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;backprop&quot;&gt;Backprop&lt;/h2&gt;

&lt;h3 id=&quot;inputsoutputs-1&quot;&gt;Inputs/Outputs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;inputs:
    &lt;ul&gt;
      &lt;li&gt;depth $l$&lt;/li&gt;
      &lt;li&gt;$l$ weight matrices of the model $\mathbf{W}^{(i)}$&lt;/li&gt;
      &lt;li&gt;$l$ biases of the model $\mathbf{b}^{(i)}$&lt;/li&gt;
      &lt;li&gt;outputs of Forward Propagation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;outputs:
    &lt;ul&gt;
      &lt;li&gt;gradients w.r.t. all weights and biases $\nabla_{\mathbf{W}^{(k)}}J$ and $\nabla_{\mathbf{b}^{(k)}}J$
        &lt;ul&gt;
          &lt;li&gt;also computes all $\nabla_{\mathbf{a}^{(k)}}J$ and $\nabla_{\mathbf{h}^{(k)}}J$ in the process
            &lt;ul&gt;
              &lt;li&gt;$\nabla_{\mathbf{a}^{(k)}}J$ can be interpreted as an indication of how each layer’s output should change to reduce error
                &lt;ul&gt;
                  &lt;li&gt;es gibt ein $\nabla_{\mathbf{a}^{(k)}}J$ pro layer k: jede unit in layer k entspricht einer Komponente von $\nabla_{\mathbf{a}^{(k)}}J$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;terminology-1&quot;&gt;Terminology&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;refers only to the &lt;strong&gt;method used to compute all necessary gradients&lt;/strong&gt;, whereas another algorithm (e.g. SGD) is used to perform &lt;strong&gt;learning&lt;/strong&gt; using these gradients!
    &lt;ul&gt;
      &lt;li&gt;“however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent” &lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation&quot;&gt;source&lt;/a&gt;
        &lt;blockquote&gt;
          &lt;p&gt;&lt;a name=&quot;reverse_mode_accumulation&quot;&gt;&lt;/a&gt;“More generally, the field of &lt;strong&gt;automatic differentiation&lt;/strong&gt; is concerned with how to compute derivatives algorithmically. The back-propagation algorithm described here is only one approach to automatic differentiation. It is a special case of a broader class of techniques called &lt;strong&gt;reverse mode accumulation&lt;/strong&gt;.” (Goodfellow, Bengio)&lt;/p&gt;
        &lt;/blockquote&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;modular-implementation&quot;&gt;Modular Implementation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;“layer below builds upon (gradient) result of layer above” (basically, chain rule)
    &lt;ul&gt;
      &lt;li&gt;this is why it’s called “backprop”&lt;/li&gt;
      &lt;li&gt;“propagates the gradient backwards through the layers”&lt;/li&gt;
      &lt;li&gt;this suggests a modular (layerwise) implementation:
        &lt;ul&gt;
          &lt;li&gt;each layer is a module
  &lt;img src=&quot;/home/assets/images/ML_part2/modular_implementation.png&quot; alt=&quot;modular_implementation.png&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;complexity&quot;&gt;Complexity&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;“performs on the order of one &lt;strong&gt;Jacobian product&lt;/strong&gt; per node in the graph” (Goodfellow, Bengio)
    &lt;ul&gt;
      &lt;li&gt;This can be seen from the fact that Backprop visits each edge (of the computational graph for this problem) only once&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;“[…] the amount of computation required for performing the back-propagation &lt;strong&gt;scales linearly with the number of edges&lt;/strong&gt; in $\mathcal{G}$, where the computation &lt;strong&gt;&lt;mark&gt;for each edge&lt;/mark&gt;&lt;/strong&gt; corresponds to computing
    &lt;ul&gt;
      &lt;li&gt;a partial derivative (of one node with respect to one of its parents) as well as performing&lt;/li&gt;
      &lt;li&gt;one multiplication and&lt;/li&gt;
      &lt;li&gt;one addition.” (Goodfellow, Bengio)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;computational-graphs&quot;&gt;Computational Graphs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;the following texts from &lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt; describe the same graphs as Olah is describing in his &lt;a href=&quot;https://colah.github.io/posts/2015-08-Backprop/&quot;&gt;blog post&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;“That algorithm specifies the &lt;strong&gt;forward propagation&lt;/strong&gt; computation, which we could put in a graph $\mathcal{G}$. In order to perform &lt;strong&gt;back-propagation&lt;/strong&gt;, we can construct a computational graph that depends on $\mathcal{G}$ and adds to it an extra set of nodes. These form a &lt;strong&gt;subgraph&lt;/strong&gt; $\mathcal{B}$ with one node per node of $\mathcal{G}$. Computation in $\mathcal{B}$ proceeds in exactly the reverse of the order of computation in $\mathcal{G}$, and each node of $\mathcal{B}$ computes the derivative $\frac{\partial u^{(n)}}{\partial u^{(i)}}$ associated with the &lt;strong&gt;forward graph&lt;/strong&gt; node $u^{(i)}$.” (Goodfellow, Bengio)&lt;/li&gt;
      &lt;li&gt;“The subgraph $\mathcal{B}$ contains exactly one edge for each edge from node $u^{(j)}$ to node $u^{(i)}$ of $\mathcal{G}$.” (Goodfellow, Bengio)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;a computer programming method
    &lt;ul&gt;
      &lt;li&gt;though, in literature one often finds the plural form “dynamic programming methods”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner
    &lt;ul&gt;
      &lt;li&gt;if this “breaking down” is possible for a problem, then the problem is said to have &lt;strong&gt;optimal substructure&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;example-fibonacci-sequence&quot;&gt;Example: Fibonacci sequence&lt;/h4&gt;

&lt;p&gt;source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming#Fibonacci_sequence&quot;&gt;https://en.wikipedia.org/wiki/Dynamic_programming#Fibonacci_sequence&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;→&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;→&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;−&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;−&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;This technique of saving values that have already been calculated is called &lt;strong&gt;memoization&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The function requires only $\mathcal{O}(n)$ time instead of &lt;strong&gt;exponential time&lt;/strong&gt; (but requires $\mathcal{O}(n)$ space)
    &lt;ul&gt;
      &lt;li&gt;i.e. the number of common subexpressions is reduced &lt;strong&gt;without regard to memory&lt;/strong&gt;!&lt;/li&gt;
      &lt;li&gt;note: sometimes recalculating instead of storing can be a good decision, &lt;strong&gt;if memory is limited&lt;/strong&gt;!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;relation-to-backprop&quot;&gt;Relation to Backprop&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Backprop stores the $y_i^{(k-1)}$ during the forward pass and re-uses it during the backward pass to calculate $\frac{\partial E}{\partial w_{ji}^{(k-1)}}=y_i^{(k-1)}\frac{\partial E}{\partial w_{ji}^{(k-1)}}$ (memoization, Dynamic Programming)&lt;/li&gt;
  &lt;li&gt;During the backward pass Backprop visits each edge only once (see above) and gradients that have already been calculated are saved in memory (cf. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_table[u[i]]&lt;/code&gt; in Algo 6.2 or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g&lt;/code&gt; in Algo 6.4 Goodfellow, Bengio)! (memoization, Dynamic Programming)
    &lt;ul&gt;
      &lt;li&gt;this is analogous to the Fibonacci Sequence Algo’s map &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt; (see above) which saves the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fib(n − 1) + fib(n − 2)&lt;/code&gt; that have already been calculated in memory&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(cf. Figure 6.9 in Goodfellow, Bengio) Back-propagation avoids the exponential explosion in &lt;strong&gt;repeated subexpressions&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;similar to the Fibonacci example “the back-propagation algorithm is designed to reduce the number of common subexpressions &lt;strong&gt;without regard to memory&lt;/strong&gt;.” (Goodfellow, Bengio)&lt;/li&gt;
  &lt;li&gt;“When the memory required to store the value of these expressions is low, the back-propagation approach of equation 6.52 &lt;img src=&quot;/assets/images/goodfellow_ml/Goodf_6_50-6_53.png&quot; alt=&quot;6.52&quot; /&gt; is clearly preferable because of its reduced runtime. However, equation 6.53 is also a valid implementation of the chain rule, and is useful &lt;strong&gt;when memory is limited&lt;/strong&gt;.” (Goodfellow, Bengio)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;softmax&quot;&gt;Softmax&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Softmax loss:
    &lt;ul&gt;
      &lt;li&gt;die inputs $\mathbf{w}_k^\top\mathbf{x}$ und $\mathbf{w}_j^\top\mathbf{x}$ für $j=1,\ldots,K$ kommen vom vorigen FC layer (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.Linear&lt;/code&gt; layer), d.h. die $\mathbf{w}_j$ sind Spalten der FC layer parameter Matrix $\mathbf{W}$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;der Softmax layer selbst hat keine Parameter&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;der Softmax layer kann als eine Art activation function gesehen werden, der $\mathbf{Wx}$ transformiert [ähnlich wie RELU]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[&lt;a href=&quot;https://stackoverflow.com/a/52662650&quot;&gt;source&lt;/a&gt;] Softmax is a &lt;strong&gt;parameter free&lt;/strong&gt; activation function like RELU, Tanh or Sigmoid: it doesn’t need to be trained. It only computes the exponential of every logit and then normalize the output vector by the sum of the exponentials.&lt;/li&gt;
  &lt;li&gt;&lt;img src=&quot;/home/assets/images/ML_part2/softmax.png&quot; alt=&quot;softmax.png&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;implementing-softmax-correctly&quot;&gt;Implementing Softmax Correctly&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Problem: Exponentials get very big and can have very different magnitudes
    &lt;ul&gt;
      &lt;li&gt;Solution:
        &lt;ul&gt;
          &lt;li&gt;Evaluate $\ln{(\sum_{j=1}^K\exp{(\mathbf{w}_j^\top\mathbf{x})})}$ in the denominator &lt;strong&gt;before&lt;/strong&gt; calculating the fraction&lt;/li&gt;
          &lt;li&gt;since $\text{softmax}(\mathbf{a} + \mathbf{b}) = \text{softmax}(\mathbf{a})$ for all $\mathbf{b}\in\mathbb{R}^D$, one can subtract the largest $\mathbf{w}_j$ from the others
            &lt;ul&gt;
              &lt;li&gt;(entspricht $\mathbf{a}=\mathbf{w}_j^\top\mathbf{x}$ und $\mathbf{b}=\mathbf{w}_M^\top\mathbf{x}$ bzw. Kürzen des Bruches mit $\exp{(\mathbf{w}_M^\top\mathbf{x})}$, wobei $\mathbf{w}_M$ das größte weight ist)&lt;/li&gt;
              &lt;li&gt;(egal, ob $\mathbf{b}$ von $\mathbf{x}$ abhängt oder nicht!)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mlp-in-numpy-from-scratch&quot;&gt;MLP in numpy from scratch&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;see &lt;a href=&quot;https://htmlpreview.github.io/?https://github.com/pharath/home/blob/master/_posts_html/notebooks_in_html/Expl_NN_in_numpy_copy.html&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stochastic-learning-vs-batch-learning&quot;&gt;Stochastic Learning vs Batch Learning&lt;/h2&gt;

&lt;p&gt;[source: LeCun et al. “Efficient BackProp”]&lt;/p&gt;

&lt;h3 id=&quot;sgd-one-at-a-time&quot;&gt;SGD (one at a time)&lt;/h3&gt;

&lt;h4 id=&quot;pros&quot;&gt;Pros&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Pros:
    &lt;ol&gt;
      &lt;li&gt;is usually much faster than batch learning
        &lt;ul&gt;
          &lt;li&gt;consider large redundant data set
            &lt;ul&gt;
              &lt;li&gt;example: training set of size 1000 is inadvertently composed of 10 identical copies of a set with 100 samples&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;also often results in better solutions because of the noise in the updates
        &lt;ul&gt;
          &lt;li&gt;because the noise present in the updates can result in the weights jumping into the basin of another, possibly deeper, local minimum. This has been demonstrated in certain simplified cases&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;can be used for tracking changes
        &lt;ul&gt;
          &lt;li&gt;useful when the function being modeled is changing over time&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Small batches can offer a &lt;strong&gt;regularizing effect&lt;/strong&gt; (Wilson and Martinez, 2003), &lt;strong&gt;perhaps due to the noise they add&lt;/strong&gt; to the learning process. &lt;strong&gt;Generalization error is often best&lt;/strong&gt; for a batch size of 1.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;cons&quot;&gt;Cons&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Cons:
    &lt;ol&gt;
      &lt;li&gt;noise also prevents full convergence to the minimum
        &lt;ul&gt;
          &lt;li&gt;Instead of converging to the exact minimum, the convergence stalls out due to the &lt;strong&gt;weight fluctuations&lt;/strong&gt;:
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;size&lt;/strong&gt; of the fluctuations depend on the degree of noise of the stochastic updates:
                &lt;ul&gt;
                  &lt;li&gt;The variance of the fluctuations around the local minimum is proportional to the learning rate $\eta$
                    &lt;ul&gt;
                      &lt;li&gt;which in turn is inversely proportional to the number of patterns presented $\eta\propto\frac{c}{t}$ [see paper]&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;“So in order &lt;strong&gt;to reduce the fluctuations&lt;/strong&gt; we can either”
                &lt;ol&gt;
                  &lt;li&gt;“decrease (anneal) the learning rate or”
                    &lt;ul&gt;
                      &lt;li&gt;start with large $\eta$ and decrease $\eta$ as the training proceeds&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;“have an adaptive batch size.”
                    &lt;ul&gt;
                      &lt;li&gt;start with small mini-batch size $t$ and increase mini-batch size $t$ as the training proceeds
        - determining an appropriate $t$ is as difficult as determining an appropriate $\eta$ and, therefore, methods (i) and (ii) are equally efficient at reducing these weight fluctuations&lt;/li&gt;
                      &lt;li&gt;Note: This is &lt;strong&gt;only valid in the noise regime at the end of training&lt;/strong&gt; and not a general rule for training!
                        &lt;ul&gt;
                          &lt;li&gt;&lt;strong&gt;in the beginning of the training&lt;/strong&gt; for a non-convex, non-quadratic error surface simple rules like this one do not apply, i.e. increasing the mini-batch size may have a different effect than decreasing the learning rate!&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
              &lt;li&gt;however, this weight fluctuation problem may be “less severe than one thinks because of &lt;strong&gt;generalization&lt;/strong&gt;”
                &lt;ul&gt;
                  &lt;li&gt;i.e. “&lt;strong&gt;Overtraining&lt;/strong&gt; may occur long before the &lt;strong&gt;noise regime&lt;/strong&gt; is even reached.”&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convergence&quot;&gt;Convergence&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;konvergiert &lt;strong&gt;nicht&lt;/strong&gt; immer, im Ggs. zu Batch GD!
    &lt;ul&gt;
      &lt;li&gt;Wikipedia “SGD”:
        &lt;ul&gt;
          &lt;li&gt;The convergence of stochastic gradient descent has been analyzed using the theories of &lt;strong&gt;convex minimization&lt;/strong&gt; and of &lt;strong&gt;stochastic approximation&lt;/strong&gt;.&lt;/li&gt;
          &lt;li&gt;Briefly, when the learning rates $\eta$ decrease with an appropriate rate, and subject to relatively mild assumptions, stochastic gradient descent converges &lt;strong&gt;almost surely&lt;/strong&gt; to a &lt;strong&gt;global minimum&lt;/strong&gt; when the objective function is &lt;strong&gt;convex&lt;/strong&gt; or pseudoconvex, and &lt;strong&gt;otherwise&lt;/strong&gt; converges almost surely to a &lt;strong&gt;local minimum&lt;/strong&gt;.
            &lt;ul&gt;
              &lt;li&gt;This is in fact a consequence of the &lt;strong&gt;Robbins-Siegmund theorem&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-gd-entire-training-set&quot;&gt;Batch GD (entire training set)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Pros:
    &lt;ul&gt;
      &lt;li&gt;Conditions of convergence are &lt;strong&gt;well understood&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Many &lt;strong&gt;acceleration techniques&lt;/strong&gt; (most &lt;a href=&quot;#2nd_order_methods&quot;&gt;2nd order methods&lt;/a&gt;, e.g. conjugate gradient) only operate in batch learning.&lt;/li&gt;
      &lt;li&gt;Theoretical analysis of the weight dynamics and convergence rates are simpler&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons:
    &lt;ul&gt;
      &lt;li&gt;redundancy can make batch learning much slower than on-line&lt;/li&gt;
      &lt;li&gt;often results in worse solutions because of the absence of noise in the updates
        &lt;ul&gt;
          &lt;li&gt;will discover the minimum of whatever basin the weights are initially placed&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;changes go undetected and we obtain rather bad results since we are likely to average over several rules&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;mini-batch-gd&quot;&gt;Mini-batch GD&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;LeCun “Efficient BackProp”:
    &lt;ul&gt;
      &lt;li&gt;Another method to remove noise [in SGD] is to use “mini-batches”, that is, start with a small batch size and increase the size as training proceeds.
        &lt;ul&gt;
          &lt;li&gt;However, deciding the rate at which to increase the batch size and which inputs to include in the small batches is as difficult as determining the proper learning rate. &lt;strong&gt;Effectively the size of the learning rate in stochastic learning corresponds to the respective size of the mini batch.&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Note also that the problem of removing the noise in the data may be less critical than one thinks because of generalization. &lt;strong&gt;Overtraining may occur long before the noise regime is even reached.&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[Jastrzębski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., &amp;amp; Storkey, A., (2018, October), “&lt;strong&gt;Width of Minima&lt;/strong&gt; Reached by Stochastic Gradient Descent is Influenced by &lt;strong&gt;Learning Rate to Batch Size Ratio&lt;/strong&gt;”]
    &lt;ul&gt;
      &lt;li&gt;The authors give the mathematical and empirical foundation to the idea that the &lt;strong&gt;ratio of learning rate to batch size&lt;/strong&gt; influences the generalization capacity of DNN. They show that this ratio plays a major role in the &lt;strong&gt;width of the minima&lt;/strong&gt; found by SGD. &lt;strong&gt;The higher ratio the wider is minima and better generalization&lt;/strong&gt;. &lt;a href=&quot;https://stackoverflow.com/a/55690257&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hyperparameter-tuning-and-optimization&quot;&gt;Hyperparameter Tuning and Optimization&lt;/h2&gt;

&lt;h3 id=&quot;manual-hyperparameter-search&quot;&gt;Manual Hyperparameter Search&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The primary &lt;strong&gt;goal&lt;/strong&gt; of manual hyperparameter search is to adjust the effective capacity of the model to match the complexity of the task.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;overfitting&quot;&gt;Overfitting&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: training error and test error diverge (see below: &lt;strong&gt;generalization gap&lt;/strong&gt;), if we use too many parameters&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Causes&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;large $\mathbf{w}$ magnitudes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;add weight decay/regularization term
        &lt;ul&gt;
          &lt;li&gt;does &lt;strong&gt;not&lt;/strong&gt; reduce the generalization gap, but rather reduces the capacity of the model!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dropout (i.e. randomly switching off units during training)&lt;/li&gt;
      &lt;li&gt;data augmentation (i.e. artificially enlarge the dataset using label-preserving transformation)&lt;/li&gt;
      &lt;li&gt;(validation-based) early stopping
        &lt;ol&gt;
          &lt;li&gt;split training data into training and test set&lt;/li&gt;
          &lt;li&gt;train only on training set and evaluate test error once in a while (e.g. after every 5th epoch)&lt;/li&gt;
          &lt;li&gt;stop training as soon as error on test set is higher than it was the last time it was checked&lt;/li&gt;
          &lt;li&gt;use the weights the NN had in the previous iteration as final weights&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;extension&lt;/strong&gt;: use cross-validation
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;complication&lt;/strong&gt;: multiple local minima&lt;/li&gt;
              &lt;li&gt;there are many ad hoc rules to decide when overfitting has truly begun&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;reduce number of parameters (if there is no other way)&lt;/li&gt;
      &lt;li&gt;use a different model
        &lt;ul&gt;
          &lt;li&gt;use transfer learning&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;capacity&quot;&gt;Capacity&lt;/h3&gt;

&lt;h4 id=&quot;forms-of-capacity&quot;&gt;Forms of Capacity&lt;a name=&quot;forms_capacity&quot;&gt;&lt;/a&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;representational capacity&lt;/strong&gt; of a model
        &lt;ul&gt;
          &lt;li&gt;which family of functions the learning algorithm can choose from when varying the parameters&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;effective capacity&lt;/strong&gt;&lt;a name=&quot;effective_capacity&quot;&gt;&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;the &lt;strong&gt;effective capacity&lt;/strong&gt; may be less than the &lt;strong&gt;representational capacity&lt;/strong&gt; because of additional limitations, e.g.:
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;imperfection of the optimization algorithm&lt;/strong&gt;: in practice, the learning algorithm does not actually find the best function, but merely one that significantly reduces the training error.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Effective capacity is &lt;strong&gt;constrained by three factors&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;the representational capacity of the model,&lt;/li&gt;
          &lt;li&gt;the ability of the learning algorithm to successfully minimize the cost function used to train the model, and&lt;/li&gt;
          &lt;li&gt;the degree to which the cost function and training procedure regularize the model.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;A model with more layers and more hidden units per layer has higher &lt;strong&gt;representational capacity&lt;/strong&gt; - it is capable of representing more complicated functions.&lt;/li&gt;
      &lt;li&gt;It can &lt;strong&gt;not&lt;/strong&gt; necessarily actually &lt;strong&gt;learn all of these functions&lt;/strong&gt; though, if the training algorithm cannot discover that certain functions do a good job of minimizing the training cost, or if regularization terms such as weight decay forbid some of these functions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;changing-a-models-capacity&quot;&gt;Changing a Model’s Capacity&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;[Representation Capacity:]
        &lt;ul&gt;
          &lt;li&gt;by changing the number of input features it [the model] has,
            &lt;ul&gt;
              &lt;li&gt;and simultaneously adding new parameters associated with those features&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;:
            &lt;ul&gt;
              &lt;li&gt;In polynomial regression example:
                &lt;ul&gt;
                  &lt;li&gt;nur &lt;strong&gt;ein&lt;/strong&gt; input feature (mit polynomial basis function)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;[Effective Capacity:]
        &lt;ul&gt;
          &lt;li&gt;see &lt;a href=&quot;#forms_capacity&quot;&gt;forms_capacity&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;quantifying-a-models-capacity-statistical-learning-theory&quot;&gt;Quantifying a Model’s Capacity (Statistical Learning Theory)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;VC dimension&lt;/li&gt;
  &lt;li&gt;The most important results in statistical learning theory show that
    &lt;ul&gt;
      &lt;li&gt;(Vapnik and Chervonenkis, 1971; Vapnik, 1982; Blumer et al., 1989; Vapnik, 1995)
        &lt;ul&gt;
          &lt;li&gt;the &lt;strong&gt;discrepancy between training error and generalization error&lt;/strong&gt; [“generalization gap”] is bounded from above by a quantity that
            &lt;ul&gt;
              &lt;li&gt;grows as the model capacity grows but&lt;/li&gt;
              &lt;li&gt;shrinks as the number of training examples increases.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;these bounds provide intellectual justification that machine learning algorithms can work&lt;/li&gt;
          &lt;li&gt;however, these bounds are rarely used in practice because:
            &lt;ul&gt;
              &lt;li&gt;bounds are often quite loose&lt;/li&gt;
              &lt;li&gt;it is difficult to determine the capacity of deep learning models
                &lt;ul&gt;
                  &lt;li&gt;because “effective capacity is limited by the capabilities of the optimization algorithm, and we have little theoretical understanding of the very general non-convex optimization problems involved in deep learning.”&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;relationship-between-capacity-and-error&quot;&gt;Relationship between Capacity and Error&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;terminology: test error = generalization error&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: Figure 5.3 shows the &lt;strong&gt;capacity&lt;/strong&gt; on the x-axis and &lt;strong&gt;not&lt;/strong&gt; the training epochs!
&lt;img src=&quot;/home/assets/images/goodfellow_ml/capacity_and_error.png&quot; alt=&quot;capacity_and_error.png&quot; /&gt;
source: &lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;basically the same plot as in &lt;a href=&quot;#Bishop_2006&quot;&gt;Bishop_2006&lt;/a&gt; “polynomial curve fitting” training and test error $E_{\text{RMS}}$ vs. order of the polynomial $M$ (where $M$ corresponds to &lt;strong&gt;capacity&lt;/strong&gt; here)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-curve-diagnostics&quot;&gt;Learning Curve Diagnostics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;see &lt;a href=&quot;https://rstudio-conf-2020.github.io/dl-keras-tf/notebooks/learning-curve-diagnostics.nb.html&quot;&gt;Learning Curve Diagnostics and Solutions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;original version, but slightly less content: &lt;a href=&quot;https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/&quot;&gt;Jason Brownlee “How to use Learning Curves to Diagnose Machine Learning Model Performance”&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-rate&quot;&gt;Learning Rate&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;controls the &lt;a href=&quot;#effective_capacity&quot;&gt;effective capacity&lt;/a&gt; of the model&lt;/strong&gt; in a more complicated way than other hyperparameters
        &lt;ul&gt;
          &lt;li&gt;the effective capacity of the model is highest when the learning rate is &lt;strong&gt;correct&lt;/strong&gt; for the optimization problem, &lt;strong&gt;not when the learning rate is especially large or especially small&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The learning rate has a U-shaped curve for training error, illustrated in figure 11.1.
        &lt;ul&gt;
          &lt;li&gt;When the learning rate is &lt;strong&gt;too large&lt;/strong&gt;, gradient descent can inadvertently increase rather than decrease the training error.
            &lt;ul&gt;
              &lt;li&gt;In the idealized quadratic case, this occurs if the learning rate is at least twice as large as its optimal value (LeCun et al., 1998a).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;When the learning rate is &lt;strong&gt;too small&lt;/strong&gt;, training is &lt;strong&gt;not only slower, but may become permanently stuck&lt;/strong&gt; with a high training error.
            &lt;ul&gt;
              &lt;li&gt;This effect is poorly understood (it &lt;strong&gt;would not happen for a convex loss function&lt;/strong&gt;).&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adaptive-learning-rate-methods&quot;&gt;Adaptive Learning Rate Methods&lt;/h3&gt;

&lt;h4 id=&quot;methods-if-directions-of-sensitivity-are-not-axis-aligned-momentum-based-methods&quot;&gt;Methods, if Directions of Sensitivity are NOT axis-aligned (momentum based methods)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;the smoothing effects of momentum based techniques also result in &lt;strong&gt;overshooting&lt;/strong&gt; and &lt;strong&gt;correction&lt;/strong&gt;. (see &lt;a href=&quot;https://imgur.com/a/Hqolp&quot;&gt;Alec Radford&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;handle large / dynamic gradients much more poorly than gradient scaling based algorithms and vanilla SGD&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;momentum-method&quot;&gt;Momentum Method&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;start with $0.5$&lt;/li&gt;
  &lt;li&gt;once the large gradients have disappeared (plot gradient norm) and the weights are stuck in a ravine increase the momentum to $0.9$ or even $0.99$&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;nesterov-accelerated-gradient&quot;&gt;Nesterov-accelerated gradient&lt;/h5&gt;

&lt;h4 id=&quot;methods-if-directions-of-sensitivity-are-axis-aligned&quot;&gt;Methods, if Directions of Sensitivity ARE axis-aligned&lt;/h4&gt;

&lt;h5 id=&quot;separate-adaptive-learning-rates-based-methods&quot;&gt;Separate, Adaptive Learning Rates based methods&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Hinton, Lecture 6.4 — Adaptive learning rates for each connection:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;only&lt;/strong&gt; deals with &lt;strong&gt;axis-aligned effects&lt;/strong&gt; [Goodfellow: “If we believe that the directions of sensitivity are somewhat &lt;strong&gt;axis-aligned&lt;/strong&gt;, it can make sense to use a separate learning rate for each parameter, and automatically adapt these learning rates throughout the course of learning.” &lt;strong&gt;{d.h. die Achsen sind entlang der $w_{ij}$}&lt;/strong&gt;]
        &lt;ul&gt;
          &lt;li&gt;as opposed to momentum which can deal with these diagonal ellipses [i.e. correlated $\mathbf{w}_{ij}$] and go in that diagonal direction quickly&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Motivation&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;magnitudes of gradients&lt;/strong&gt; very different for different layers (smaller in early layers)
        &lt;ul&gt;
          &lt;li&gt;therefore, &lt;strong&gt;learning rates&lt;/strong&gt; should be allowed to be different for different &lt;strong&gt;layers&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;fan-in&lt;/strong&gt; very different for different layers (the fan-in determines size of &lt;strong&gt;“overshoot” effect&lt;/strong&gt;: the larger the fan-in of a unit the more prone to overshooting the “local minimum &lt;strong&gt;for that unit&lt;/strong&gt;”)
        &lt;ul&gt;
          &lt;li&gt;therefore, &lt;strong&gt;learning rates&lt;/strong&gt; should be allowed to be different for different &lt;del&gt;&lt;strong&gt;units&lt;/strong&gt;&lt;/del&gt; &lt;strong&gt;layers&lt;/strong&gt; (assuming the fan-in is the same for all units in a layer)&lt;/li&gt;
          &lt;li&gt;therefore, &lt;strong&gt;weights&lt;/strong&gt; should be initialized inversely proportional to the fan-in of the units $\Rightarrow$ Glorot initialization&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;robert-a-jacobs-method-delta-bar-delta-algorithm&quot;&gt;Robert A. Jacobs’ method (delta-bar-delta algorithm)&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;idea&lt;/strong&gt;: a change of gradient sign indicates that the last update was too big and that the algorithm has jumped over a local minimum&lt;/li&gt;
  &lt;li&gt;designed for full batch learning
    &lt;ul&gt;
      &lt;li&gt;use large minibatch
        &lt;ul&gt;
          &lt;li&gt;ensures that changes of sign are &lt;strong&gt;not&lt;/strong&gt; due to sampling error of a minibatch, but due to really going to the other side of the ravine&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;global&lt;/strong&gt; learning rate multiplied by a &lt;strong&gt;local gain per weight&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ensures that big gains decay rapidly when oscillations across a ravine start&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: unlike Rprop this method uses
    &lt;ul&gt;
      &lt;li&gt;the gradient magnitude&lt;/li&gt;
      &lt;li&gt;an additive increase (in Rprop: multiplicative increase)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/0893608088900032&quot;&gt;Robert A. Jacobs, 1988&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;delta-bar-delta&lt;/strong&gt; algorithm&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;rprop&quot;&gt;Rprop&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;only for full batch learning
    &lt;ul&gt;
      &lt;li&gt;why: Rprop violates the fundamental property of SGD “averaging over minibatches” (i.e. over sufficiently many minibatches the SGD gradient will approximate the full batch gradient)
        &lt;ul&gt;
          &lt;li&gt;Rprop gradients $\frac{\textbf{grad}}{\Vert grad\Vert}$ always have magnitude 1, so that “magnitudinal averaging” is not possible over minibatches (although “directional averaging” would be possible, but, since the gradients always have magnitude 1, this “directional averaging” does not work, too)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;make it work for minibatches, enforce “averaging over minibatches” $\Rightarrow$ &lt;strong&gt;RMSprop&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;local&lt;/strong&gt; learning rate per weight (unlike RMSprop), but &lt;strong&gt;global gain&lt;/strong&gt; $\eta^+$ or $\eta^-$ respectively
    &lt;ul&gt;
      &lt;li&gt;one initial learning rate $\gamma_{ij}^{(0)}$ per weight $w_{ij}$ which will be updated in each iteration by a &lt;strong&gt;global gain&lt;/strong&gt; $\eta^+$ or $\eta^-$ respectively&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hinton, Lecture 6.5 — Rmsprop: normalize the gradient:
    &lt;ul&gt;
      &lt;li&gt;for issues like escaping from &lt;strong&gt;plateaus with very small gradients&lt;/strong&gt; this is a great technique
        &lt;ul&gt;
          &lt;li&gt;because even with tiny gradients we will take quite big steps&lt;/li&gt;
          &lt;li&gt;we could not achieve that just by turning up the learning rate because then the steps we took for weights that had big gradients would be much too big&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;combines the idea of just using the &lt;strong&gt;sign of the gradient&lt;/strong&gt; with the idea of making the &lt;strong&gt;step size&lt;/strong&gt; depend on which weight it is
        &lt;ul&gt;
          &lt;li&gt;so to decide how much to change a weight
            &lt;ul&gt;
              &lt;li&gt;you &lt;strong&gt;don’t&lt;/strong&gt; look at the &lt;strong&gt;magnitude of the gradient&lt;/strong&gt; $\frac{\partial E}{\partial w_{ij}}$,&lt;/li&gt;
              &lt;li&gt;we just look at the &lt;strong&gt;sign of the gradient&lt;/strong&gt; $\frac{\partial E}{\partial w_{ij}}$,&lt;/li&gt;
              &lt;li&gt;but you do look at the &lt;strong&gt;step size&lt;/strong&gt; $\eta_{ij}$ that you have decided on for that weight and that step size &lt;strong&gt;adapts over time&lt;/strong&gt;, again, without looking at the magnitude of the gradient
                &lt;ul&gt;
                  &lt;li&gt;increase step size multiplicatively (if last 2 gradient signs agree)
                    &lt;ul&gt;
                      &lt;li&gt;as opposed to Robert Jacobs’ “Adpative Learning Rate” method which increases &lt;strong&gt;additively&lt;/strong&gt;&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;decrease step size multiplicatively (if last 2 gradient signs disagree)&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;gradient-scaling-based-methods&quot;&gt;Gradient Scaling based methods&lt;/h5&gt;

&lt;h6 id=&quot;adagrad&quot;&gt;AdaGrad&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;“The AdaGrad algorithm, shown in algorithm 8.4,
        &lt;ul&gt;
          &lt;li&gt;individually adapts the learning rates of all model parameters by scaling them &lt;strong&gt;inversely proportional to the square root of the sum of &lt;mark&gt;all&lt;/mark&gt; [im Ggs. zu RMSprop] of their historical squared values&lt;/strong&gt; &lt;a href=&quot;https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;(Duchi et al., 2011)&lt;/a&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;[Effekt wie bei Momentum, nur axis-aligned error surface contours]
        &lt;ul&gt;
          &lt;li&gt;The parameters with the largest partial derivative of the loss &lt;strong&gt;[largest gradients]&lt;/strong&gt; have a correspondingly &lt;strong&gt;rapid decrease&lt;/strong&gt; in their learning rate, while parameters with small partial derivatives &lt;strong&gt;[small gradients]&lt;/strong&gt; have a relatively &lt;strong&gt;small decrease&lt;/strong&gt; in their learning rate.
            &lt;ul&gt;
              &lt;li&gt;The &lt;strong&gt;net effect&lt;/strong&gt; is greater progress in the more gently sloped directions of parameter space [wie bei Momentum].&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;empirically it has been found that - for training deep neural network models - the accumulation of squared gradients &lt;em&gt;from the beginning of training&lt;/em&gt; can result in a &lt;strong&gt;premature and excessive decrease in the effective learning rate&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;AdaGrad performs well for some but not all deep learning models.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;rmsprop&quot;&gt;RMSprop&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;global&lt;/strong&gt; learning rate&lt;/li&gt;
  &lt;li&gt;eigentlich eine Verbesserung von AdaGrad:
    &lt;ul&gt;
      &lt;li&gt;Zaheer, Shaziya, “A Study of the Optimization Algorithms in Deep Learning”:
        &lt;ul&gt;
          &lt;li&gt;RMSProp changes &lt;del&gt;the&lt;/del&gt; adagrad in a way how the gradient is accumulated.
            &lt;ul&gt;
              &lt;li&gt;Gradients are accumulated into an exponentially weighted average.&lt;/li&gt;
              &lt;li&gt;RMSProp &lt;strong&gt;discards the history&lt;/strong&gt; and &lt;strong&gt;maintains only recent gradient information&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Andrew Ng, Oscillation sketch:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Richtung&lt;/strong&gt; für jede Dimension/Achse $w_{ij}$ wird über &lt;strong&gt;sign&lt;/strong&gt; of gradient gegeben.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Betrag&lt;/strong&gt; für jede Dimension wird über EMA gegeben (global learning rate kann sich ja nicht ändern!), weshalb oszillierende Dimensionen gedämpft werden und nicht oszillierende Dimensionen gleich bleiben bzw. sich etwas verzögert an Änderungen anpassen.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hinton:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Motivation:&lt;/strong&gt; “Rprop for mini-batch learning”
        &lt;ul&gt;
          &lt;li&gt;Rprop is equivalent to using the gradient, but also dividing by the magnitude of the gradient
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;problem:&lt;/strong&gt; we divide by a different magnitude for each mini-batch!
                &lt;ul&gt;
                  &lt;li&gt;I.e. the core idea of SGD of stochastic “averaging of weight updates over mini-batches” is violated&lt;/li&gt;
                  &lt;li&gt;in other words, the weight update noise is not the same as in standard mini-batch SGD !&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;solution:&lt;/strong&gt; force the number we divide by to be pretty much the same for nearby mini-batches&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;combines
        &lt;ul&gt;
          &lt;li&gt;the robustness of Rprop (which allows to combine gradients in the right way)&lt;/li&gt;
          &lt;li&gt;efficiency of mini-batches&lt;/li&gt;
          &lt;li&gt;averaging of gradients over mini-batches&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Hinton:
        &lt;ul&gt;
          &lt;li&gt;notice, that we are &lt;strong&gt;not adapting the learning rate separately for each connection&lt;/strong&gt; here!&lt;/li&gt;
          &lt;li&gt;This is a simpler method, where we simply, &lt;strong&gt;for each connection&lt;/strong&gt;, keep a running average of the root mean square gradient and divide by that.&lt;/li&gt;
          &lt;li&gt;Extensions:
            &lt;ul&gt;
              &lt;li&gt;can be combined with &lt;strong&gt;momentum&lt;/strong&gt;, but does not help as much as momentum normally does
                &lt;ul&gt;
                  &lt;li&gt;that needs more investigation&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;can be combined with &lt;strong&gt;adaptive learning rates for each connection&lt;/strong&gt;
                &lt;ul&gt;
                  &lt;li&gt;Hinton: “That needs more investigation. I just do not know how helpful that will be.”&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/goodfellow_ml/RMSprop.png&quot; alt=&quot;RMSprop&quot; /&gt;
&lt;a href=&quot;#Goodfellow_2016&quot;&gt;source: Goodfellow_2016&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;learning-rate-schedules&quot;&gt;Learning rate schedules&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;can be used &lt;strong&gt;instead of&lt;/strong&gt; using “adaptive learning rate methods” (see above)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Learning_rate&quot;&gt;source&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations.&lt;/li&gt;
      &lt;li&gt;This is mainly [in most implementations] done with &lt;strong&gt;two parameters&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;decay&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;momentum&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;There are many different learning rate schedules but the most common are &lt;strong&gt;time-based&lt;/strong&gt;, &lt;strong&gt;step-based&lt;/strong&gt; and &lt;strong&gt;exponential&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;saddle-points-and-local-minima&quot;&gt;Saddle Points and Local Minima&lt;/h3&gt;

&lt;h4 id=&quot;hessian-2nd-derivative-test&quot;&gt;Hessian (2nd derivative test)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;When the Hessian is positive definite (all its eigenvalues are positive), the point is a &lt;strong&gt;local minimum&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;when the Hessian is negative definite (all its eigenvalues are negative), the point is a &lt;strong&gt;local maximum&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;In multiple dimensions, it is actually possible to find positive evidence of &lt;strong&gt;saddle points&lt;/strong&gt; in some cases.
    &lt;ul&gt;
      &lt;li&gt;When at least one eigenvalue is positive and at least one eigenvalue is negative, we know that $x$ is a local maximum on one cross section of $f$ but a local minimum on another cross section.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The test is &lt;strong&gt;inconclusive whenever&lt;/strong&gt; all of the non-zero eigenvalues have the same sign, but at least one eigenvalue is zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-condition-number-of-the-hessian&quot;&gt;The Condition Number of the Hessian&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;tool, um “long canyon” Probleme (i.e. “krümmungsbasierte” Konvergenzprobleme) zu messen
&lt;img src=&quot;/home/assets/images/ML_part2/long_canyon.png&quot; alt=&quot;long_canyon.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;“In multiple dimensions, there is a different second derivative for each direction at a single point. The &lt;strong&gt;condition number&lt;/strong&gt; of the Hessian at this point &lt;strong&gt;measures how much the second derivatives differ from each other&lt;/strong&gt;.”
        &lt;ul&gt;
          &lt;li&gt;&lt;mark&gt;&quot;When the Hessian has a poor condition number, **gradient descent performs poorly**.&quot;&lt;/mark&gt;
            &lt;ul&gt;
              &lt;li&gt;“This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. &lt;strong&gt;Gradient descent is unaware of this change in the derivative&lt;/strong&gt;” [“unaware”, weil “change in the derivative” mit Hessian gemessen wird und GD Hessian nicht benutzt]
                &lt;ul&gt;
                  &lt;li&gt;information about the &lt;strong&gt;change in the function&lt;/strong&gt; is contained in &lt;strong&gt;1st derivative&lt;/strong&gt;&lt;/li&gt;
                  &lt;li&gt;information about the &lt;strong&gt;change in the derivative&lt;/strong&gt; is contained in the &lt;strong&gt;2nd derivative!&lt;/strong&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;“so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.”&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;the-proliferation-of-saddle-points-in-higher-dimensions&quot;&gt;The proliferation of saddle points in higher dimensions&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Goodfellow_2016:
    &lt;ul&gt;
      &lt;li&gt;At a saddle point, the &lt;strong&gt;Hessian matrix&lt;/strong&gt; [important tool for analyzing critical points, e.g. determine Eigenvalues to check, if it is a saddle point, a local minimum or a maximum] has both positive and negative eigenvalues.&lt;/li&gt;
      &lt;li&gt;Points lying &lt;strong&gt;along eigenvectors&lt;/strong&gt; associated with &lt;strong&gt;positive eigenvalues&lt;/strong&gt; have greater cost than the saddle point, while points lying &lt;strong&gt;along negative eigenvalues&lt;/strong&gt; have lower value.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://videolectures.net/deeplearning2015_bengio_theoretical_motivations/&quot;&gt;Bengio, summer school lecture&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;wrong idea: non-convex error surface $\Rightarrow$ many local minima $\Rightarrow$ we don’t get any guarantees to find the optimal solution and furthermore, we might be stuck in very poor solutions
        &lt;ul&gt;
          &lt;li&gt;one of the reasons why researchers lost interest in NNs&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;there is theoretical and empirical evidence that this issue of non-convexity is not an issue at all
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1405.4604&quot;&gt;Pascanu, Dauphin, Ganguli, Bengio&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1406.2572&quot;&gt;Dauphin, Pascanu, Gulcehre, Cho, Ganguli, Bengio&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.0233&quot;&gt;Choromanska, Henaff, Mathieu, Ben Arous &amp;amp; LeCun&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;these papers describe saddle points in high dimensions:
                &lt;ul&gt;
                  &lt;li&gt;for a local minimum all the directions starting from the &lt;strong&gt;critical point&lt;/strong&gt; (i.e. derivative equal to zero) must go up (and for a local maximum v.v.)&lt;/li&gt;
                  &lt;li&gt;the more dimensions you have the more “unlikely” (if you construct a function with some randomness and you choose independently whether a direction starting from the critical point goes up or down) it gets that &lt;strong&gt;all&lt;/strong&gt; the directions go up at a critical point
                    &lt;ul&gt;
                      &lt;li&gt;this gets &lt;strong&gt;exponentially&lt;/strong&gt; more unlikely with each additional dimension!&lt;/li&gt;
                      &lt;li&gt;&lt;strong&gt;except&lt;/strong&gt;, if you are at the bottom of your high-dimensional landscape, i.e. near the global minimum
                        &lt;ul&gt;
                          &lt;li&gt;because if you have a minimum near the global minimum you cannot go further down, so all the directions have to go up
                            &lt;ul&gt;
                              &lt;li&gt;this means, you have local minima [for high-dimensional functions], but they are very close to the global minimum in terms of their objective function&lt;/li&gt;
                            &lt;/ul&gt;
                          &lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Note in talk: “Of course, you could always construct a high-dimensional function, where there is a local minimum which is &lt;strong&gt;not&lt;/strong&gt; close to the global minimum. You could just place a local minimum by hand at some higher point of the error surface. I just argue that local minima are very &lt;strong&gt;likely&lt;/strong&gt; not the problem, when training gets stuck.”&lt;/li&gt;
              &lt;li&gt;plot: training error and norm of the gradients:
                &lt;ul&gt;
                  &lt;li&gt;you see the error go down and it plateaus and then, if you are lucky, something happens and then it goes down again and it plateaus again and then we might think this is the best we can get, &lt;strong&gt;but&lt;/strong&gt; the gradients do not approach zero, what’s going on?
                    &lt;ul&gt;
                      &lt;li&gt;this looks like we are approaching a &lt;strong&gt;saddle point&lt;/strong&gt;
                        &lt;ul&gt;
                          &lt;li&gt;the gradient is bouncing around and not finding the escape route&lt;a name=&quot;gradient_fluctuating_on_plateau&quot;&gt;&lt;/a&gt;&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                      &lt;li&gt;my intuition is that this such a high-dimensional space that there is only a few dimensions where it is going down and somehow simple Gradient Descent is not finding them (maybe because of curvature problems [Goodfellow_2016, Fig. 4.5] or other things)
                        &lt;ul&gt;
                          &lt;li&gt;What I’m saying is, yes, it is worthwhile to continue exploring other optimization algorithms beyond simple GD, but we need to take into account the fact that maybe what we are fighting is not local minima, it might be something else. It might be something classical like differences of curvature or something more subtle.&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;as you go further down it &lt;strong&gt;spends more and more time&lt;/strong&gt; on these plateaus (and it gets harder and harder to decide whether we are still on a saddle point), presumably &lt;strong&gt;because there are less directions going down&lt;/strong&gt; [i.e. we are close to the global minimum]&lt;/li&gt;
                  &lt;li&gt;cite: &lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
                    &lt;ul&gt;
                      &lt;li&gt;experts now suspect that, for sufficiently large neural networks, &lt;strong&gt;most local minima have a low cost function value&lt;/strong&gt;, and that it is not important to find a true global minimum rather than to find a point in parameter space that has low but not minimal cost&lt;/li&gt;
                      &lt;li&gt;Many practitioners attribute nearly all difficulty with neural network optimization to &lt;strong&gt;local minima&lt;/strong&gt;.
                        &lt;ul&gt;
                          &lt;li&gt;We encourage practitioners to &lt;strong&gt;carefully test&lt;/strong&gt; for specific problems.
                            &lt;ul&gt;
                              &lt;li&gt;A &lt;strong&gt;test&lt;/strong&gt; that can rule out local minima as the problem is to &lt;strong&gt;&lt;mark&gt;plot the norm of the gradient over time&lt;/mark&gt;&lt;/strong&gt;. If the norm of the gradient does not shrink to insignificant size [same problem as discussed &lt;a href=&quot;#gradient_fluctuating_on_plateau&quot;&gt;here&lt;/a&gt;], the problem is neither local minima nor any other kind of critical point. [then it’s either a saddle point or something more subtle]&lt;/li&gt;
                              &lt;li&gt;This kind of negative test can rule out local minima.
                                &lt;ul&gt;
                                  &lt;li&gt;[also einfach beim training die Gradient Norm mitplotten und nur wenn diese Norm plötzlich 0 wird ist es ein &lt;strong&gt;local Minimum&lt;/strong&gt;, sonst nicht!]&lt;/li&gt;
                                &lt;/ul&gt;
                              &lt;/li&gt;
                            &lt;/ul&gt;
                          &lt;/li&gt;
                          &lt;li&gt;[those practitioners are wrong because] In high dimensional spaces, it can be very difficult to positively establish that local minima are the problem. Many structures other than local minima also have small gradients.&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Note in talk: stopping criterion? You should be ready to wait. People discarded NNs in part because they were not ready to wait long enough (when I was a PhD student, we were ready to wait weeks, people are getting lazy these days ;))&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The team of Yoshua Bengio have experimentally found that when optimising the parameters of high-dimensional neural nets, there effectively are no local minima. Instead, there are saddle points which are local minima in some dimensions but not all. This means that training can slow down quite a lot in these points, until the network figures out how to escape, but as long as we’re willing to wait long enough then it will find a way.&lt;/li&gt;
      &lt;li&gt;Given one specific dimension, there is some small probability $p$ with which a point is a local minimum, but not a global minimum, in that dimension. Now, the probability of a point in a $1000$-dimensional space being an incorrect local minimum in all of these would be $p^{1000}$, which is just astronomically small. However, the probability of it being a local minimum in some of these dimensions is actually quite high. And when we get these minima in many dimensions at once, then training can appear to be stuck until it finds the right direction.&lt;/li&gt;
      &lt;li&gt;In addition, this probability $p$ will increase as the loss function gets closer to the global minimum.  This means that if we do ever end up at a genuine local minimum, then for all intents and purposes it will be close enough to the global minimum that it will not matter.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;For many high-dimensional non-convex functions, local minima (and maxima) are in fact rare compared to another kind of point with zero gradient: a saddle point.&lt;/li&gt;
      &lt;li&gt;Some points around a saddle point have greater cost than the saddle point, while others have a lower cost.&lt;/li&gt;
      &lt;li&gt;At a saddle point, the &lt;strong&gt;Hessian matrix&lt;/strong&gt; has both positive and negative eigenvalues.
        &lt;ul&gt;
          &lt;li&gt;Points lying along eigenvectors associated with positive eigenvalues have greater cost than the saddle point, while points lying along negative eigenvalues have lower value.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;We can think of a saddle point as being a local minimum along one cross-section [sozusagen als würde man mit einer Ebene senkrecht durch den Graph schneiden] of the cost function and a local maximum along another cross-section. See figure 4.5 for an illustration.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;shufflingorderingemphasizing-schemes&quot;&gt;Shuffling/Ordering/Emphasizing Schemes&lt;/h3&gt;

&lt;h4 id=&quot;shuffling&quot;&gt;Shuffling&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;irrelevant for full batch learning&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Example&lt;/strong&gt;: Within the training set, the images are ordered in such a way that all the dog images come first and all the cat images come after. The classes are balanced.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Effect&lt;/strong&gt;: The optimization is much harder with mini-batch gradient descent because the loss function moves by a lot when going from the one type of image to another.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Shuffle before training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ordering&quot;&gt;Ordering&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Müller, Montavon:
    &lt;ul&gt;
      &lt;li&gt;Networks learn the fastest from the most unexpected sample. Therefore, it is advisable to &lt;mark&gt;**choose a sample at each iteration that is the most unfamiliar to the system**&lt;/mark&gt;.
        &lt;ul&gt;
          &lt;li&gt;Note, this applies only to stochastic learning since the &lt;strong&gt;order&lt;/strong&gt; of input presentation &lt;strong&gt;is irrelevant for batch&lt;/strong&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;[Method 1 for “choose a sample at each iteration that is the most unfamiliar to the system”:] Of course, there is no simple way to know &lt;strong&gt;which inputs are information rich&lt;/strong&gt;,
        &lt;ul&gt;
          &lt;li&gt;however, a very simple &lt;strong&gt;trick&lt;/strong&gt; that crudely implements this idea is to simply &lt;strong&gt;&lt;mark&gt;choose successive examples that are from different classes&lt;/mark&gt;&lt;/strong&gt; since training examples belonging to the same class will most likely contain similar information.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;emphasizing-schemes&quot;&gt;Emphasizing Schemes&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Müller, Montavon:
    &lt;ul&gt;
      &lt;li&gt;[Method 2 for “choose a sample at each iteration that is the most unfamiliar to the system”:] Another heuristic for judging how much new information a training example contains is to &lt;strong&gt;&lt;mark&gt;examine the error between the network output and the target value&lt;/mark&gt;&lt;/strong&gt; when this input is presented.
        &lt;ul&gt;
          &lt;li&gt;A &lt;strong&gt;large error indicates that this input has not been learned&lt;/strong&gt; by the network and so contains a lot of new information.&lt;/li&gt;
          &lt;li&gt;Therefore, it makes sense to &lt;strong&gt;&lt;mark&gt;present this input more frequently&lt;/mark&gt;&lt;/strong&gt;.&lt;/li&gt;
          &lt;li&gt;Of course, by “large” we mean relative to all of the other training examples.&lt;/li&gt;
          &lt;li&gt;As the network trains, these relative errors will change and so should the &lt;strong&gt;frequency of presentation&lt;/strong&gt; for a particular input pattern.&lt;/li&gt;
          &lt;li&gt;A method that modifies the &lt;strong&gt;probability of appearance of each pattern&lt;/strong&gt; is called an &lt;mark&gt;**emphasizing scheme**&lt;/mark&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;However, &lt;strong&gt;one must be careful when perturbing the normal frequencies of input examples&lt;/strong&gt; because this changes the relative importance that the network places on different examples. This may or may not be desirable.
        &lt;ul&gt;
          &lt;li&gt;For example, this technique applied to data containing &lt;strong&gt;outliers&lt;/strong&gt; can be disastrous because outliers can produce large errors yet should not be presented frequently.&lt;/li&gt;
          &lt;li&gt;On the other hand, this technique can be particularly beneficial for &lt;strong&gt;boosting the performance for infrequently occurring inputs&lt;/strong&gt;, e.g. /z/ in phoneme recognition.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transforming-the-inputs-shift-scale-decorrelate&quot;&gt;Transforming the inputs (shift, scale, decorrelate)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: many possible methods for image data:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/&quot;&gt;normalization, centering, standardization&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;also many possible &lt;strong&gt;orders&lt;/strong&gt; in which these steps are done&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href=&quot;https://cs230.stanford.edu/files/cs230exam_fall18_soln.pdf&quot;&gt;source&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Normalizing the input impacts the landscape of the loss function.&lt;/li&gt;
      &lt;li&gt;The normalizing mean and variance computed on the &lt;strong&gt;training&lt;/strong&gt; set, and used to train the model, should be used to normalize &lt;strong&gt;test&lt;/strong&gt; data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hinton, Lecture 6.2 — A bag of tricks for mini batch gradient descent:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;red line&lt;/strong&gt; corresponds to the bottom of the “trough” defined by the training point $x_1=(101,99)$, the corresponding output of the simple 2 unit NN shown below is $0$
        &lt;ul&gt;
          &lt;li&gt;“trough” in 2-D: “parabolic cylinder”, see below: graph in the middle: $z=x^2$
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;superposition&lt;/strong&gt; of two &lt;strong&gt;parabolic cylinders&lt;/strong&gt; results in an &lt;strong&gt;elliptic paraboloid&lt;/strong&gt;, see below: left graph: $z=x^2+y^2$
                &lt;ul&gt;
                  &lt;li&gt;this is why $x_1$ and $x_2$ together form an elliptic paraboloid error surface&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;green line&lt;/strong&gt; corresponds to the bottom of the “trough” defined by the training point $x_2=(101,101)$, the corresponding output of the simple 2 unit NN shown below is $2$&lt;/li&gt;
      &lt;li&gt;the &lt;strong&gt;black ellipse&lt;/strong&gt; corresponds to the contours of the error surface (which in 2-D has the shape of an elliptic paraboloid)
        &lt;ul&gt;
          &lt;li&gt;this ellipse has a very &lt;strong&gt;elongated shape&lt;/strong&gt; because $x_1$ and $x_2$ are both positive and do not have average zero
            &lt;ul&gt;
              &lt;li&gt;steepest descent methods have difficulties with such “troughs”&lt;/li&gt;
              &lt;li&gt;slows down learning&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;shifting the inputs, so that the average over the whole training set is close to zero makes the error surface contours more circular (see below)
            &lt;ul&gt;
              &lt;li&gt;speeds learning with steepest descent&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/optimization/normalization1_hinton.png&quot; alt=&quot;normalization1_hinton&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/optimization/paraboloids.png&quot; alt=&quot;paraboloids&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/optimization/normalization2_hinton.png&quot; alt=&quot;normalization2_hinton&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LeCun “Efficient BackProp”:
    &lt;ol&gt;
      &lt;li&gt;“The average of each input variable over the training set should be close to zero.”
        &lt;ul&gt;
          &lt;li&gt;When all of the components of an input vector are positive, all of the updates of weights that feed into a node will be the same sign (i.e. $\text{sign}(\delta)$).
            &lt;ul&gt;
              &lt;li&gt;biases the updates in a particular direction&lt;/li&gt;
              &lt;li&gt;As a result, these weights can only all decrease or all increase &lt;strong&gt;together&lt;/strong&gt; for a given input pattern.
                &lt;ul&gt;
                  &lt;li&gt;Thus, if a weight vector must change direction it can only do so by &lt;strong&gt;zigzagging&lt;/strong&gt; which is inefficient and thus very slow.
  &lt;img src=&quot;/assets/images/optimization/normalization.JPG&quot; alt=&quot;normalization&quot; /&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;In the above example, the inputs were all positive. However, &lt;strong&gt;in general, any shift of the average input away from zero will bias the updates in a particular direction&lt;/strong&gt; and thus slow down learning.&lt;/li&gt;
          &lt;li&gt;Therefore, it is good to &lt;strong&gt;shift the inputs so that the average over the training set is close to zero&lt;/strong&gt;.
            &lt;ul&gt;
              &lt;li&gt;This heuristic should be applied &lt;strong&gt;at all layers&lt;/strong&gt; which means that we want the average of the outputs of a node to be close to zero because these outputs are the inputs to the next layer [19], chapter 10.
                &lt;ul&gt;
                  &lt;li&gt;This problem can be addressed by coordinating how the inputs are transformed with the choice of sigmoidal activation function.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;“Scale input variables so that their covariances are about the same.”
        &lt;ul&gt;
          &lt;li&gt;helps to balance out the rate at which the weights connected to the input nodes learn.&lt;/li&gt;
          &lt;li&gt;The value of the covariance should be &lt;strong&gt;matched with that of the sigmoid&lt;/strong&gt; used.&lt;/li&gt;
          &lt;li&gt;The &lt;strong&gt;exception&lt;/strong&gt; to scaling all covariances to the same value occurs
            &lt;ul&gt;
              &lt;li&gt;when it is known that some inputs are of less significance than others.
                &lt;ul&gt;
                  &lt;li&gt;In such a case, it can be beneficial to scale the less significant inputs down so that they are “less visible” to the learning process&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;“Input variables should be uncorrelated if possible.”
        &lt;ul&gt;
          &lt;li&gt;If inputs are uncorrelated then it is possible to solve for the value of $w_1$ that minimizes the error &lt;strong&gt;without any concern for $w_2$&lt;/strong&gt;, and vice versa.
            &lt;ul&gt;
              &lt;li&gt;In other words, the &lt;strong&gt;two variables are independent&lt;/strong&gt; (the system of equations is diagonal).&lt;/li&gt;
              &lt;li&gt;With correlated inputs, one must solve for both simultaneously which is a much harder problem.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Principal component analysis&lt;/strong&gt; (also known as the &lt;strong&gt;Karhunen-Loeve expansion&lt;/strong&gt;) can be used to remove linear correlations in inputs [10].&lt;/li&gt;
          &lt;li&gt;Inputs that are linearly dependent (the extreme case of correlation) may also produce &lt;strong&gt;degeneracies&lt;/strong&gt; which may slow learning.
            &lt;ul&gt;
              &lt;li&gt;Consider the case where one input is always twice the other input ($z_2 = 2z_1$).
                &lt;ul&gt;
                  &lt;li&gt;We are trying to solve in 2-D what is effectively only a 1-D problem.&lt;/li&gt;
                  &lt;li&gt;&lt;strong&gt;Ideally&lt;/strong&gt; we want to &lt;strong&gt;remove one of the inputs&lt;/strong&gt; which will &lt;strong&gt;decrease the size of the network&lt;/strong&gt;.
                    &lt;ul&gt;
                      &lt;li&gt;Summary:&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;(1) shift inputs so the mean is zero,&lt;/li&gt;
          &lt;li&gt;(2) decorrelate inputs, and&lt;/li&gt;
          &lt;li&gt;(3) equalize covariances.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;sigmoids&quot;&gt;Sigmoids&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;LeCun, “Efficient BackProp”:
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Symmetric sigmoids&lt;/strong&gt; such as hyperbolic tangent [$\tanh$] often &lt;strong&gt;converge faster&lt;/strong&gt; than the standard logistic function.&lt;/li&gt;
      &lt;li&gt;A recommended sigmoid is: $f(x) = 1.7159\tanh\frac{2}{3}x$. Since the $\tanh$ function is sometimes computationally expensive, an approximation of it by a ratio of polynomials can be used instead.
        &lt;ul&gt;
          &lt;li&gt;The constants in the recommended sigmoid given above have been chosen so that, when used with transformed inputs (see previous discussion), the &lt;strong&gt;variance of the outputs will also be close to 1&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;because the &lt;strong&gt;effective gain of the sigmoid is roughly 1&lt;/strong&gt; over its useful range.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Properties:
            &lt;ul&gt;
              &lt;li&gt;(a) $f(±1) = ±1$,
                &lt;ul&gt;
                  &lt;li&gt;[why good? später im Text] set the target values [d.h. 1 und -1] to be within the range of the sigmoid, rather than at the asymptotic values [and at the same time] &lt;strong&gt;ensure that the node is not restricted to only the linear part&lt;/strong&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;(b) the second derivative is a maximum at $x = 1$ [siehe “1.4.5 Choosing Target Values”], and
                &lt;ul&gt;
                  &lt;li&gt;[why good? später im Text] Setting the target values to the point of the maximum second derivative on the sigmoid is the &lt;strong&gt;best way to take advantage of the nonlinearity without saturating the sigmoid&lt;/strong&gt;.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;(c) the effective gain is close to $1$.
                &lt;ul&gt;
                  &lt;li&gt;[why good?] if the variance of the inputs is 1, then the variance of the outputs will also be close to 1&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Sometimes it is helpful to add a small linear term, e.g. $f(x) = \tanh(x) + ax$ so as to avoid flat spots.
        &lt;ul&gt;
          &lt;li&gt;One of the potential problems with using symmetric sigmoids is that the &lt;strong&gt;error surface can be very flat near the origin&lt;/strong&gt;.
            &lt;ul&gt;
              &lt;li&gt;For this reason it is good to &lt;strong&gt;avoid initializing with very small weights&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Because of the saturation of the sigmoids, the &lt;strong&gt;error surface is also flat far from the origin&lt;/strong&gt;.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Adding a small linear term&lt;/strong&gt; to the sigmoid can sometimes help avoid the flat regions (see chapter 9).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Sigmoids that are symmetric about the origin are preferred for the same reason that inputs should be normalized, namely, because they are more likely to produce outputs (which are inputs to the next layer) that are on average close to zero.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;biases-initialization&quot;&gt;Biases Initialization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Typically, we set the &lt;strong&gt;biases&lt;/strong&gt; for each unit to heuristically chosen constants, and initialize only the weights randomly.&lt;/li&gt;
      &lt;li&gt;The approach for setting the biases must be coordinated with the approach for settings the weights. Setting the biases to zero is compatible with most weight initialization schemes. There are a few situations where we may set some biases to non-zero values … (see Goodfellow_2016)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;perform a &lt;strong&gt;grid search&lt;/strong&gt; [Le, Hinton 2015]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;weight-initialization&quot;&gt;Weight initialization&lt;/h3&gt;

&lt;h4 id=&quot;why-does-it-matter&quot;&gt;Why does it matter?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The initial point can determine &lt;strong&gt;whether the algorithm converges at all&lt;/strong&gt;, with some initial points being so unstable that the algorithm encounters numerical difficulties and fails altogether.&lt;/li&gt;
      &lt;li&gt;When learning does converge, the initial point can determine
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;how quickly&lt;/strong&gt; learning converges and&lt;/li&gt;
          &lt;li&gt;whether it converges to a point with &lt;strong&gt;high or low cost&lt;/strong&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Also, points of comparable cost can have wildly varying &lt;strong&gt;generalization error&lt;/strong&gt;, and the initial point can affect the generalization as well&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;open-questions&quot;&gt;Open Questions&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Modern initialization strategies are simple and &lt;strong&gt;heuristic&lt;/strong&gt; [i.e. strategies work, but we do not know why].&lt;/li&gt;
  &lt;li&gt;Designing improved initialization strategies is a difficult task because neural network &lt;strong&gt;optimization is not yet well understood&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Most initialization strategies are based on achieving some nice properties &lt;strong&gt;when the network is initialized&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;However, &lt;strong&gt;we do not have a good understanding&lt;/strong&gt; of which of these properties are &lt;strong&gt;preserved&lt;/strong&gt; under which circumstances after learning begins to proceed.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A further difficulty is that some initial points may be beneficial from the viewpoint of optimization but detrimental from the viewpoint of &lt;strong&gt;generalization&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Our &lt;strong&gt;understanding of how the initial point affects generalization is especially primitive&lt;/strong&gt;, offering little to no guidance for how to select the initial point.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;symmetry-breaking-problem&quot;&gt;Symmetry breaking Problem&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coursera.org/lecture/neural-networks-deep-learning/random-initialization-XtFPI&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;for logistic regression we &lt;strong&gt;can&lt;/strong&gt; initialize the weights with zeros, but for NNs this does &lt;strong&gt;not&lt;/strong&gt; work
    &lt;ul&gt;
      &lt;li&gt;in logistic regression:
        &lt;ul&gt;
          &lt;li&gt;derivative of the binary cross entropy loss with respect to a single dimension in the weight vector $w_i$ is a function of $x_i$, which is in general different than $x_j$ when $i\neq j$
            &lt;ul&gt;
              &lt;li&gt;d.h. weight update der Komponente $w_i$ i.e. $-\eta \frac{\partial E}{\partial w_i}$ wird sich i. Allg. vom weight update der anderen Komponenten $-\eta \frac{\partial E}{\partial w_j}$ mit $i\neq j$ unterscheiden
                &lt;ul&gt;
                  &lt;li&gt;aka weights werden asymmetrisch geupdatet&lt;/li&gt;
                  &lt;li&gt;daher ist 0 initialization kein Problem&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Problem&lt;/strong&gt;: consider 2 layer NN below:
    &lt;ul&gt;
      &lt;li&gt;if $W_1$ and $W_2$ are initialized with constants or with zeros, both hidden units will calculate the same function (i.e. they are symmetric) during forward &lt;strong&gt;and&lt;/strong&gt; backward pass
        &lt;ul&gt;
          &lt;li&gt;i.e. there is no reason to have more than one hidden unit here&lt;/li&gt;
          &lt;li&gt;also applies to more than 2 hidden units&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;proof by induction:
        &lt;ul&gt;
          &lt;li&gt;first iteration: both hidden units start off by computing the same function&lt;/li&gt;
          &lt;li&gt;next iteration: still compute the same function&lt;/li&gt;
          &lt;li&gt;by induction: they will always compute the same function during training&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: break this symmetry by using randomly initialized weights&lt;/li&gt;
  &lt;li&gt;too large initialization $\Rightarrow$ sigmoid/tanh will saturate $\Rightarrow$ slows down learning
    &lt;ul&gt;
      &lt;li&gt;Thus initialize with &lt;strong&gt;small&lt;/strong&gt; Gaussian numbers, here $0.01$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$b$ does not have this “Symmetry breaking problem”, hence, it can be initialized with $0$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/home/assets/images/ML_part2/random_init.png&quot; alt=&quot;random_init.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Perhaps the only property known with complete certainty is that the initial parameters need to “&lt;strong&gt;break symmetry&lt;/strong&gt;” between different units.
        &lt;ul&gt;
          &lt;li&gt;If two hidden units with the &lt;strong&gt;same activation function&lt;/strong&gt; are connected to the &lt;strong&gt;same inputs&lt;/strong&gt;, then these units &lt;mark&gt;must have different initial parameters&lt;/mark&gt;.&lt;/li&gt;
          &lt;li&gt;If they have the &lt;strong&gt;same initial parameters&lt;/strong&gt;, then a &lt;strong&gt;deterministic learning&lt;/strong&gt; algorithm applied to a deterministic cost and model will constantly &lt;strong&gt;update both of these units in the same way&lt;/strong&gt;.
            &lt;ul&gt;
              &lt;li&gt;Even if the model or training algorithm is capable of &lt;strong&gt;using stochasticity&lt;/strong&gt; [as opposed to deterministic updates] to compute different updates for different units (for example, if one trains with &lt;strong&gt;dropout&lt;/strong&gt;), it is usually best to initialize each unit to compute a different function from all of the other units.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;This may help to make sure that no input patterns are lost in the null space of forward propagation and no gradient patterns are lost in the null space of back-propagation.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;random-initialization&quot;&gt;Random Initialization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The goal of having each unit compute a different function motivates &lt;strong&gt;random initialization&lt;/strong&gt; of the parameters.
        &lt;ul&gt;
          &lt;li&gt;We could explicitly search for a large set of basis functions that are all mutually different from each other, but this often incurs a noticeable computational cost.
            &lt;ul&gt;
              &lt;li&gt;For example, if we have at most as many outputs as inputs, we could use Gram-Schmidt orthogonalization on an initial weight matrix, and be guaranteed that each unit computes a very different function from each other unit.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Random initialization from a high-entropy distribution over a high-dimensional space is computationally cheaper and unlikely to assign any units to compute the same function as each other.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;biases-and-other-parameters&quot;&gt;Biases and other Parameters&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Typically, we set the &lt;strong&gt;biases&lt;/strong&gt; for each unit to heuristically chosen constants, and initialize only the weights randomly.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Extra parameters&lt;/strong&gt;, for example, parameters encoding the conditional variance of a prediction, are usually set to heuristically chosen constants much like the biases are.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;gaussian-or-uniform&quot;&gt;Gaussian or Uniform&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;We almost always initialize all the weights in the model to values drawn randomly from a &lt;strong&gt;Gaussian&lt;/strong&gt; or &lt;strong&gt;uniform distribution&lt;/strong&gt;.
        &lt;ul&gt;
          &lt;li&gt;The choice of Gaussian or uniform distribution does not seem to matter very much, but &lt;strong&gt;has not been exhaustively studied&lt;/strong&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;scale&quot;&gt;Scale&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;strong&gt;scale&lt;/strong&gt; of the initial distribution, however, does have a large effect on both the outcome of the optimization procedure and on the ability of the network to generalize.
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Larger initial weights&lt;/strong&gt; will yield a &lt;strong&gt;stronger symmetry breaking&lt;/strong&gt; effect, helping to avoid redundant units. They also help to avoid losing signal during forward or back-propagation through the linear component of each layer - larger values in the matrix result in larger outputs of matrix multiplication.
            &lt;ul&gt;
              &lt;li&gt;Initial weights that are &lt;strong&gt;too large&lt;/strong&gt; may, however, result in exploding values during forward propagation or back-propagation.
                &lt;ul&gt;
                  &lt;li&gt;&lt;strong&gt;In recurrent networks&lt;/strong&gt;, large weights can also result in &lt;strong&gt;chaos&lt;/strong&gt; (such extreme sensitivity to small perturbations of the input that the behavior of the deterministic forward propagation procedure appears random).
                    &lt;ul&gt;
                      &lt;li&gt;To some extent, the exploding gradient problem can be mitigated by &lt;strong&gt;gradient clipping&lt;/strong&gt; (thresholding the values of the gradients before performing a gradient descent step).&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;Large weights may also result in extreme values that &lt;strong&gt;cause the activation function to saturate&lt;/strong&gt;, causing complete loss of gradient through saturated units.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;These &lt;strong&gt;competing factors&lt;/strong&gt; determine the &lt;strong&gt;ideal initial scale&lt;/strong&gt; of the weights.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;optimization-vs-regularization-perspective&quot;&gt;Optimization vs Regularization Perspective&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The perspectives of regularization and optimization can give very different insights into how we should initialize a network.
        &lt;ul&gt;
          &lt;li&gt;The &lt;strong&gt;optimization&lt;/strong&gt; perspective suggests that the weights should be &lt;strong&gt;large enough&lt;/strong&gt; to propagate information successfully,&lt;/li&gt;
          &lt;li&gt;but some &lt;strong&gt;regularization&lt;/strong&gt; concerns encourage &lt;strong&gt;making them smaller&lt;/strong&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;implicit-prior-of-sgd&quot;&gt;Implicit Prior of SGD&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;The use of an optimization algorithm such as stochastic gradient descent […] expresses a prior that the final parameters should be close to the initial parameters.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;vanishing-gradients-problem&quot;&gt;Vanishing Gradients Problem&lt;/h4&gt;

&lt;h5 id=&quot;difference-sigmoidals-vs-relu&quot;&gt;Difference: Sigmoidals vs ReLU&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;sigmoidal&lt;/strong&gt; activation functions have the “vanishing gradients” problem&lt;/li&gt;
  &lt;li&gt;&lt;del&gt;&lt;strong&gt;ReLU&lt;/strong&gt; does &lt;strong&gt;not&lt;/strong&gt; have a “vanishing gradients” problem!&lt;/del&gt; Wrong, ReLU &lt;strong&gt;does&lt;/strong&gt; have the “vanishing gradients” problem! Use He initialization to avoid this problem.
    &lt;blockquote&gt;
      &lt;p&gt;Nevertheless, this Xavier initialization (after Glorot’s first name) is a neat trick that works well in practice. However, along came rectified linear units (ReLU), a non-linearity that is scale-invariant around 0 and does not saturate at large input values. This seemingly solved both of the problems the sigmoid function had; or were they just alleviated? I am unsure of how widely used Xavier initialization is, but if it is not, perhaps it is because ReLU seemingly eliminated this problem. &lt;a href=&quot;http://deepdish.io/&quot;&gt;http://deepdish.io/&lt;/a&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;deepdish-explanation&quot;&gt;Deepdish explanation&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://deepdish.io/&quot;&gt;http://deepdish.io/&lt;/a&gt; What happens for sigmoidal activations?:
    &lt;ul&gt;
      &lt;li&gt;First, let’s go back to the time of sigmoidal activation functions and initialization of parameters using i.i.d. Gaussian or uniform distributions with fairly &lt;strong&gt;arbitrarily set variances&lt;/strong&gt;.
        &lt;ul&gt;
          &lt;li&gt;Building deep networks was difficult because of &lt;strong&gt;exploding or vanishing activations and gradients&lt;/strong&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Let’s take &lt;strong&gt;activations&lt;/strong&gt; first:
        &lt;ul&gt;
          &lt;li&gt;If all your parameters are &lt;strong&gt;too small&lt;/strong&gt;,
            &lt;ul&gt;
              &lt;li&gt;the &lt;strong&gt;variance of your activations will drop in each layer&lt;/strong&gt;.&lt;/li&gt;
              &lt;li&gt;This is a problem if your activation &lt;strong&gt;function is sigmoidal&lt;/strong&gt;, since it is &lt;strong&gt;approximately linear close to 0&lt;/strong&gt;.
                &lt;ul&gt;
                  &lt;li&gt;That is, you &lt;strong&gt;gradually lose your non-linearity&lt;/strong&gt;, which means there is &lt;strong&gt;no benefit to having multiple layers&lt;/strong&gt;.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;If, on the other hand, your activations become &lt;strong&gt;larger and larger&lt;/strong&gt;,
            &lt;ul&gt;
              &lt;li&gt;then your &lt;strong&gt;activations will saturate&lt;/strong&gt; and become meaningless, with &lt;strong&gt;gradients approaching 0&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;code-experiment&quot;&gt;Code experiment&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;vanishing gradients problem: &lt;a href=&quot;https://github.com/pharath/home/tree/master/_posts_html/Weight%20Initialization%20in%20Neural%20Networks%20A%20Journey%20From%20the%20Basics%20to%20Kaiming%20%7C%20by%20James%20Dellinger%20%7C%20Towards%20Data%20Science&quot;&gt;code example&lt;/a&gt; (download this directory and view the html file locally, else it does not work)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;leibe&quot;&gt;Leibe&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;Leibe:
    &lt;ul&gt;
      &lt;li&gt;Main problem is &lt;strong&gt;getting the gradients back to the early layers&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;because if the gradients do not come through to the early layers, the early layers will compute random suboptimal features (“garbage”)&lt;/li&gt;
          &lt;li&gt;furthermore, since the gradients do not get backpropagated to the early layers those suboptimal features will not get updated and so the training accuracy gets stuck&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;for RNNs&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;they severely restrict the dependencies the RNN can learn&lt;/li&gt;
          &lt;li&gt;problem gets more severe the deeper the network is&lt;/li&gt;
          &lt;li&gt;can be very hard to diagnose that vanishing gradients occur
            &lt;ul&gt;
              &lt;li&gt;you just see that learning gets stuck&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;solutions&quot;&gt;Solutions&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Solutions&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;use LeCun tanh with Glorot/He initialization&lt;/li&gt;
      &lt;li&gt;use ReLU with He initialization&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;for RNNs&lt;/strong&gt;:
        &lt;ul&gt;
          &lt;li&gt;more complex hidden units (e.g. LSTM, GRU)&lt;/li&gt;
          &lt;li&gt;initialize $\mathbf{W}_{hh}$ with identity matrix and use ReLU
            &lt;ul&gt;
              &lt;li&gt;Le, Hinton 2015:
                &lt;ul&gt;
                  &lt;li&gt;The identity initialization has the very desirable property that when the error derivatives for the hidden units are backpropagated through time they remain constant provided no extra error-derivatives are added. This is the same behavior as LSTMs when their forget gates are set so that there is no decay and it makes it easy to learn very long-range temporal dependencies.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;rnns&quot;&gt;RNNs&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#Goodfellow_2016&quot;&gt;Goodfellow_2016&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;for RNNs&lt;/strong&gt;: gradients through such a [RNN] graph are also scaled according to $\text{diag}(\vec{\lambda})^t$&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Vanishing gradients&lt;/strong&gt; make it difficult to know which direction the parameters should move to improve the cost function, while &lt;strong&gt;exploding gradients&lt;/strong&gt; can make learning unstable.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;effect&lt;/strong&gt;: long-range dependencies are not learned by the RNN
    &lt;ul&gt;
      &lt;li&gt;i.e. words from timesteps far away are not taken into consideration when predicting the next word&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;xavier-glorot-initialization&quot;&gt;Xavier Glorot Initialization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;for tanh nonlinearities
    &lt;ul&gt;
      &lt;li&gt;because the derivation &lt;strong&gt;assumes linearity around 0&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&quot;&gt;Glorot, Bengio paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Xavier Glorot is the author’s full name&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;kaiming-he-initialization&quot;&gt;Kaiming He Initialization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;for ReLU nonlinearities&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;dependence-on-nn-depth&quot;&gt;Dependence on NN depth&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;lec 17 (from He et al paper):
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;The deeper the NNs are the more the right initialization method matters&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;e.g.
            &lt;ul&gt;
              &lt;li&gt;22-layer ReLU NN will converge with Xavier initialization, although this initialization method is wrong for ReLU nonlinearities
                &lt;ul&gt;
                  &lt;li&gt;albeit convergence will be slower than with He initialization&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;however, a 30-layer ReLU NN with Xavier initialization will not converge and gets stuck, whereas with He initialization it does converge!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dying-relu-problem&quot;&gt;Dying ReLU problem&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;causes:
    &lt;ul&gt;
      &lt;li&gt;too high learning rate&lt;/li&gt;
      &lt;li&gt;too large negative bias&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;solutions:
    &lt;ul&gt;
      &lt;li&gt;use a lower learning rate&lt;/li&gt;
      &lt;li&gt;use a variation of ReLU, e.g.
        &lt;ul&gt;
          &lt;li&gt;Leaky ReLU: $\max{\beta a, a}$
            &lt;ul&gt;
              &lt;li&gt;pro: avoids stuck-at-zero units&lt;/li&gt;
              &lt;li&gt;pro: weaker offset bias&lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;con&lt;/strong&gt;: does not have the &lt;strong&gt;non-informative deactivation states&lt;/strong&gt; property (cf. below) like ReLU, i.e. inactive Leaky ReLU units carry information because the gradient is not zero for negative inputs!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;PReLU: same as Leaky ReLU, but $\beta$ is a &lt;strong&gt;learnable&lt;/strong&gt; parameter&lt;/li&gt;
          &lt;li&gt;ELU: $e^a-1$ for $a \leq 0$
            &lt;ul&gt;
              &lt;li&gt;&lt;strong&gt;pro&lt;/strong&gt;: &lt;mark&gt;no offset bias&lt;/mark&gt; (“bias” means that ReLUs have an average activation $\gt 0$ which increases the chances of internal covariate shift)
                &lt;ul&gt;
                  &lt;li&gt;[&lt;a href=&quot;https://paperswithcode.com/method/elu&quot;&gt;source&lt;/a&gt;]
                    &lt;ul&gt;
                      &lt;li&gt;In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero &lt;strong&gt;like batch normalization&lt;/strong&gt; but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a &lt;strong&gt;reduced bias shift&lt;/strong&gt; effect.
                        &lt;ul&gt;
                          &lt;li&gt;“&lt;strong&gt;bias&lt;/strong&gt;” means that ReLUs have an average activation $\gt 0$ which increases the chances of internal covariate shift (cf. below)&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;[&lt;a href=&quot;https://numpy-ml.readthedocs.io/en/latest/numpy_ml.neural_nets.activations.html&quot;&gt;source&lt;/a&gt;]
                    &lt;ul&gt;
                      &lt;li&gt;ELUs are intended to address the fact that &lt;strong&gt;ReLUs&lt;/strong&gt; are strictly nonnegative and thus have an average activation $\gt 0$, &lt;strong&gt;increasing the chances of internal covariate shift&lt;/strong&gt; and slowing down learning. ELU units address this by
                        &lt;ul&gt;
                          &lt;li&gt;(1) allowing negative values when $x\lt0$, which&lt;/li&gt;
                          &lt;li&gt;(2) are bounded by a value $-\alpha$.&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                      &lt;li&gt;&lt;strong&gt;Similar to LeakyReLU&lt;/strong&gt;, the negative activation values help to push the average unit activation towards 0.
                        &lt;ul&gt;
                          &lt;li&gt;&lt;strong&gt;Unlike LeakyReLU&lt;/strong&gt;, however, the boundedness of the negative activation allows for greater robustness in the face of large negative values, allowing the function to &lt;strong&gt;avoid&lt;/strong&gt; conveying the degree of “absence” (negative activation) in the input.
                            &lt;ul&gt;
                              &lt;li&gt;“degree of absence” soll heißen, wir wollen &lt;strong&gt;nicht&lt;/strong&gt; quantifizieren wie stark negativ die activation ist! Wir wollen möglichst nur die positiven activations propagieren. Die “nicht aktiven” units sollen möglichst keine Information propagieren! Es ist also gut, dass ReLUs zero gradient haben für negative inputs. Leaky ReLUs haben das nicht! (diese Eigenschaft “non-informative deactivation states” ist nützlich in Anwendungen, s. Hochreiter paper)
                                &lt;ul&gt;
                                  &lt;li&gt;Hochreiter paper:
                                    &lt;ul&gt;
                                      &lt;li&gt;“ELUs code the degree of presence of input concepts, while they neither quantify the degree of their absence nor distinguish the causes of their absence. This property of &lt;strong&gt;non-informative deactivation states&lt;/strong&gt; is also present at ReLUs and allowed to detect biclusters corresponding to biological modules in gene expression datasets (Clevert et al., 2015) and to identify toxicophores in toxicity prediction (Unterthiner et al., 2015; Mayr et al., 2015). The enabling features for these interpretations is that activation can be clearly distinguished from deactivation and that &lt;strong&gt;only active units carry relevant information and can crosstalk&lt;/strong&gt;”&lt;/li&gt;
                                    &lt;/ul&gt;
                                  &lt;/li&gt;
                                &lt;/ul&gt;
                              &lt;/li&gt;
                            &lt;/ul&gt;
                          &lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;strong&gt;con&lt;/strong&gt;: need to store activations&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;modification of the initialization method
        &lt;ul&gt;
          &lt;li&gt;do &lt;strong&gt;not&lt;/strong&gt; use He initialization (i.e. initializing weights and biases through &lt;strong&gt;symmetric&lt;/strong&gt; probability distributions)
            &lt;ul&gt;
              &lt;li&gt;instead, use &lt;strong&gt;randomized asymmetric initialization&lt;/strong&gt; [&lt;a href=&quot;https://arxiv.org/abs/1903.06733&quot;&gt;Lu, Shin, Su, Karniadakis&lt;/a&gt;]&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;batch-normalization-ioffe-szegedy-2015&quot;&gt;Batch Normalization (Ioffe, Szegedy 2015)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;[basically, similar idea as zero mean and unit covariance in LeCun “Efficient BackProp”, but &lt;strong&gt;learn&lt;/strong&gt; $\mu$ and $\sigma^2$ instead of fixing them to zero or one, respectively]
    &lt;ul&gt;
      &lt;li&gt;i.e. BN reduces &lt;strong&gt;internal covariate shift&lt;/strong&gt; (see BN paper [Ioffe, Szegedy])
        &lt;ul&gt;
          &lt;li&gt;We define Internal Covariate Shift as the change in the distribution of network activations due to the change in network parameters during training&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;source of the following: &lt;strong&gt;all&lt;/strong&gt; Andrew Ng videos about BN&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;steps&quot;&gt;Steps&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Done
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;in each layer&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;after $\mathbf{W}^{[l]}a^{[l]}+b^{[l]}$, but before the nonlinearity&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Steps:
    &lt;ol&gt;
      &lt;li&gt;normalize: $z^{[l]}_{norm}=\frac{z^{[l]}-\mu}{\sqrt{\sigma^2+\epsilon}}$ (where $z^{[l]}=\mathbf{W}^{[l]}a^{[l]}+b^{[l]}$)&lt;/li&gt;
      &lt;li&gt;learn scaling of $\mu$ and $\sigma^2$: $\tilde{z}^{[l]}=\gamma z^{[l]}_{norm}+\beta$
        &lt;ul&gt;
          &lt;li&gt;$\gamma$ and $\beta$ are like additional weights of the NN!&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;additional-weights-gamma-and-beta&quot;&gt;Additional weights $\gamma$ and $\beta$&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$\gamma$ and $\beta$
    &lt;ul&gt;
      &lt;li&gt;have the same dimension as $b^{[l]}$
        &lt;ul&gt;
          &lt;li&gt;[technically step 2 should be $\tilde{z}^{[l]}=\text{diag}(\gamma) z^{[l]}_{norm}+\beta$ then???]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;are used to scale the mean and the variance of each hidden unit to whatever value the NN wants to set them to
        &lt;ul&gt;
          &lt;li&gt;we do &lt;strong&gt;not&lt;/strong&gt; want to force them to have mean 0 and unit covariance!
            &lt;ul&gt;
              &lt;li&gt;because maybe some other unknown distribution [with $\mu\neq 0$ and $\sigma^2\neq 1$] will (1) make the NN converge faster or (2) take better advantage of the nonlinearity (better generalization), so let the NN &lt;strong&gt;learn&lt;/strong&gt; that distribution&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;effect&quot;&gt;Effect&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;effect:
    &lt;ol&gt;
      &lt;li&gt;greatly improved convergence speed
        &lt;ul&gt;
          &lt;li&gt;because of reduced &lt;strong&gt;internal covariate shift&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;often better accuracy (i.e. w/o BN accuracy would not even reach this level in the first place!)
        &lt;ul&gt;
          &lt;li&gt;because of regularization (multiplicative and additive noise)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;implementation&quot;&gt;Implementation&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;BN makes the bias parameter $b^{[l]}$ of each layer redundant because the normalization step of BN zeros out the mean of $z^{[l]}=\mathbf{W}^{[l]}a^{[l]}+b^{[l]}$ anyway
    &lt;ul&gt;
      &lt;li&gt;the $\beta$ of the BN layer sort of replaces $b^{[l]}$ and ends up controlling the shift/bias of the layer&lt;/li&gt;
      &lt;li&gt;therefore, $b^{[l]}$ can be left out in the implementation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dropout&quot;&gt;Dropout&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&quot;&gt;Srivastava, Hinton&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Abstract:
        &lt;ul&gt;
          &lt;li&gt;[at training time] Deep neural nets with a large number of parameters are very powerful machine learning systems. However, &lt;strong&gt;overfitting&lt;/strong&gt; is a serious problem in such networks [weil “Modell hat zu viele Parameter” $\Rightarrow$ overfitting (vgl. polynomial curve fitting)].&lt;/li&gt;
          &lt;li&gt;[at test time] Large networks are also slow to use, making it &lt;strong&gt;difficult to deal with overfitting by combining the predictions of many different large neural nets at test time&lt;/strong&gt;.
            &lt;ul&gt;
              &lt;li&gt;wenn EIN “Large NN” at test time schon langsam ist, dann sind ZWEI doppelt so langsam und ein ENSEMBLE ist noch viel langsamer at test time&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Dropout&lt;/strong&gt; is a technique for addressing this problem.
            &lt;ul&gt;
              &lt;li&gt;The key idea is to randomly drop units (along with their connections) from the neural network during training.
                &lt;ul&gt;
                  &lt;li&gt;This prevents units from co-adapting too much.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;During training, dropout samples from an &lt;strong&gt;exponential number&lt;/strong&gt; of different “thinned” networks.&lt;/li&gt;
              &lt;li&gt;At test time, it is easy to &lt;strong&gt;approximate the effect of averaging the predictions of all these thinned networks&lt;/strong&gt; by simply using a single unthinned network that has smaller weights.&lt;/li&gt;
              &lt;li&gt;This significantly &lt;strong&gt;reduces overfitting&lt;/strong&gt; and gives major &lt;strong&gt;improvements over other regularization methods&lt;/strong&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;there are $2^n$ (Mächtigkeit der Potenzmenge für n Elemente) thinned NNs, where $n$ is the number of hidden units
        &lt;ul&gt;
          &lt;li&gt;but still only $\mathcal{O}(n^2)$ parameters (soll heißen Dropout führt keine neuen Parameter ein)
            &lt;ul&gt;
              &lt;li&gt;Das $\mathcal{O}(n^2)$ ist ein upper bound? [&lt;a href=&quot;https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension#VC_dimension_of_a_neural_network&quot;&gt;VC dimension of a NN&lt;/a&gt;]&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;at test time use $p\mathbf{w}$, so that the output of a node &lt;strong&gt;at test time&lt;/strong&gt; is the expected output &lt;strong&gt;at training time&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/&quot;&gt;source&lt;/a&gt;: Note: The rescaling of the weights can be performed at training time instead, after each weight update at the end of the mini-batch. This is sometimes called “inverse dropout” and does not require any modification of weights during training. Both the &lt;strong&gt;Keras&lt;/strong&gt; and &lt;strong&gt;PyTorch&lt;/strong&gt; deep learning libraries implement dropout in this way.&lt;/li&gt;
          &lt;li&gt;[mulipliziere $\mathbf{w}$ mit $p$ heißt das “Neuron” feuert nur in einem Bruchteil $p$ aller Fälle, sodass der gesamte Erwartungswert über &lt;strong&gt;alle&lt;/strong&gt; “Neuronen”, d.h. der Output des gesamten NN, aber stimmt (auch wenn $p\mathbf{w}$ nur ein &lt;strong&gt;geschätzter&lt;/strong&gt; Erwartungswert &lt;strong&gt;eines bestimmten&lt;/strong&gt; “Neurons” ist)]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;/assets/images/optimization/dropout.png&quot; alt=&quot;dropout.png&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/&quot;&gt;source&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Ensembles of neural networks&lt;/strong&gt; with different model configurations are known to &lt;strong&gt;reduce overfitting&lt;/strong&gt;, &lt;strong&gt;but&lt;/strong&gt; require the &lt;strong&gt;additional computational expense&lt;/strong&gt; of training and maintaining multiple models.&lt;/li&gt;
      &lt;li&gt;A single model can be used to &lt;strong&gt;simulate having a large number of different network architectures&lt;/strong&gt; by randomly dropping out nodes during training.
        &lt;ul&gt;
          &lt;li&gt;This is called &lt;strong&gt;dropout&lt;/strong&gt; and offers a very &lt;strong&gt;computationally cheap&lt;/strong&gt; and remarkably effective &lt;strong&gt;regularization method&lt;/strong&gt; to &lt;strong&gt;reduce overfitting&lt;/strong&gt; and &lt;strong&gt;improve generalization error&lt;/strong&gt; in deep neural networks of all kinds.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Large weights&lt;/strong&gt; in a neural network &lt;strong&gt;are a sign&lt;/strong&gt; of a more complex network that has overfit the training data.&lt;/li&gt;
      &lt;li&gt;Probabilistically dropping out nodes in the network is a simple and effective &lt;strong&gt;regularization&lt;/strong&gt; method.&lt;/li&gt;
      &lt;li&gt;A &lt;strong&gt;large&lt;/strong&gt; network with &lt;strong&gt;more training&lt;/strong&gt; and the use of a &lt;strong&gt;weight constraint&lt;/strong&gt; are &lt;strong&gt;suggested&lt;/strong&gt; when using dropout.
        &lt;ul&gt;
          &lt;li&gt;large NN:
            &lt;ul&gt;
              &lt;li&gt;It is common for &lt;strong&gt;larger&lt;/strong&gt; networks (more layers or more nodes) to &lt;strong&gt;more easily overfit&lt;/strong&gt; the training data.&lt;/li&gt;
              &lt;li&gt;When using dropout regularization, it is possible to use larger networks with less risk of overfitting. In fact, a large network (more nodes per layer) may be required as dropout will probabilistically reduce the capacity of the network.&lt;/li&gt;
              &lt;li&gt;A good &lt;strong&gt;rule of thumb&lt;/strong&gt; is to divide the number of nodes in the layer before dropout by the proposed dropout rate and use that as the number of nodes in the new network that uses dropout. For example, a network with 100 nodes and a proposed dropout rate of 0.5 will require 200 nodes (100 / 0.5) when using dropout.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;grid search parameters:
            &lt;ul&gt;
              &lt;li&gt;Rather than guess at a suitable dropout rate for your network, test different rates systematically.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;weight constraint [= regularization, s. Dropout paper] [e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxnorm&lt;/code&gt;, L2, …] [Genaue Erklärung: s. Kapitel 5.1, 2. Absatz in Dropout paper]:
            &lt;ul&gt;
              &lt;li&gt;Network weights will increase in size in response to the probabilistic removal of layer activations. [?: Steht so nicht im paper. Eigentlich sollte Dropout einen regularizing effect haben, d.h. die weights shrinken.]&lt;/li&gt;
              &lt;li&gt;Large weight size can be a sign of an unstable network.&lt;/li&gt;
              &lt;li&gt;To counter this effect a &lt;strong&gt;weight constraint&lt;/strong&gt; can be imposed to force the norm (magnitude) of all weights in a layer to be below a specified value.
                &lt;ul&gt;
                  &lt;li&gt;For example, the &lt;strong&gt;maximum norm constraint&lt;/strong&gt; is recommended [im Dropout paper] with a value between 3-4.
                    &lt;ul&gt;
                      &lt;li&gt;[Genaue Erklärung: s. Kapitel 5.1, 2. Absatz bis letzter Absatz in Dropout paper]
                        &lt;ul&gt;
                          &lt;li&gt;eher Vermutungen als Erklärungen ?
                            &lt;ul&gt;
                              &lt;li&gt;“A possible justification is that constraining weight vectors to lie inside a ball of fixed radius makes it possible to use a huge learning rate without the possibility of weights blowing up. The noise provided by dropout then allows the optimization process to explore different regions of the weight space that would have otherwise been difficult to reach. As the learning rate decays, the optimization takes shorter steps, thereby doing less exploration and eventually settles into a minimum.”&lt;/li&gt;
                            &lt;/ul&gt;
                          &lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;classical-2nd-order-optimization-methods&quot;&gt;Classical 2nd order optimization methods&lt;a name=&quot;2nd_order_methods&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;[source: LeCun et al. “Efficient BackProp”]&lt;/p&gt;

&lt;p&gt;“Second order methods &lt;strong&gt;speed learning&lt;/strong&gt; by estimating not just the &lt;strong&gt;gradient&lt;/strong&gt; but also the &lt;strong&gt;curvature&lt;/strong&gt; of the cost surface. Given the curvature, one can estimate the approximate location of the actual minimum.”&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Newton-Raphson Algorithm
    &lt;ul&gt;
      &lt;li&gt;it is a $\mathcal{O}(N^3)$ (complexity of the matrix inversion of the $N\times N$ Hessian) method, where $N$ is the number of components of the parameter vector $\mathbf{w}$ &lt;a href=&quot;https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra&quot;&gt;[complexity of mathematical operations]&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;see notes part 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Conjugate Gradient
    &lt;ul&gt;
      &lt;li&gt;(1) it is a $\mathcal{O}(N)$ method,&lt;/li&gt;
      &lt;li&gt;(2) it doesn’t use the Hessian explicitly,&lt;/li&gt;
      &lt;li&gt;(3) it attempts to find descent directions that try to minimally spoil the result achieved in the previous iterations,&lt;/li&gt;
      &lt;li&gt;(4) it uses a line search, and most importantly,&lt;/li&gt;
      &lt;li&gt;(5) it &lt;strong&gt;works only for batch learning&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Quasi-Newton (BFGS)
    &lt;ul&gt;
      &lt;li&gt;(1) iteratively computes an estimate of the inverse Hessian,&lt;/li&gt;
      &lt;li&gt;(2) is an $\mathcal{O}(N^2)$ algorithm,
        &lt;ul&gt;
          &lt;li&gt;“Since the updates of the BFGS curvature matrix do not require matrix inversion, its computational complexity is only $\mathcal{O}(n^2)$, compared to $\mathcal{O}(n^3)$ in Newton’s method” - &lt;a href=&quot;https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&quot;&gt;Wikipedia&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;(3) requires line search and&lt;/li&gt;
      &lt;li&gt;(4) it &lt;strong&gt;works only for batch learning&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gauss-Newton and Levenberg Marquardt algorithm
    &lt;ul&gt;
      &lt;li&gt;(1) use the square Jacobi approximation,&lt;/li&gt;
      &lt;li&gt;(2) are &lt;strong&gt;mainly designed for batch learning&lt;/strong&gt;,&lt;/li&gt;
      &lt;li&gt;(3) have a complexity of $\mathcal{O}(N^3)$ and&lt;/li&gt;
      &lt;li&gt;(4) most important, they work only for mean squared error loss functions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mlp-example-implementations&quot;&gt;MLP Example Implementations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/Expl_NN_in_numpy.ipynb&quot;&gt;Expl_NN_in_numpy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/MLP_in_numpy.ipynb&quot;&gt;MLP_in_numpy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/MLP_selbst_versucht.ipynb&quot;&gt;MLP_selbst_versucht&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.org/github/pharath/home/blob/master/assets/notebooks/WofuerIst__name__gut.ipynb&quot;&gt;WofuerIst__name__gut&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;REFERENCES&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a name=&quot;Bishop_2006&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738&quot;&gt;Bishop, Christopher M., &lt;em&gt;Pattern Recognition and Machine Learning (Information Science and Statistics)&lt;/em&gt; (2006), Springer-Verlag, Berlin, Heidelberg, 0387310738.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a name=&quot;Goodfellow_2016&quot;&gt;&lt;/a&gt; &lt;a href=&quot;http://www.deeplearningbook.org&quot;&gt;Ian J. Goodfellow and Yoshua Bengio and Aaron Courville, &lt;em&gt;Deep Learning&lt;/em&gt; (2016), MIT Press, Cambridge, MA, USA&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Lecture_Notes" /><category term="Machine_Learning" /><category term="lecture_notes" /><category term="ml" /><summary type="html">Notes on Machine Learning theory. Based on C. M. Bishop, &quot;Pattern Recognition and Machine Learning&quot; (2011) and Goodfellow, Bengio, Courville, &quot;Deep Learning&quot;.</summary></entry><entry><title type="html">Computer Vision</title><link href="http://localhost:4000/summaries/summary-CV/" rel="alternate" type="text/html" title="Computer Vision" /><published>2022-01-12T00:00:00+01:00</published><updated>2022-01-12T21:00:52+01:00</updated><id>http://localhost:4000/summaries/summary-CV</id><content type="html" xml:base="http://localhost:4000/summaries/summary-CV/"></content><author><name>Pharath Palesuvaran</name></author><category term="Summaries" /><category term="summaries" /><category term="cv" /><summary type="html">Click on &quot;Content in HTML&quot; button below.</summary></entry><entry><title type="html">Learn Blockchains by Building One</title><link href="http://localhost:4000/build/blockchain/build-blockchain-dvf/" rel="alternate" type="text/html" title="Learn Blockchains by Building One" /><published>2021-12-24T00:00:00+01:00</published><updated>2021-12-24T00:00:00+01:00</updated><id>http://localhost:4000/build/blockchain/build-blockchain-dvf</id><content type="html" xml:base="http://localhost:4000/build/blockchain/build-blockchain-dvf/">&lt;h1 id=&quot;postman-settings&quot;&gt;Postman Settings&lt;/h1&gt;

&lt;p&gt;To interact with the built &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;blockchain.py&lt;/code&gt; API over a network via the Postman App:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Select Authorization Type “No Auth” !&lt;/li&gt;
  &lt;li&gt;select “raw”&lt;/li&gt;
  &lt;li&gt;select “JSON” instead of “Text”&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;set-up-python-environment&quot;&gt;Set up python environment&lt;/h1&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# create a python environment&lt;/span&gt;
python3 &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; venv &lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# activate the python environment&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source env&lt;/span&gt;/bin/activate

&lt;span class=&quot;c&quot;&gt;# install necessary packages in this environment&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Note: try newer version of Flask (I used version 2.0.2), if something does not work in the following)&lt;/span&gt;
pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;0.12.2 &lt;span class=&quot;nv&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;2.18.4 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;modify-old-blockchainpy&quot;&gt;Modify old blockchain.py&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;add the following &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;app.config&lt;/code&gt; line:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Instantiate the Node
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'JSONIFY_PRETTYPRINT_REGULAR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;start-the-server&quot;&gt;Start the server&lt;/h1&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 blockchain.py &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 5001
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;http-requests&quot;&gt;http requests&lt;/h1&gt;

&lt;h2 id=&quot;general-structure&quot;&gt;General structure&lt;/h2&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;GET &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:5000/chain&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;POST &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:5001/transactions/new&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;examples-1&quot;&gt;Examples&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;command&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GET /chain&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;zeigt die gesamte Blockchain&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GET /mine&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;erzeugt einen neuen Block&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POST /transactions/new&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fügt eine Transaktion zum nächsten Block (der noch nicht in der &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GET /chain&lt;/code&gt; Liste ist) hinzu, der geminet wird. Der nächste &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GET /mine&lt;/code&gt; Call erzeugt dann den Block, in dem diese transaction ist. Beispieltransaktion:&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sender&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d4ee26eee15148ee92c6cd394edd974e&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;recipient&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;someone-other-address&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;amount&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spin up another node on machine, on a different port [port 5001], and register it with current node [on port 5000]:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;command&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;POST nodes/register&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;nodes&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;http://127.0.0.1:5001/&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;oder alternativ:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;pi&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;nodes&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;http://192.168.2.126:5001/&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;command&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GET /nodes/resolve&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;replace shorter chains with the longest chain by the Consensus Algorithm&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Build" /><category term="Blockchain" /><category term="build" /><category term="blockchain" /><summary type="html">Original project by Daniel van Flymen: original [blog post](https://hackernoon.com/learn-blockchains-by-building-one-117428612f46)</summary></entry><entry><title type="html">Triaxiale Schwarzschild-Modelle für elliptische Galaxien und ihre Anwendung auf NGC 4365</title><link href="http://localhost:4000/bachelor_thesis/link-Bachelor-Thesis/" rel="alternate" type="text/html" title="Triaxiale Schwarzschild-Modelle für elliptische Galaxien und ihre Anwendung auf NGC 4365" /><published>2021-12-23T00:00:00+01:00</published><updated>2021-12-23T00:00:00+01:00</updated><id>http://localhost:4000/bachelor_thesis/link-Bachelor-Thesis</id><content type="html" xml:base="http://localhost:4000/bachelor_thesis/link-Bachelor-Thesis/">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;In der vorliegenden Arbeit wird eine Implementierung eines triaxialen Schwarzschild-Modells beschrieben, die von R. C. E. van den Bosch et alii (2008, in [11]) entwickelt wurde. Mit diesem Modell lassen sich eine Vielzahl verschiedener elliptischer Galaxien beschreiben. Es gibt Aufschluss über die interne Dynamik der Galaxie und über deren räumliche Orientierung und Form. Auch viele andere Eigenschaften (u. a. die Masse eines zentralen supermassiven Schwarzen Lochs und die Verteilung der dunklen Materie in der Galaxie) lassen sich mit diesem Modell untersuchen. Für die Konstruktion dieses Modells fittet man die beobachtete Flächenhelligkeit der Galaxie (z.B. vom Hubble-Weltraumteleskop) und kinematische Daten (z.B. von einem Integral-Feld-Spektrographen). Um das Modell zu testen, werden Vorhersagen eines theoretischen Test-Abel-Modells als ’simulierte’ Beobachtungen genutzt. Es wird gezeigt, dass das Schwarzschild-Modell die theoretischen Vorhersagen reproduzieren kann. Zudem wird eine (von van den Bosch et alii durchgeführte) Anwendung des Modells anhand der Galaxie NGC 4365 präsentiert.&lt;/p&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="Bachelor_Thesis" /><category term="bachelor_thesis" /><summary type="html">Bachelorarbeit an der Fakultät für Physik der Ludwig–Maximilians–Universität München.</summary></entry><entry><title type="html">PyTorch</title><link href="http://localhost:4000/pytorch/machine_learning/notes-pytorch/" rel="alternate" type="text/html" title="PyTorch" /><published>2021-12-20T00:00:00+01:00</published><updated>2021-09-23T22:00:52+02:00</updated><id>http://localhost:4000/pytorch/machine_learning/notes-pytorch</id><content type="html" xml:base="http://localhost:4000/pytorch/machine_learning/notes-pytorch/">&lt;h1 id=&quot;pytorch-doc&quot;&gt;PyTorch Doc&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/notes/modules.html&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;modules&quot;&gt;Modules&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;read &lt;a href=&quot;https://pytorch.org/docs/stable/notes/modules.html#a-simple-custom-module&quot;&gt;A Simple Custom Module&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;“Note that the module itself is callable, and that calling it invokes its &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward()&lt;/code&gt; function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
    &lt;ul&gt;
      &lt;li&gt;The &lt;mark&gt;“forward pass”&lt;/mark&gt; is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet).&lt;/li&gt;
      &lt;li&gt;The &lt;mark&gt;“backward pass”&lt;/mark&gt; &lt;strong&gt;computes gradients&lt;/strong&gt; of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods.
        &lt;ul&gt;
          &lt;li&gt;PyTorch’s &lt;strong&gt;autograd system&lt;/strong&gt; automatically takes care of this backward pass computation, so it is not required to manually implement a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward()&lt;/code&gt; function for each module.”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-does-pytorch-create-a-computational-graph&quot;&gt;How does PyTorch create a computational graph?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;tensors&quot;&gt;Tensors&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;“On it’s own, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; is just like a numpy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ndarray&lt;/code&gt;. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; attribute of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; to True.”&lt;/li&gt;
  &lt;li&gt;“&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; is contagious. It means that when a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; is created by operating on other &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt;s, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; of the resultant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; would be set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; given at least one of the tensors used for creation has it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; set to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;.”&lt;/li&gt;
  &lt;li&gt;“Each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; has […] an attribute called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt;, which refers to the mathematical operator that creates the variable [d.h. zB., wenn die Variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt; über &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d = w3*b + w4*c&lt;/code&gt; definiert ist, dann ist das &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt; von &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt; der Additionsoperator &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt;]. If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requires_grad&lt;/code&gt; is set to False, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt; would be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt;.” (kann man mit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print(&quot;The grad fn for a is&quot;, a.grad_fn)&lt;/code&gt; testen!) (lies das nochmal genauer im Post!)&lt;/li&gt;
  &lt;li&gt;“One can use the member function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;is_leaf&lt;/code&gt; to determine whether a variable is a leaf &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; or not.”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torchnnautogradfunction-class&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.Autograd.Function&lt;/code&gt; class&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;“This class has two important member functions we need to look at.”:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;“simply computes the output using it’s inputs”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;“takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function $f$ is basically the &lt;strong&gt;gradient that is backpropagated to $f$ from the layers in front of it&lt;/strong&gt; multiplied by &lt;strong&gt;the local gradient of the output of f with respect to it’s inputs&lt;/strong&gt;. This is exactly what the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward&lt;/code&gt; function does.” (lies das nochmal genauer nach!)
            &lt;ul&gt;
              &lt;li&gt;Let’s again understand with our example of \(d = f(w_3b , w_4c)\)
                &lt;ol&gt;
                  &lt;li&gt;&lt;em&gt;d&lt;/em&gt; is our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; here. It’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt;  is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ThAddBackward&amp;gt;&lt;/code&gt;&lt;em&gt;.&lt;/em&gt; This is basically the addition operation since the function that creates &lt;em&gt;d&lt;/em&gt; adds inputs.&lt;/li&gt;
                  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; function of the it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt;  receives the inputs $w_3b$ &lt;em&gt;and&lt;/em&gt; $w_4c$ and adds them. This value is basically stored in the &lt;em&gt;d&lt;/em&gt;&lt;/li&gt;
                  &lt;li&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward&lt;/code&gt; function of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ThAddBackward&amp;gt;&lt;/code&gt;  basically takes the the &lt;strong&gt;incoming gradient&lt;/strong&gt; from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from &lt;em&gt;L&lt;/em&gt; to &lt;em&gt;d.&lt;/em&gt; This gradient is also the gradient of &lt;em&gt;L&lt;/em&gt; w.r.t to &lt;em&gt;d&lt;/em&gt; and is stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad&lt;/code&gt;  attribute of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt;. It can be accessed by calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d.grad&lt;/code&gt;&lt;em&gt;.&lt;/em&gt;&lt;/li&gt;
                  &lt;li&gt;It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$ and $\frac{\partial{d}}{\partial{w_3b}}$.&lt;/li&gt;
                  &lt;li&gt;Then the backward function multiplies the incoming gradient with the &lt;strong&gt;locally computed gradients&lt;/strong&gt; respectively and “&lt;em&gt;&lt;strong&gt;sends&lt;/strong&gt;&lt;/em&gt;” the gradients to it’s inputs by invoking the backward method of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt; of their inputs.&lt;/li&gt;
                  &lt;li&gt;For example, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward&lt;/code&gt; function of  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ThAddBackward&amp;gt;&lt;/code&gt;  associated with &lt;em&gt;d&lt;/em&gt; invokes backward function of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt; of the $w_4*c$ (Here, $w_4*c$ is a intermediate Tensor, and it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ThMulBackward&amp;gt;&lt;/code&gt;. At time of invocation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward&lt;/code&gt; function, the gradient $\frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{w_4c}} $ is passed as the input.&lt;/li&gt;
                  &lt;li&gt;Now, for the variable $w_4*c$, $\frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{w_4c}} $ becomes the incoming gradient, like $\frac{\partial{L}}{\partial{d}} $ was for $d$ in step 3 and the process repeats.&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-are-pytorchs-graphs-different-from-tensorflow-graphs&quot;&gt;How are PyTorch’s graphs different from TensorFlow graphs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch creates something called a &lt;strong&gt;Dynamic Computation Graph&lt;/strong&gt;, which means that the graph is generated on the fly.
    &lt;ul&gt;
      &lt;li&gt;in contrast to the &lt;strong&gt;Static Computation Graphs&lt;/strong&gt; used by TensorFlow where the graph is declared &lt;strong&gt;before&lt;/strong&gt; running the program&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Until the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; function of a Variable is called, there exists no node for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Tensor&lt;/code&gt; (it’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grad_fn&lt;/code&gt;) in the graph.&lt;/p&gt;

    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  ```python
      a = torch.randn((3,3), requires_grad = True)   #No graph yet, as a is a leaf
        
      w1 = torch.randn((3,3), requires_grad = True)  #Same logic as above
        
      b = w1*a   #Graph with node `mulBackward` is created.
  ```    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The graph is created as a result of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; function of many &lt;em&gt;Tensors&lt;/em&gt; being invoked. Only then, the buffers for the non-leaf nodes are allocated for the graph and intermediate values (used for computing gradients later). When you call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;backward&lt;/code&gt;, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is &lt;em&gt;destroyed&lt;/em&gt; (In a sense, you can't backpropagate through it, since the buffers holding values to compute the gradients are gone).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Next time, you will call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; on the same set of tensors, &lt;strong&gt;the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;lies den Abschnitt im Post !&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Pharath Palesuvaran</name></author><category term="PyTorch" /><category term="Machine_Learning" /><category term="pytorch" /><category term="ml" /><summary type="html">Notes on PyTorch.</summary></entry></feed>