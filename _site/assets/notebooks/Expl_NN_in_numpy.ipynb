{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth of ANN Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> from: [[Source](https://datascience.stackexchange.com/questions/14027/counting-the-number-of-layers-in-a-neural-network)] \n",
    ">\n",
    "> Input layer is a layer, it's not wrong to say that.\n",
    "> \n",
    "> However, when calculating the depth of a deep neural network, we only consider the layers that have tunable weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.\n",
    "\n",
    "    — Ian Goodfellow, DLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online versus Batch learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hinton Online versus Batch learning zig zag constraints](https://slideplayer.com/slide/6097875/18/images/11/Online+versus+batch+learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Erklärung zu NeuralNetwork in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weight matrices\n",
    "layer=[1,10,1]\n",
    "weight_L1zuL2=np.random.randn(layer[0+1],layer[0])\n",
    "weight_L2zuL3=np.random.randn(layer[1+1],layer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct weight tensor\n",
    "weights=[]\n",
    "weights.append(weight_L1zuL2)\n",
    "weights.append(weight_L2zuL3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.36230993],\n",
       "        [ 1.03101203],\n",
       "        [-1.42677305],\n",
       "        [-0.78497715],\n",
       "        [-0.77183381],\n",
       "        [-0.25876917],\n",
       "        [ 0.59590358],\n",
       "        [ 0.22608001],\n",
       "        [ 0.89552097],\n",
       "        [ 1.76104799]]),\n",
       " array([[ 0.18158636,  0.76247564,  0.03074861,  1.17271327,  0.24022962,\n",
       "          1.05575277, -0.48346829,  0.03496241,  1.36294595,  0.21159984]])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight tensor\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight tensor ist eine Liste und kein numpy array! (Eine Liste von numpy arrays sozusagen)\n",
    "type(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "X = 2*np.pi*np.random.rand(1000).reshape(1, -1)  # reshape column vector to row vector\n",
    "y = np.sin(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` muss ein Zeilenvektor sein, weil `weights[0]` die Form `(10,1)` (i.e. Spaltenvektor) hat und das dot product `weights[0].dot(X)` sonst nicht das gewünschte dot product wäre (s.u. section 'Batch oder online learning')! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# warum reshape ?\n",
    "print(X.shape)\n",
    "X_ohne_reshape = 2*np.pi*np.random.rand(1000)\n",
    "print(X_ohne_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_anderes_reshape = 2*np.pi*np.random.rand(1000).reshape((1, -1)) # Doppelte Klammer reshape((1,-1)) nicht nötig \n",
    "X_anderes_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4],\n",
       "       [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(10).reshape(2,5) # Doppelte Klammer reshape((2,5)) nicht nötig \n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch oder Online learning ?\n",
    "\n",
    "**Weight updates are performed after each Mini-batch !** => **Minibatch algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welche Dimension haben die pre-nonlinearity activations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen $$\\bf{W}\\cdot \\bf{X}_{Minibatch}=\\bf{Y}_{Minibatch}$$ berechnen, wobei $\\bf{X}_{Minibatch}$ die $1\\times $ `batch_size`-dimensionalen Trainingdaten sind und $\\bf{Y}_{Minibatch}$ die $10\\times $ `batch_size`-dimensionale Outputmatrix ist (die i-te Spalte von $\\bf{Y}_{Minibatch}$ ist der Output für einen forward pass mit input $\\bf{X}_i$, die i-te Komponente von $\\bf{X}_{Minibatch}$). Wir fassen also in jedem forward pass alle Outputs für `batch_size` Trainingspunkte in eine Matrix $\\bf{Y}_{Minibatch}$ zusammen und updaten die weights unter Benutzung von $\\bf{Y}_{Minibatch}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrixmultiplikation WX+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.82218119,  1.68440298,  2.05690932, ...,  1.41028044,\n",
       "         0.6709957 ,  2.22184378],\n",
       "       [ 5.18531395,  4.79324358,  5.85327115, ...,  4.01318313,\n",
       "         1.90942776,  6.32261907],\n",
       "       [-7.17573216, -6.6331629 , -8.1000893 , ..., -5.55367091,\n",
       "        -2.64237466, -8.74959963],\n",
       "       ...,\n",
       "       [ 1.1370341 ,  1.05106103,  1.28350356, ...,  0.8800096 ,\n",
       "         0.41869875,  1.38642203],\n",
       "       [ 4.50388282,  4.16333661,  5.08406003, ...,  3.48578827,\n",
       "         1.65849917,  5.49172831],\n",
       "       [ 8.85691575,  8.18722934,  9.99783812, ...,  6.854826  ,\n",
       "         3.26144974, 10.79952053]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-nonlinearity activation of input layer\n",
    "weights[0].dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].dot(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct biases\n",
    "b_1zu2 = np.random.randn(layer[1], 1)\n",
    "b_2zu3 = np.random.randn(layer[2], 1)\n",
    "b=[]\n",
    "b.append(b_1zu2)\n",
    "b.append(b_2zu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.91116328e-01, 8.75229958e-01, 7.62662213e-01, ...,\n",
       "        8.63748571e-01, 8.99121019e-01, 9.40039741e-01],\n",
       "       [3.22576044e-01, 9.24818897e-01, 5.71539702e-01, ...,\n",
       "        9.02174156e-01, 9.60501107e-01, 9.91823281e-01],\n",
       "       [1.31896437e-01, 1.68530677e-03, 3.52366203e-02, ...,\n",
       "        2.50874556e-03, 6.56981256e-04, 7.11140288e-05],\n",
       "       ...,\n",
       "       [2.94249264e-01, 4.59634882e-01, 3.43224816e-01, ...,\n",
       "        4.43990419e-01, 4.96905813e-01, 5.84195688e-01],\n",
       "       [3.56305623e-01, 9.03165533e-01, 5.75246252e-01, ...,\n",
       "        8.78966746e-01, 9.44002916e-01, 9.85524364e-01],\n",
       "       [6.13038739e-01, 9.97562114e-01, 9.01994421e-01, ...,\n",
       "        9.96018558e-01, 9.99237550e-01, 9.99950979e-01]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activation of input layer\n",
    "import NeuralNetwork\n",
    "activation_function1 = NeuralNetwork.NeuralNetwork.getActivationFunction('sigmoid')\n",
    "activation_function(weights[0].dot(X)+b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute activation of next layer (= first hidden layer) \n",
    "# which is also the final output of the first forward pass\n",
    "activation_function2 = NeuralNetwork.NeuralNetwork.getActivationFunction('linear')\n",
    "a_L1 = activation_function1(weights[0].dot(X)+b[0])\n",
    "a_L2 = activation_function2(weights[1].dot(a_L1)+b[1])\n",
    "a_L2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement Mini-batch ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "    i=0\n",
    "    while i<len(y[0]):\n",
    "        x_batch=x[0][i:i+batch_size].reshape(1,-1)\n",
    "        y_batch=y[0][i:i+batch_size].reshape(1,-1)\n",
    "        # for debug\n",
    "        if i<=5*batch_size: \n",
    "            print(x_batch.shape)\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "Train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens, if we define `some_list=[1,2,3]` and we call `some_list[0:100]` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "a[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens, if the used `batch_size` value is too large (i.e. larger than `len(X)`) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "Train(X,y,batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welche Form hat der Output von `.feedforward()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:06<00:00, 286.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 4.665356156175609\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import NeuralNetwork2\n",
    "import matplotlib.pyplot as plt\n",
    "nn = NeuralNetwork2.NeuralNetwork([1, 10, 10, 1],activations=['sigmoid', 'sigmoid', 'linear'])\n",
    "X = 2*np.pi*np.random.rand(1000).reshape(1, -1)\n",
    "y = np.sin(X)    \n",
    "\n",
    "nn.train(X, y, epochs=2000, batch_size=1000, lr = .5)\n",
    "_, a_s = nn.feedforward(X)\n",
    "#print(y, X)\n",
    "plt.scatter(X.flatten(), y.flatten())\n",
    "plt.scatter(X.flatten(), a_s[-1].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merke:** `.shape` ist für `np.arrays` und `len()` ist für **Listen** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merke**: Alle Objekte sind **Listen**, außer folgende **np.arrays**:\n",
    "- Input `X` und Mini-batches `x_batch`\n",
    "- Targets `y` und `y_batch`\n",
    "- pre- (`z_s`) und post-nonlinearity activations (`a_s`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gibt eine Liste mit 2 Listen (z_s und a_s) zurück\n",
    "len(NeuralNetwork.NeuralNetwork.feedforward(nn,X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_s, a_s = NeuralNetwork.NeuralNetwork.feedforward(nn,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gibt eine Liste mit 3 np.arrays (die pre-nonlinearity activations\n",
    "# für jeden Layer) zurück\n",
    "len(z_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gibt ein np.array zurück (pre-nonlinearity activation Wx+b des input layers)\n",
    "z_s[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Backprop\n",
    "Hinton alle slides: [[Source](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec3.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize list for `deltas[j]` $=\\frac{\\partial E}{\\partial z_j}$ (1. Zeile auf Hinton Folie), where $z_j=$ `z_s[j]` and $y_j=$ `a_s[j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas=[None]*len(weights)\n",
    "deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quote von Hinton Folie:\n",
    "\n",
    "> $$E=\\frac{1}{2}\\sum_{j\\in output}(t_j-y_j)^2$$\n",
    "> $$\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)$$\n",
    "\n",
    "dh. die $y_j$ sind die post-nonlinearity activations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E$ wird **nicht** berechnet, nur $$\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)$$ (nämlich in `.backpropagation()`), weil man für die weight updates nur die Gradienten von $E$ braucht und nicht $E$ selbst!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wofür steht j ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`j` geht jeweils über alle x-Werte in `x_batch`, d.h. `j`$=1\\,,\\dotso\\,,$ `batch_size`, \n",
    "\n",
    "(**Achtung**: Bei `deltas[j]`, `z_s[j]` und `a_s[j]` steht das `j` für den **Layer** und hat nichts mit dem $j$ in $=\\frac{\\partial E}{\\partial z_j}$, $z_j$ und $y_j$ zu tun, was für einen x-Wert im batch steht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Shapes, dot products and Hadamard (element-wise) products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung**: Matrix product geht nicht, wenn einer der beiden np.arrays einfach nur transponiert wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla1=np.arange(4).T\n",
    "bla2=np.arange(4)\n",
    "bla1.dot(bla2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .T geht nur bei Matrizen, nicht bei Vektoren !\n",
    "bla1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stattdessen muss man `.reshape(1,-1)` anwenden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 2, 3],\n",
       "       [0, 2, 4, 6],\n",
       "       [0, 3, 6, 9]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bla3=np.arange(4).reshape(1,-1).T\n",
    "bla4=np.arange(4).reshape(1,-1)\n",
    "bla3.dot(bla4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung**: Bei .dot auf dim aufpassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec=np.arange(0,10).reshape(1,-1)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# geht\n",
    "# normal matrix product (nur komisch weil (10x1) x (1x10) ungewohnt, aber \n",
    "# recall, zB. (10x3) x (3x10) ging ja auch immer)\n",
    "weights[1].T.dot(vec).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,10) and (1,10) not aligned: 10 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-7b397d8d659b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# geht nicht: Dimensionsfehler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,10) and (1,10) not aligned: 10 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# geht nicht: Dimensionsfehler\n",
    "weights[1].dot(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of transpose matrix\n",
    "weights[1].T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check matrix product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5748538274623725"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element [1][6] of matrix\n",
    "weights[1].T.dot(vec)[1][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5748538274623725"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute element [1][6] manually\n",
    "vec[0][6]*weights[1][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop in einzelnen Schritten rekonstruiert (Zeile 1 und 2 Hinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define getDerivativeActivationFunction for convenience\n",
    "deriv = lambda p: NeuralNetwork.NeuralNetwork.getDerivitiveActivationFunction('linear')(p)\n",
    "# define prenon_activations for convenience\n",
    "prenon_activation_L2 = weights[1].dot(a_L1)+b[1]\n",
    "prenon_activation_L1 = weights[0].dot(X)+b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute delta of last layer\n",
    "deltas[-1] = (-(y-a_L2)*deriv(prenon_activation_L2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In the following `weights[1].T.dot(deltas[-1])` is a matrix product! The shapes of the matrices are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[-1].shape # Eine Spalte für jeden x-Wert im Batch [bzw. für jeden entsprechenden output y-Wert]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, hence, the dimension of their product is $(1 \\times 10)^T\\odot (1 \\times 1000)$. Eine Spalte für jeden x-Wert im Batch also 1000 Spalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1].T.dot(deltas[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deriv(prenon_activation_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagate this delta to previous layer\n",
    "i=range(len(deltas)-1)\n",
    "i=i[-1]\n",
    "deltas[i]=weights[i+1].T.dot(deltas[i+1])*deriv(prenon_activation_L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return Error derivatives wrt weights (Zeile 3 Hinton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die letzte activation, i.e. `activations[2]=a_L2` des Output Layers, brauchen wir nicht für $\\frac{\\partial E}{\\partial w_{ij}}$ [s. in Hinton Zeile 3 steht $y_i$ (activation of **previous** layer) und nicht $y_j$ (activation of current layer)].\n",
    "\n",
    "> Goodfellow Algo 6.4, Backprop: \n",
    ">\n",
    "> $$\\nabla_{\\bf{W}^{(k)}} J=\\bf{g}{\\bf{h}^{(k-1)}}^T$$\n",
    "\n",
    "Auch hier startet die $\\frac{\\partial E}{\\partial w_{ij}}$ Rechnung bei $k-1$ ($k=l,l-1,\\dotso,1$), also mit der vorletzten activation $\\bf{h}^{(l-1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Diese activation geht nicht dE/dw ein !\n",
    "activations=[X,a_L1,a_L2]\n",
    "activations[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Die vorletzte activation aber schon\n",
    "activations=[X,a_L1,a_L2]\n",
    "activations[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dies ist $\\frac{\\partial E}{\\partial z_j}$ ist `deltas[j]` (wobei `j` der current layer ist):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung**: `len(deltas)=2` und `len(activations)=3`, sodass `i` im folgenden Code nur die ersten beiden activations anspricht (die Output layer activation soll ja nicht in die Rechnung eingehen, wie gerade oben in dieser Section gesagt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return Error derivatives wrt weights\n",
    "dw=[]\n",
    "db=[]\n",
    "dw=[d.dot(activations[i].T) for i, d in enumerate(deltas)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product d.dot(activations[i].T) has dimensions:\n",
      "(1, 1000)  x  (1000, 10)\n",
      "The resulting product has dimensions:\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "print('The dot product d.dot(activations[i].T) has dimensions:')\n",
    "print(deltas[1].shape, ' x ', activations[1].T.shape)\n",
    "print('The resulting product has dimensions:')\n",
    "print(deltas[1].dot(activations[1].T).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********\n",
    "How does `enumerate()` work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 3), (2, 13)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how does enumerate work\n",
    "r=[2,3,13]\n",
    "list(enumerate(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objekt1, objekt2 = list(enumerate(r))[0]\n",
    "objekt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation der Hinton Folie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung**: Hintons `i` ist sozusagen `j-1`. Hinton will folgenden Algo mit der Folie beschreiben: \n",
    "1. Compute for all j $$\\frac{\\partial E}{\\partial z_{j}}=\\left[y_{j}(1-y_{j})\\right]_{y_j=\\sigma(\\text{z_s[-1]})}\\frac{\\partial E}{\\partial y_j}\\quad \\text{for }j=1\\,,\\dotso\\,,\\text{batch_size}$$\n",
    "(where $\\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)$ (np.array with dimension ($1$, `batch_size`)) and $y_{j}(1-y_{j})$ can be computed using the forward pass results).\n",
    "\n",
    "`z_s[-1]` is the **PRE**-nonlinearity activation of the last layer [könnte man aber auch mit `a_s[-1]` implementieren, man müsste dann nur `.getDerivitiveActivationFunction()` umschreiben].\n",
    "\n",
    "2. Compute **dot product** of the weight matrix which connects layer `j` and `j-1` and $\\frac{\\partial E}{\\partial z_{j}}$ from step 1: $$\\frac{\\partial E}{\\partial y_{j-1}}=\\sum_j w_{j-1,j}\\underbrace{\\frac{\\partial E}{\\partial z_j}}_\\text{bereits in 1. Zeile berechnet}$$\n",
    "or in other words: `self.weights[i+1].T.dot(deltas[i+1])`\n",
    "3. Nicht auf Folie, aber Hinton meint: Repeat step 1: Use $\\frac{\\partial E}{\\partial y_{j-1}}$ from step 2 to calculate $$\\frac{\\partial E}{\\partial z_{j-1}}$$\n",
    "or in other words compute $$\\frac{\\partial E}{\\partial z_{j-1}}=y_{j-1}(1-y_{j-1})\\underbrace{\\left(\\sum_j w_{j-1,j}\\frac{\\partial E}{\\partial z_j}\\right)}_\\text{bereits in 2. Zeile berechnet}$$\n",
    "4. Use $\\frac{\\partial E}{\\partial z_{j}}$ from step 1 to compute for all `j` (i.e. for all points in the batch) $$\\frac{\\partial E}{\\partial w_{j-1,j}}=y_{j-1}\\frac{\\partial E}{\\partial z_j}\\quad \\text{for }j=1\\,,\\dotso\\,,\\text{batch_size}$$\n",
    "which is a normal matrix product `dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]` because (s. Diskussion und Rekonstruktion zu `dw` oben)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hinton Folie Backprop](https://i.stack.imgur.com/OfPD6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die erste Zeile auf der Folie entspricht\n",
    "\n",
    "`# insert the last layer error\n",
    "deltas[-1] = (-(y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))`\n",
    "\n",
    "Dann wird die 2. Zeile (unter Benutzung des Ergebnisses der 1. Zeile $\\frac{\\partial E}{\\partial z_j}$) ausgeführt, was\n",
    "\n",
    "`self.weights[i+1].T.dot(deltas[i+1])`\n",
    "\n",
    "in der `for` loop entspricht. Danach wird rekursiv wieder die 1. Zeile (unter Benutzung des Ergebnisses der 2. Zeile $\\frac{\\partial E}{\\partial y_i}$) ausgeführt, d.h. $$\\frac{\\partial E}{\\partial z_{j-1}}=y_{j-1}(1-y_{j-1})\\underbrace{\\left(\\sum_j w_{j-1,j}\\frac{\\partial E}{\\partial z_j}\\right)}_\\text{bereits in 2. Zeile berechnet}\\,,$$ wobei $y_{j-1}$ die activation der previous layer ist. Dies entspricht\n",
    "\n",
    "`# Perform BackPropagation\n",
    "for i in reversed(range(len(deltas)-1)):\n",
    "    deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "und die 3. Zeile entspricht\n",
    "\n",
    "`dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]`\n",
    "\n",
    "wobei `/float(batch_size)`, weil bei Minibatch der loss $E$ mit der batch_size normalized werden muss (s. ML Folie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise product `*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar multiplication is element-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.arange(4)\n",
    "t*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector multiplication is also element-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 10, 18])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=np.array([1,2,3])\n",
    "m=np.array([4,5,6])\n",
    "k*m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix products **must not** be performed with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,2) (2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-5472db04532a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (2,3) "
     ]
    }
   ],
   "source": [
    "# does not work ! use .dot() instead\n",
    "n=np.array([[1,2,3],[4,5,6]]).T\n",
    "o=np.array([[1,2,3],[4,5,6]])\n",
    "n*o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... although it works with one-dimensional matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 4, 6],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad coding style\n",
    "n2=np.array([[1,2,3]]).T\n",
    "o2=np.array([[1,2,3]])\n",
    "n2*o2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zu .getDerivitiveActivationFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14, -16])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conditional np.array filtering\n",
    "q=np.array([13,-14,15,-16])\n",
    "q[q<0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Shuffling ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://datascience.stackexchange.com/questions/24511/why-should-the-data-be-shuffled-for-machine-learning-tasks?newreg=cf6d76873092423c93d59539b758c42d)\n",
    "\n",
    "**Note: throughout this answer I refer to minimization of training loss and I do not discuss stopping criteria such as validation loss. The choice of stopping criteria does not affect the process/concepts described below.**\n",
    "\n",
    "The process of training a neural network is to find the minimum value of a loss function $\\mathcal{L}_X(W)$\n",
    ", where $W$ represents a matrix (or several matrices) of weights between neurons and $X$ represents the training dataset. I use a subscript for $X$ to indicate that our minimization of $\\mathcal{L}$ occurs only over the weights $W$ (that is, we are looking for $W$ such that $\\mathcal{L}$ is minimized) while $X$ is fixed.\n",
    "\n",
    "Now, if we assume that we have $P$ elements in $W$ (that is, there are $P$ weights in the network), $\\mathcal{L}$ is a surface in a $P$+1-dimensional space. To give a visual analogue, imagine that we have only two neuron weights ($P$=2). Then $\\mathcal{L}$ has an easy geometric interpretation: it is a surface in a 3-dimensional space. This arises from the fact that for any given matrices of weights $W$, the loss function can be evaluated on $X$ and that value becomes the elevation of the surface.\n",
    "\n",
    "But there is the problem of non-convexity; the surface I described will have numerous local minima, and therefore gradient descent algorithms are susceptible to becoming \"stuck\" in those minima while a deeper/lower/better solution may lie nearby. This is likely to occur if $X$ is unchanged over all training iterations, because the surface is fixed for a given $X$; all its features are static, including its various minima.\n",
    "\n",
    "A solution to this is mini-batch training combined with shuffling. By shuffling the rows and training on only a subset of them during a given iteration, $X$ changes with every iteration, and it is actually quite possible that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same $X$. The effect is that the solver can easily \"bounce\" out of a local minimum. Imagine that the solver is stuck in a local minimum at iteration 𝑖 with training mini-batch $X_i$. This local minimum corresponds to $\\mathcal{L}$ evaluated at a particular value of weights; we'll call it $\\mathcal{L}_{X_i}(W_i)$. On the next iteration the shape of our loss surface actually changes because we are using $X_{i+1}$, that is, $\\mathcal{L}_{X_{i+1}}(W_i)$ may take on a very different value from $\\mathcal{L}_{X_i}(W_i)$ and it is quite possible that it does not correspond to a local minimum! We can now compute a gradient update and continue with training. To be clear: the shape of $\\mathcal{L}_{X_{i+1}}$ will -- in general -- be different from that of $\\mathcal{L}_{X_i}$. Note that here I am referring to the loss function $\\mathcal{L}$ evaluated on a training set $X$; it is a complete surface defined over all possible values of $W$, rather than the evaluation of that loss (which is just a scalar) for a specific value of $W$. Note also that if mini-batches are used without shuffling there is still a degree of \"diversification\" of loss surfaces, but there will be a finite (and relatively small) number of unique error surfaces seen by the solver (specifically, it will see the same exact set of mini-batches -- and therefore loss surfaces -- during each epoch).\n",
    "\n",
    "One thing I deliberately avoided was a discussion of mini-batch sizes, because there are a million opinions on this and it has significant practical implications (greater parallelization can be achieved with larger batches). However, I believe the following is worth mentioning. Because $\\mathcal{L}$ is evaluated by computing a value for each row of $X$ (and summing or taking the average; i.e., a commutative operator) for a given set of weight matrices $W$, the arrangement of the rows of $X$ has no effect when using full-batch gradient descent (that is, when each batch is the full $X$, and iterations and epochs are the same thing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
