<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_vm44gmi40r2u-1{list-style-type:none}ul.lst-kix_vm44gmi40r2u-0{list-style-type:none}ul.lst-kix_vm44gmi40r2u-3{list-style-type:none}ul.lst-kix_vm44gmi40r2u-2{list-style-type:none}ul.lst-kix_i64ijt69btn4-6{list-style-type:none}ul.lst-kix_vm44gmi40r2u-8{list-style-type:none}ul.lst-kix_i64ijt69btn4-7{list-style-type:none}ul.lst-kix_i64ijt69btn4-8{list-style-type:none}ul.lst-kix_vm44gmi40r2u-5{list-style-type:none}ul.lst-kix_i64ijt69btn4-2{list-style-type:none}ul.lst-kix_vm44gmi40r2u-4{list-style-type:none}ul.lst-kix_i64ijt69btn4-3{list-style-type:none}ul.lst-kix_vm44gmi40r2u-7{list-style-type:none}ul.lst-kix_i64ijt69btn4-4{list-style-type:none}ul.lst-kix_vm44gmi40r2u-6{list-style-type:none}ul.lst-kix_i64ijt69btn4-5{list-style-type:none}ul.lst-kix_i64ijt69btn4-0{list-style-type:none}ul.lst-kix_i64ijt69btn4-1{list-style-type:none}.lst-kix_ko5ielnjz2sf-0>li:before{content:"\0025cf  "}.lst-kix_ko5ielnjz2sf-2>li:before{content:"\0025a0  "}.lst-kix_ko5ielnjz2sf-3>li:before{content:"\0025cf  "}ul.lst-kix_rzjrajl6hx6o-0{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-2{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-1{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-4{list-style-type:none}.lst-kix_ko5ielnjz2sf-1>li:before{content:"\0025cb  "}ul.lst-kix_rzjrajl6hx6o-3{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-6{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-5{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-8{list-style-type:none}ul.lst-kix_rzjrajl6hx6o-7{list-style-type:none}.lst-kix_ko5ielnjz2sf-8>li:before{content:"\0025a0  "}.lst-kix_ko5ielnjz2sf-6>li:before{content:"\0025cf  "}.lst-kix_ko5ielnjz2sf-7>li:before{content:"\0025cb  "}.lst-kix_ko5ielnjz2sf-4>li:before{content:"\0025cb  "}.lst-kix_ko5ielnjz2sf-5>li:before{content:"\0025a0  "}.lst-kix_vuu595jt9mz7-0>li:before{content:"\0025cf  "}.lst-kix_8ty6am9dc4ch-7>li:before{content:"\0025cb  "}.lst-kix_vuu595jt9mz7-2>li:before{content:"\0025a0  "}.lst-kix_8ty6am9dc4ch-3>li:before{content:"\0025cf  "}.lst-kix_8ty6am9dc4ch-5>li:before{content:"\0025a0  "}.lst-kix_vuu595jt9mz7-8>li:before{content:"\0025a0  "}.lst-kix_otwzvwudtuep-7>li:before{content:"\0025cb  "}.lst-kix_vuu595jt9mz7-6>li:before{content:"\0025cf  "}ul.lst-kix_hhjcx5eamba3-1{list-style-type:none}ul.lst-kix_hhjcx5eamba3-0{list-style-type:none}.lst-kix_vuu595jt9mz7-4>li:before{content:"\0025cb  "}ul.lst-kix_hhjcx5eamba3-3{list-style-type:none}.lst-kix_otwzvwudtuep-5>li:before{content:"\0025a0  "}ul.lst-kix_hhjcx5eamba3-2{list-style-type:none}ul.lst-kix_hhjcx5eamba3-5{list-style-type:none}ul.lst-kix_hhjcx5eamba3-4{list-style-type:none}ul.lst-kix_hhjcx5eamba3-7{list-style-type:none}ul.lst-kix_hhjcx5eamba3-6{list-style-type:none}ul.lst-kix_iu8p9gst7pod-8{list-style-type:none}.lst-kix_otwzvwudtuep-1>li:before{content:"\0025cb  "}.lst-kix_jr91aycyc992-0>li:before{content:"\0025cf  "}ul.lst-kix_iu8p9gst7pod-2{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-3{list-style-type:none}ul.lst-kix_iu8p9gst7pod-3{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-4{list-style-type:none}ul.lst-kix_iu8p9gst7pod-0{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-1{list-style-type:none}ul.lst-kix_iu8p9gst7pod-1{list-style-type:none}.lst-kix_otwzvwudtuep-3>li:before{content:"\0025cf  "}ul.lst-kix_xlk5poyb1w7k-2{list-style-type:none}ul.lst-kix_iu8p9gst7pod-6{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-7{list-style-type:none}ul.lst-kix_iu8p9gst7pod-7{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-8{list-style-type:none}ul.lst-kix_iu8p9gst7pod-4{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-5{list-style-type:none}ul.lst-kix_iu8p9gst7pod-5{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-6{list-style-type:none}ul.lst-kix_xlk5poyb1w7k-0{list-style-type:none}.lst-kix_jr91aycyc992-8>li:before{content:"\0025a0  "}.lst-kix_3vkdlyb1qfl7-5>li:before{content:"\0025a0  "}.lst-kix_jr91aycyc992-2>li:before{content:"\0025a0  "}.lst-kix_3vkdlyb1qfl7-3>li:before{content:"\0025cf  "}ul.lst-kix_hhjcx5eamba3-8{list-style-type:none}.lst-kix_hze350db386w-4>li:before{content:"\0025cb  "}.lst-kix_hze350db386w-6>li:before{content:"\0025cf  "}.lst-kix_jr91aycyc992-4>li:before{content:"\0025cb  "}.lst-kix_hze350db386w-8>li:before{content:"\0025a0  "}.lst-kix_i64ijt69btn4-8>li:before{content:"\0025a0  "}.lst-kix_jr91aycyc992-6>li:before{content:"\0025cf  "}.lst-kix_3vkdlyb1qfl7-7>li:before{content:"\0025cb  "}.lst-kix_i64ijt69btn4-6>li:before{content:"\0025cf  "}.lst-kix_i64ijt69btn4-2>li:before{content:"\0025a0  "}.lst-kix_hze350db386w-2>li:before{content:"\0025a0  "}.lst-kix_8ty6am9dc4ch-1>li:before{content:"\0025cb  "}.lst-kix_i64ijt69btn4-4>li:before{content:"\0025cb  "}.lst-kix_3vkdlyb1qfl7-1>li:before{content:"\0025cb  "}.lst-kix_hze350db386w-0>li:before{content:"\0025cf  "}.lst-kix_i64ijt69btn4-0>li:before{content:"\0025cf  "}.lst-kix_dffublbbtuo-0>li:before{content:"\0025cf  "}.lst-kix_owldsdw60k96-3>li:before{content:"\0025cf  "}.lst-kix_dffublbbtuo-3>li:before{content:"\0025cf  "}.lst-kix_owldsdw60k96-4>li:before{content:"\0025cb  "}.lst-kix_owldsdw60k96-8>li:before{content:"\0025a0  "}ul.lst-kix_3rnw7d266aai-1{list-style-type:none}.lst-kix_dffublbbtuo-4>li:before{content:"\0025cb  "}ul.lst-kix_3rnw7d266aai-2{list-style-type:none}ul.lst-kix_3rnw7d266aai-3{list-style-type:none}ul.lst-kix_3rnw7d266aai-4{list-style-type:none}ul.lst-kix_3rnw7d266aai-0{list-style-type:none}.lst-kix_dffublbbtuo-8>li:before{content:"\0025a0  "}.lst-kix_tw47x1th0rzd-0>li:before{content:"\0025cf  "}ul.lst-kix_3rnw7d266aai-5{list-style-type:none}ul.lst-kix_34siv8i9m0f-7{list-style-type:none}ul.lst-kix_3rnw7d266aai-6{list-style-type:none}ul.lst-kix_34siv8i9m0f-8{list-style-type:none}.lst-kix_dffublbbtuo-7>li:before{content:"\0025cb  "}ul.lst-kix_3rnw7d266aai-7{list-style-type:none}.lst-kix_owldsdw60k96-0>li:before{content:"\0025cf  "}ul.lst-kix_3rnw7d266aai-8{list-style-type:none}ul.lst-kix_34siv8i9m0f-3{list-style-type:none}ul.lst-kix_34siv8i9m0f-4{list-style-type:none}ul.lst-kix_34siv8i9m0f-5{list-style-type:none}ul.lst-kix_34siv8i9m0f-6{list-style-type:none}ul.lst-kix_34siv8i9m0f-0{list-style-type:none}ul.lst-kix_34siv8i9m0f-1{list-style-type:none}ul.lst-kix_34siv8i9m0f-2{list-style-type:none}.lst-kix_tw47x1th0rzd-8>li:before{content:"\0025a0  "}.lst-kix_tw47x1th0rzd-1>li:before{content:"\0025cb  "}.lst-kix_vm44gmi40r2u-0>li:before{content:"\0025cf  "}.lst-kix_pl51pjh7ltk8-1>li:before{content:"\0025cb  "}.lst-kix_pl51pjh7ltk8-5>li:before{content:"\0025a0  "}.lst-kix_tw47x1th0rzd-4>li:before{content:"\0025cb  "}.lst-kix_tw47x1th0rzd-5>li:before{content:"\0025a0  "}.lst-kix_owldsdw60k96-7>li:before{content:"\0025cb  "}.lst-kix_pl51pjh7ltk8-2>li:before{content:"\0025a0  "}.lst-kix_eb6ic6z2m9sh-3>li:before{content:"\0025cf  "}.lst-kix_eb6ic6z2m9sh-7>li:before{content:"\0025cb  "}.lst-kix_eb6ic6z2m9sh-0>li:before{content:"\0025cf  "}.lst-kix_eb6ic6z2m9sh-8>li:before{content:"\0025a0  "}.lst-kix_pl51pjh7ltk8-6>li:before{content:"\0025cf  "}ul.lst-kix_8ty6am9dc4ch-7{list-style-type:none}ul.lst-kix_8ty6am9dc4ch-8{list-style-type:none}ul.lst-kix_8ty6am9dc4ch-3{list-style-type:none}ul.lst-kix_8ty6am9dc4ch-4{list-style-type:none}ul.lst-kix_8ty6am9dc4ch-5{list-style-type:none}.lst-kix_vm44gmi40r2u-3>li:before{content:"\0025cf  "}ul.lst-kix_8ty6am9dc4ch-6{list-style-type:none}ul.lst-kix_8ty6am9dc4ch-0{list-style-type:none}ul.lst-kix_8ty6am9dc4ch-1{list-style-type:none}.lst-kix_vm44gmi40r2u-4>li:before{content:"\0025cb  "}ul.lst-kix_8ty6am9dc4ch-2{list-style-type:none}.lst-kix_eb6ic6z2m9sh-4>li:before{content:"\0025cb  "}.lst-kix_vm44gmi40r2u-8>li:before{content:"\0025a0  "}.lst-kix_vm44gmi40r2u-7>li:before{content:"\0025cb  "}ul.lst-kix_yc6oucdrd8k8-0{list-style-type:none}ul.lst-kix_dffublbbtuo-0{list-style-type:none}ul.lst-kix_dffublbbtuo-1{list-style-type:none}ul.lst-kix_dffublbbtuo-2{list-style-type:none}ul.lst-kix_dffublbbtuo-3{list-style-type:none}ul.lst-kix_dffublbbtuo-4{list-style-type:none}ul.lst-kix_dffublbbtuo-5{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-2{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-1{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-4{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-3{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-6{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-5{list-style-type:none}ul.lst-kix_yc6oucdrd8k8-8{list-style-type:none}.lst-kix_8ty6am9dc4ch-6>li:before{content:"\0025cf  "}ul.lst-kix_yc6oucdrd8k8-7{list-style-type:none}.lst-kix_vuu595jt9mz7-1>li:before{content:"\0025cb  "}.lst-kix_5gjpir8u93bi-3>li:before{content:"\0025cf  "}.lst-kix_otwzvwudtuep-8>li:before{content:"\0025a0  "}ul.lst-kix_5gjpir8u93bi-0{list-style-type:none}.lst-kix_5gjpir8u93bi-7>li:before{content:"\0025cb  "}ul.lst-kix_5gjpir8u93bi-1{list-style-type:none}.lst-kix_vuu595jt9mz7-5>li:before{content:"\0025a0  "}.lst-kix_jr91aycyc992-1>li:before{content:"\0025cb  "}.lst-kix_otwzvwudtuep-0>li:before{content:"\0025cf  "}.lst-kix_otwzvwudtuep-4>li:before{content:"\0025cb  "}.lst-kix_3rnw7d266aai-3>li:before{content:"\0025cf  "}.lst-kix_3vkdlyb1qfl7-8>li:before{content:"\0025a0  "}.lst-kix_wdeqgbjjk4l4-5>li:before{content:"\0025a0  "}.lst-kix_3vkdlyb1qfl7-4>li:before{content:"\0025cb  "}ul.lst-kix_5gjpir8u93bi-6{list-style-type:none}ul.lst-kix_pl51pjh7ltk8-0{list-style-type:none}ul.lst-kix_5gjpir8u93bi-7{list-style-type:none}.lst-kix_wdeqgbjjk4l4-1>li:before{content:"\0025cb  "}ul.lst-kix_pl51pjh7ltk8-1{list-style-type:none}ul.lst-kix_42xno5nwptez-0{list-style-type:none}ul.lst-kix_5gjpir8u93bi-8{list-style-type:none}ul.lst-kix_pl51pjh7ltk8-2{list-style-type:none}ul.lst-kix_42xno5nwptez-2{list-style-type:none}ul.lst-kix_5gjpir8u93bi-2{list-style-type:none}ul.lst-kix_42xno5nwptez-1{list-style-type:none}ul.lst-kix_5gjpir8u93bi-3{list-style-type:none}ul.lst-kix_42xno5nwptez-4{list-style-type:none}ul.lst-kix_5gjpir8u93bi-4{list-style-type:none}ul.lst-kix_42xno5nwptez-3{list-style-type:none}ul.lst-kix_5gjpir8u93bi-5{list-style-type:none}ul.lst-kix_pl51pjh7ltk8-7{list-style-type:none}.lst-kix_jr91aycyc992-5>li:before{content:"\0025a0  "}ul.lst-kix_pl51pjh7ltk8-8{list-style-type:none}ul.lst-kix_pl51pjh7ltk8-3{list-style-type:none}.lst-kix_hze350db386w-7>li:before{content:"\0025cb  "}ul.lst-kix_pl51pjh7ltk8-4{list-style-type:none}ul.lst-kix_pl51pjh7ltk8-5{list-style-type:none}.lst-kix_i64ijt69btn4-7>li:before{content:"\0025cb  "}ul.lst-kix_pl51pjh7ltk8-6{list-style-type:none}.lst-kix_8ty6am9dc4ch-2>li:before{content:"\0025a0  "}.lst-kix_hze350db386w-3>li:before{content:"\0025cf  "}.lst-kix_i64ijt69btn4-3>li:before{content:"\0025cf  "}.lst-kix_3vkdlyb1qfl7-0>li:before{content:"\0025cf  "}.lst-kix_sezodd14tbn8-2>li:before{content:"\0025a0  "}.lst-kix_1qx006ze2ww-7>li:before{content:"\0025cb  "}.lst-kix_sezodd14tbn8-4>li:before{content:"\0025cb  "}.lst-kix_1qx006ze2ww-2>li:before{content:"\0025a0  "}.lst-kix_1qx006ze2ww-5>li:before{content:"\0025a0  "}.lst-kix_1qx006ze2ww-4>li:before{content:"\0025cb  "}.lst-kix_sezodd14tbn8-1>li:before{content:"\0025cb  "}.lst-kix_xlk5poyb1w7k-6>li:before{content:"\0025cf  "}.lst-kix_xlk5poyb1w7k-8>li:before{content:"\0025a0  "}.lst-kix_xlk5poyb1w7k-5>li:before{content:"\0025a0  "}.lst-kix_kipghvp0ii5t-1>li:before{content:"\0025cb  "}.lst-kix_xlk5poyb1w7k-0>li:before{content:"\0025cf  "}.lst-kix_xlk5poyb1w7k-3>li:before{content:"\0025cf  "}.lst-kix_sezodd14tbn8-7>li:before{content:"\0025cb  "}.lst-kix_3rnw7d266aai-4>li:before{content:"\0025cb  "}ul.lst-kix_kipghvp0ii5t-8{list-style-type:none}ul.lst-kix_kipghvp0ii5t-7{list-style-type:none}.lst-kix_3rnw7d266aai-6>li:before{content:"\0025cf  "}ul.lst-kix_kipghvp0ii5t-2{list-style-type:none}.lst-kix_kipghvp0ii5t-7>li:before{content:"\0025cb  "}ul.lst-kix_kipghvp0ii5t-1{list-style-type:none}ul.lst-kix_kipghvp0ii5t-0{list-style-type:none}.lst-kix_3rnw7d266aai-7>li:before{content:"\0025cb  "}ul.lst-kix_kipghvp0ii5t-6{list-style-type:none}ul.lst-kix_kipghvp0ii5t-5{list-style-type:none}ul.lst-kix_kipghvp0ii5t-4{list-style-type:none}ul.lst-kix_kipghvp0ii5t-3{list-style-type:none}.lst-kix_kipghvp0ii5t-6>li:before{content:"\0025cf  "}ul.lst-kix_1qx006ze2ww-8{list-style-type:none}ul.lst-kix_1qx006ze2ww-7{list-style-type:none}ul.lst-kix_1qx006ze2ww-6{list-style-type:none}ul.lst-kix_1qx006ze2ww-5{list-style-type:none}ul.lst-kix_1qx006ze2ww-4{list-style-type:none}ul.lst-kix_1qx006ze2ww-3{list-style-type:none}ul.lst-kix_1qx006ze2ww-2{list-style-type:none}.lst-kix_kipghvp0ii5t-4>li:before{content:"\0025cb  "}ul.lst-kix_1qx006ze2ww-1{list-style-type:none}ul.lst-kix_1qx006ze2ww-0{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-8{list-style-type:none}ul.lst-kix_eddvhvlfcek7-4{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-7{list-style-type:none}ul.lst-kix_eddvhvlfcek7-5{list-style-type:none}ul.lst-kix_eddvhvlfcek7-2{list-style-type:none}ul.lst-kix_eddvhvlfcek7-3{list-style-type:none}ul.lst-kix_eddvhvlfcek7-8{list-style-type:none}ul.lst-kix_eddvhvlfcek7-6{list-style-type:none}ul.lst-kix_eddvhvlfcek7-7{list-style-type:none}ul.lst-kix_eddvhvlfcek7-0{list-style-type:none}ul.lst-kix_eddvhvlfcek7-1{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-0{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-2{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-1{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-4{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-3{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-6{list-style-type:none}ul.lst-kix_mwssa4aj5yrr-5{list-style-type:none}.lst-kix_btugr32bfvpf-5>li:before{content:"\0025a0  "}.lst-kix_btugr32bfvpf-3>li:before{content:"\0025cf  "}ul.lst-kix_fuksty3c10yn-5{list-style-type:none}ul.lst-kix_fuksty3c10yn-6{list-style-type:none}ul.lst-kix_fuksty3c10yn-7{list-style-type:none}ul.lst-kix_fuksty3c10yn-8{list-style-type:none}ul.lst-kix_fuksty3c10yn-1{list-style-type:none}ul.lst-kix_fuksty3c10yn-2{list-style-type:none}ul.lst-kix_fuksty3c10yn-3{list-style-type:none}ul.lst-kix_fuksty3c10yn-4{list-style-type:none}.lst-kix_5gjpir8u93bi-4>li:before{content:"\0025cb  "}.lst-kix_f176rxr8jwn6-8>li:before{content:"\0025a0  "}ul.lst-kix_fuksty3c10yn-0{list-style-type:none}.lst-kix_jo7lvhxrgmeg-7>li:before{content:"\0025cb  "}.lst-kix_5gjpir8u93bi-6>li:before{content:"\0025cf  "}.lst-kix_34siv8i9m0f-5>li:before{content:"\0025a0  "}.lst-kix_34siv8i9m0f-7>li:before{content:"\0025cb  "}ul.lst-kix_vuu595jt9mz7-2{list-style-type:none}ul.lst-kix_vuu595jt9mz7-1{list-style-type:none}.lst-kix_wdeqgbjjk4l4-8>li:before{content:"\0025a0  "}ul.lst-kix_vuu595jt9mz7-0{list-style-type:none}ul.lst-kix_otwzvwudtuep-8{list-style-type:none}ul.lst-kix_otwzvwudtuep-7{list-style-type:none}ul.lst-kix_otwzvwudtuep-6{list-style-type:none}ul.lst-kix_otwzvwudtuep-5{list-style-type:none}ul.lst-kix_otwzvwudtuep-4{list-style-type:none}ul.lst-kix_otwzvwudtuep-3{list-style-type:none}ul.lst-kix_otwzvwudtuep-2{list-style-type:none}ul.lst-kix_btugr32bfvpf-0{list-style-type:none}ul.lst-kix_vuu595jt9mz7-8{list-style-type:none}ul.lst-kix_otwzvwudtuep-1{list-style-type:none}ul.lst-kix_btugr32bfvpf-1{list-style-type:none}ul.lst-kix_vuu595jt9mz7-7{list-style-type:none}ul.lst-kix_otwzvwudtuep-0{list-style-type:none}ul.lst-kix_btugr32bfvpf-2{list-style-type:none}ul.lst-kix_vuu595jt9mz7-6{list-style-type:none}ul.lst-kix_btugr32bfvpf-3{list-style-type:none}ul.lst-kix_vuu595jt9mz7-5{list-style-type:none}.lst-kix_wdeqgbjjk4l4-6>li:before{content:"\0025cf  "}ul.lst-kix_btugr32bfvpf-4{list-style-type:none}ul.lst-kix_vuu595jt9mz7-4{list-style-type:none}ul.lst-kix_btugr32bfvpf-5{list-style-type:none}ul.lst-kix_vuu595jt9mz7-3{list-style-type:none}.lst-kix_wdeqgbjjk4l4-0>li:before{content:"\0025cf  "}.lst-kix_b06p333wtkc4-3>li:before{content:"\0025cf  "}.lst-kix_b06p333wtkc4-5>li:before{content:"\0025a0  "}.lst-kix_yc6oucdrd8k8-1>li:before{content:"\0025cb  "}.lst-kix_yc6oucdrd8k8-3>li:before{content:"\0025cf  "}.lst-kix_f176rxr8jwn6-2>li:before{content:"\0025a0  "}.lst-kix_jo7lvhxrgmeg-1>li:before{content:"\0025cb  "}.lst-kix_f176rxr8jwn6-0>li:before{content:"\0025cf  "}ul.lst-kix_ixeboairdkos-7{list-style-type:none}ul.lst-kix_ixeboairdkos-8{list-style-type:none}.lst-kix_mwssa4aj5yrr-5>li:before{content:"\0025a0  "}ul.lst-kix_ixeboairdkos-1{list-style-type:none}ul.lst-kix_ixeboairdkos-2{list-style-type:none}ul.lst-kix_ixeboairdkos-0{list-style-type:none}ul.lst-kix_ixeboairdkos-5{list-style-type:none}ul.lst-kix_ixeboairdkos-6{list-style-type:none}ul.lst-kix_ixeboairdkos-3{list-style-type:none}ul.lst-kix_ixeboairdkos-4{list-style-type:none}.lst-kix_dffublbbtuo-1>li:before{content:"\0025cb  "}ul.lst-kix_k10nxdwmave6-0{list-style-type:none}.lst-kix_mwssa4aj5yrr-2>li:before{content:"\0025a0  "}.lst-kix_owldsdw60k96-5>li:before{content:"\0025a0  "}ul.lst-kix_k10nxdwmave6-3{list-style-type:none}ul.lst-kix_ujrw2gdovb97-7{list-style-type:none}ul.lst-kix_k10nxdwmave6-4{list-style-type:none}ul.lst-kix_ujrw2gdovb97-8{list-style-type:none}.lst-kix_ixeboairdkos-2>li:before{content:"\0025a0  "}ul.lst-kix_k10nxdwmave6-1{list-style-type:none}ul.lst-kix_ujrw2gdovb97-5{list-style-type:none}ul.lst-kix_k10nxdwmave6-2{list-style-type:none}ul.lst-kix_ujrw2gdovb97-6{list-style-type:none}ul.lst-kix_ujrw2gdovb97-3{list-style-type:none}ul.lst-kix_ujrw2gdovb97-4{list-style-type:none}ul.lst-kix_ujrw2gdovb97-1{list-style-type:none}ul.lst-kix_ujrw2gdovb97-2{list-style-type:none}.lst-kix_dffublbbtuo-6>li:before{content:"\0025cf  "}ul.lst-kix_ujrw2gdovb97-0{list-style-type:none}.lst-kix_owldsdw60k96-2>li:before{content:"\0025a0  "}.lst-kix_iu8p9gst7pod-8>li:before{content:"\0025a0  "}.lst-kix_yc6oucdrd8k8-6>li:before{content:"\0025cf  "}.lst-kix_hhjcx5eamba3-0>li:before{content:"\0025cf  "}.lst-kix_k10nxdwmave6-4>li:before{content:"\0025cb  "}.lst-kix_ixeboairdkos-7>li:before{content:"\0025cb  "}ul.lst-kix_k10nxdwmave6-7{list-style-type:none}ul.lst-kix_k10nxdwmave6-8{list-style-type:none}ul.lst-kix_k10nxdwmave6-5{list-style-type:none}ul.lst-kix_k10nxdwmave6-6{list-style-type:none}ul.lst-kix_wdeqgbjjk4l4-2{list-style-type:none}ul.lst-kix_wdeqgbjjk4l4-3{list-style-type:none}.lst-kix_tw47x1th0rzd-7>li:before{content:"\0025cb  "}ul.lst-kix_wdeqgbjjk4l4-0{list-style-type:none}ul.lst-kix_wdeqgbjjk4l4-1{list-style-type:none}.lst-kix_iu8p9gst7pod-0>li:before{content:"\0025cf  "}.lst-kix_hhjcx5eamba3-5>li:before{content:"\0025a0  "}.lst-kix_hhjcx5eamba3-8>li:before{content:"\0025a0  "}.lst-kix_iu8p9gst7pod-3>li:before{content:"\0025cf  "}.lst-kix_pl51pjh7ltk8-0>li:before{content:"\0025cf  "}ul.lst-kix_wdeqgbjjk4l4-8{list-style-type:none}.lst-kix_vm44gmi40r2u-2>li:before{content:"\0025a0  "}ul.lst-kix_wdeqgbjjk4l4-6{list-style-type:none}.lst-kix_5siicw1q5qtw-7>li:before{content:"\0025cb  "}ul.lst-kix_wdeqgbjjk4l4-7{list-style-type:none}ul.lst-kix_wdeqgbjjk4l4-4{list-style-type:none}ul.lst-kix_wdeqgbjjk4l4-5{list-style-type:none}ul.lst-kix_btugr32bfvpf-6{list-style-type:none}ul.lst-kix_owldsdw60k96-0{list-style-type:none}.lst-kix_pl51pjh7ltk8-3>li:before{content:"\0025cf  "}ul.lst-kix_btugr32bfvpf-7{list-style-type:none}ul.lst-kix_btugr32bfvpf-8{list-style-type:none}.lst-kix_tw47x1th0rzd-2>li:before{content:"\0025a0  "}ul.lst-kix_owldsdw60k96-8{list-style-type:none}ul.lst-kix_owldsdw60k96-7{list-style-type:none}ul.lst-kix_owldsdw60k96-6{list-style-type:none}ul.lst-kix_owldsdw60k96-5{list-style-type:none}ul.lst-kix_owldsdw60k96-4{list-style-type:none}ul.lst-kix_owldsdw60k96-3{list-style-type:none}ul.lst-kix_owldsdw60k96-2{list-style-type:none}ul.lst-kix_owldsdw60k96-1{list-style-type:none}.lst-kix_eb6ic6z2m9sh-6>li:before{content:"\0025cf  "}.lst-kix_pl51pjh7ltk8-8>li:before{content:"\0025a0  "}.lst-kix_eddvhvlfcek7-7>li:before{content:"\0025cb  "}.lst-kix_eb6ic6z2m9sh-1>li:before{content:"\0025cb  "}.lst-kix_5siicw1q5qtw-4>li:before{content:"\0025cb  "}.lst-kix_eddvhvlfcek7-2>li:before{content:"\0025a0  "}.lst-kix_vm44gmi40r2u-5>li:before{content:"\0025a0  "}.lst-kix_btugr32bfvpf-0>li:before{content:"\0025cf  "}.lst-kix_btugr32bfvpf-8>li:before{content:"\0025a0  "}.lst-kix_m5ey97dhxg5-8>li:before{content:"\0025a0  "}.lst-kix_ujrw2gdovb97-7>li:before{content:"\0025cb  "}.lst-kix_jo7lvhxrgmeg-4>li:before{content:"\0025cb  "}.lst-kix_f176rxr8jwn6-5>li:before{content:"\0025a0  "}.lst-kix_8ty6am9dc4ch-4>li:before{content:"\0025cb  "}.lst-kix_42xno5nwptez-0>li:before{content:"\0025cf  "}.lst-kix_vuu595jt9mz7-3>li:before{content:"\0025cf  "}.lst-kix_b06p333wtkc4-8>li:before{content:"\0025a0  "}.lst-kix_34siv8i9m0f-2>li:before{content:"\0025a0  "}.lst-kix_otwzvwudtuep-6>li:before{content:"\0025cf  "}.lst-kix_on37z5v164d5-4>li:before{content:"\0025cb  "}.lst-kix_5gjpir8u93bi-1>li:before{content:"\0025cb  "}.lst-kix_b06p333wtkc4-0>li:before{content:"\0025cf  "}.lst-kix_3vkdlyb1qfl7-6>li:before{content:"\0025cf  "}.lst-kix_jr91aycyc992-7>li:before{content:"\0025cb  "}.lst-kix_3rnw7d266aai-1>li:before{content:"\0025cb  "}.lst-kix_hze350db386w-5>li:before{content:"\0025a0  "}.lst-kix_42xno5nwptez-8>li:before{content:"\0025a0  "}.lst-kix_wdeqgbjjk4l4-3>li:before{content:"\0025cf  "}.lst-kix_i64ijt69btn4-5>li:before{content:"\0025a0  "}.lst-kix_rr55haih9k11-5>li:before{content:"\0025a0  "}.lst-kix_m5ey97dhxg5-0>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_k10nxdwmave6-1>li:before{content:"\0025cb  "}.lst-kix_7mgbb75pci5l-5>li:before{content:"\0025a0  "}ul.lst-kix_on37z5v164d5-0{list-style-type:none}.lst-kix_rzjrajl6hx6o-4>li:before{content:"\0025cb  "}ul.lst-kix_hze350db386w-1{list-style-type:none}ul.lst-kix_hze350db386w-0{list-style-type:none}ul.lst-kix_on37z5v164d5-6{list-style-type:none}ul.lst-kix_on37z5v164d5-5{list-style-type:none}.lst-kix_rzjrajl6hx6o-5>li:before{content:"\0025a0  "}.lst-kix_rzjrajl6hx6o-6>li:before{content:"\0025cf  "}ul.lst-kix_on37z5v164d5-8{list-style-type:none}ul.lst-kix_on37z5v164d5-7{list-style-type:none}ul.lst-kix_on37z5v164d5-2{list-style-type:none}ul.lst-kix_on37z5v164d5-1{list-style-type:none}.lst-kix_rzjrajl6hx6o-7>li:before{content:"\0025cb  "}ul.lst-kix_on37z5v164d5-4{list-style-type:none}ul.lst-kix_on37z5v164d5-3{list-style-type:none}ul.lst-kix_tw47x1th0rzd-0{list-style-type:none}ul.lst-kix_tw47x1th0rzd-1{list-style-type:none}.lst-kix_rzjrajl6hx6o-8>li:before{content:"\0025a0  "}ul.lst-kix_hze350db386w-3{list-style-type:none}ul.lst-kix_hze350db386w-2{list-style-type:none}ul.lst-kix_hze350db386w-5{list-style-type:none}ul.lst-kix_hze350db386w-4{list-style-type:none}ul.lst-kix_hze350db386w-7{list-style-type:none}ul.lst-kix_hze350db386w-6{list-style-type:none}ul.lst-kix_hze350db386w-8{list-style-type:none}.lst-kix_on37z5v164d5-0>li:before{content:"\0025cf  "}.lst-kix_on37z5v164d5-1>li:before{content:"\0025cb  "}.lst-kix_fuksty3c10yn-7>li:before{content:"\0025cb  "}.lst-kix_fuksty3c10yn-8>li:before{content:"\0025a0  "}.lst-kix_fuksty3c10yn-2>li:before{content:"\0025a0  "}.lst-kix_fuksty3c10yn-3>li:before{content:"\0025cf  "}.lst-kix_fuksty3c10yn-6>li:before{content:"\0025cf  "}ul.lst-kix_tw47x1th0rzd-8{list-style-type:none}ul.lst-kix_tw47x1th0rzd-6{list-style-type:none}ul.lst-kix_tw47x1th0rzd-7{list-style-type:none}.lst-kix_fuksty3c10yn-4>li:before{content:"\0025cb  "}ul.lst-kix_tw47x1th0rzd-4{list-style-type:none}ul.lst-kix_tw47x1th0rzd-5{list-style-type:none}.lst-kix_fuksty3c10yn-5>li:before{content:"\0025a0  "}ul.lst-kix_tw47x1th0rzd-2{list-style-type:none}ul.lst-kix_tw47x1th0rzd-3{list-style-type:none}.lst-kix_48ib1dz7zp04-7>li:before{content:"\0025cb  "}.lst-kix_ujrw2gdovb97-0>li:before{content:"\0025cf  "}.lst-kix_48ib1dz7zp04-5>li:before{content:"\0025a0  "}.lst-kix_48ib1dz7zp04-6>li:before{content:"\0025cf  "}.lst-kix_48ib1dz7zp04-3>li:before{content:"\0025cf  "}.lst-kix_ujrw2gdovb97-3>li:before{content:"\0025cf  "}.lst-kix_fuksty3c10yn-0>li:before{content:"\0025cf  "}.lst-kix_rzjrajl6hx6o-3>li:before{content:"\0025cf  "}.lst-kix_ujrw2gdovb97-1>li:before{content:"\0025cb  "}.lst-kix_ujrw2gdovb97-2>li:before{content:"\0025a0  "}.lst-kix_48ib1dz7zp04-4>li:before{content:"\0025cb  "}.lst-kix_fuksty3c10yn-1>li:before{content:"\0025cb  "}.lst-kix_rzjrajl6hx6o-2>li:before{content:"\0025a0  "}.lst-kix_48ib1dz7zp04-1>li:before{content:"\0025cb  "}.lst-kix_rzjrajl6hx6o-0>li:before{content:"\0025cf  "}.lst-kix_48ib1dz7zp04-2>li:before{content:"\0025a0  "}.lst-kix_rzjrajl6hx6o-1>li:before{content:"\0025cb  "}.lst-kix_48ib1dz7zp04-0>li:before{content:"\0025cf  "}.lst-kix_m5ey97dhxg5-7>li:before{content:"\0025cb  "}.lst-kix_m5ey97dhxg5-1>li:before{content:"\0025cb  "}.lst-kix_ujrw2gdovb97-8>li:before{content:"\0025a0  "}.lst-kix_m5ey97dhxg5-5>li:before{content:"\0025a0  "}.lst-kix_ujrw2gdovb97-4>li:before{content:"\0025cb  "}.lst-kix_m5ey97dhxg5-3>li:before{content:"\0025cf  "}.lst-kix_ujrw2gdovb97-6>li:before{content:"\0025cf  "}.lst-kix_42xno5nwptez-1>li:before{content:"\0025cb  "}.lst-kix_42xno5nwptez-5>li:before{content:"\0025a0  "}.lst-kix_42xno5nwptez-7>li:before{content:"\0025cb  "}.lst-kix_on37z5v164d5-7>li:before{content:"\0025cb  "}.lst-kix_on37z5v164d5-5>li:before{content:"\0025a0  "}.lst-kix_on37z5v164d5-3>li:before{content:"\0025cf  "}.lst-kix_42xno5nwptez-3>li:before{content:"\0025cf  "}.lst-kix_7mgbb75pci5l-2>li:before{content:"\0025a0  "}.lst-kix_7mgbb75pci5l-4>li:before{content:"\0025cb  "}.lst-kix_rr55haih9k11-6>li:before{content:"\0025cf  "}.lst-kix_7mgbb75pci5l-0>li:before{content:"\0025cf  "}.lst-kix_rr55haih9k11-8>li:before{content:"\0025a0  "}.lst-kix_rr55haih9k11-4>li:before{content:"\0025cb  "}.lst-kix_7mgbb75pci5l-6>li:before{content:"\0025cf  "}.lst-kix_rr55haih9k11-2>li:before{content:"\0025a0  "}.lst-kix_7mgbb75pci5l-8>li:before{content:"\0025a0  "}.lst-kix_rr55haih9k11-0>li:before{content:"\0025cf  "}.lst-kix_mwssa4aj5yrr-1>li:before{content:"\0025cb  "}.lst-kix_mwssa4aj5yrr-0>li:before{content:"\0025cf  "}ul.lst-kix_eb6ic6z2m9sh-6{list-style-type:none}ul.lst-kix_m5ey97dhxg5-0{list-style-type:none}ul.lst-kix_eb6ic6z2m9sh-5{list-style-type:none}ul.lst-kix_eb6ic6z2m9sh-8{list-style-type:none}ul.lst-kix_eb6ic6z2m9sh-7{list-style-type:none}.lst-kix_ixeboairdkos-5>li:before{content:"\0025a0  "}.lst-kix_ixeboairdkos-4>li:before{content:"\0025cb  "}.lst-kix_mwssa4aj5yrr-7>li:before{content:"\0025cb  "}.lst-kix_iu8p9gst7pod-6>li:before{content:"\0025cf  "}.lst-kix_yc6oucdrd8k8-7>li:before{content:"\0025cb  "}.lst-kix_yc6oucdrd8k8-8>li:before{content:"\0025a0  "}.lst-kix_mwssa4aj5yrr-8>li:before{content:"\0025a0  "}.lst-kix_iu8p9gst7pod-5>li:before{content:"\0025a0  "}ul.lst-kix_m5ey97dhxg5-3{list-style-type:none}ul.lst-kix_m5ey97dhxg5-4{list-style-type:none}ul.lst-kix_m5ey97dhxg5-1{list-style-type:none}ul.lst-kix_m5ey97dhxg5-2{list-style-type:none}ul.lst-kix_m5ey97dhxg5-7{list-style-type:none}.lst-kix_k10nxdwmave6-7>li:before{content:"\0025cb  "}ul.lst-kix_m5ey97dhxg5-8{list-style-type:none}.lst-kix_ixeboairdkos-8>li:before{content:"\0025a0  "}ul.lst-kix_m5ey97dhxg5-5{list-style-type:none}.lst-kix_k10nxdwmave6-6>li:before{content:"\0025cf  "}ul.lst-kix_m5ey97dhxg5-6{list-style-type:none}.lst-kix_hhjcx5eamba3-6>li:before{content:"\0025cf  "}.lst-kix_hhjcx5eamba3-7>li:before{content:"\0025cb  "}.lst-kix_iu8p9gst7pod-1>li:before{content:"\0025cb  "}.lst-kix_5siicw1q5qtw-6>li:before{content:"\0025cf  "}.lst-kix_iu8p9gst7pod-2>li:before{content:"\0025a0  "}.lst-kix_hhjcx5eamba3-2>li:before{content:"\0025a0  "}.lst-kix_hhjcx5eamba3-3>li:before{content:"\0025cf  "}ul.lst-kix_eb6ic6z2m9sh-2{list-style-type:none}ul.lst-kix_eb6ic6z2m9sh-1{list-style-type:none}ul.lst-kix_eb6ic6z2m9sh-4{list-style-type:none}ul.lst-kix_eb6ic6z2m9sh-3{list-style-type:none}ul.lst-kix_jr91aycyc992-3{list-style-type:none}.lst-kix_ixeboairdkos-1>li:before{content:"\0025cb  "}ul.lst-kix_jr91aycyc992-2{list-style-type:none}ul.lst-kix_jr91aycyc992-1{list-style-type:none}.lst-kix_ixeboairdkos-0>li:before{content:"\0025cf  "}ul.lst-kix_eb6ic6z2m9sh-0{list-style-type:none}ul.lst-kix_jr91aycyc992-0{list-style-type:none}.lst-kix_eddvhvlfcek7-8>li:before{content:"\0025a0  "}.lst-kix_5siicw1q5qtw-1>li:before{content:"\0025cb  "}.lst-kix_5siicw1q5qtw-2>li:before{content:"\0025a0  "}.lst-kix_eddvhvlfcek7-5>li:before{content:"\0025a0  "}.lst-kix_5siicw1q5qtw-5>li:before{content:"\0025a0  "}.lst-kix_eddvhvlfcek7-4>li:before{content:"\0025cb  "}.lst-kix_eddvhvlfcek7-1>li:before{content:"\0025cb  "}.lst-kix_eddvhvlfcek7-0>li:before{content:"\0025cf  "}.lst-kix_btugr32bfvpf-2>li:before{content:"\0025a0  "}.lst-kix_btugr32bfvpf-6>li:before{content:"\0025cf  "}.lst-kix_m5ey97dhxg5-6>li:before{content:"\0025cf  "}.lst-kix_jo7lvhxrgmeg-2>li:before{content:"\0025a0  "}.lst-kix_m5ey97dhxg5-2>li:before{content:"\0025a0  "}.lst-kix_jo7lvhxrgmeg-6>li:before{content:"\0025cf  "}.lst-kix_f176rxr8jwn6-7>li:before{content:"\0025cb  "}.lst-kix_ujrw2gdovb97-5>li:before{content:"\0025a0  "}.lst-kix_48ib1dz7zp04-8>li:before{content:"\0025a0  "}.lst-kix_42xno5nwptez-2>li:before{content:"\0025a0  "}.lst-kix_34siv8i9m0f-0>li:before{content:"\0025cf  "}.lst-kix_34siv8i9m0f-4>li:before{content:"\0025cb  "}.lst-kix_42xno5nwptez-6>li:before{content:"\0025cf  "}.lst-kix_on37z5v164d5-6>li:before{content:"\0025cf  "}.lst-kix_34siv8i9m0f-8>li:before{content:"\0025a0  "}.lst-kix_on37z5v164d5-2>li:before{content:"\0025a0  "}ul.lst-kix_jr91aycyc992-7{list-style-type:none}ul.lst-kix_jr91aycyc992-6{list-style-type:none}ul.lst-kix_jr91aycyc992-5{list-style-type:none}ul.lst-kix_jr91aycyc992-4{list-style-type:none}ul.lst-kix_jr91aycyc992-8{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-3{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-4{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-5{list-style-type:none}.lst-kix_b06p333wtkc4-2>li:before{content:"\0025a0  "}.lst-kix_7mgbb75pci5l-3>li:before{content:"\0025cf  "}ul.lst-kix_jo7lvhxrgmeg-6{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-7{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-8{list-style-type:none}.lst-kix_b06p333wtkc4-6>li:before{content:"\0025cf  "}ul.lst-kix_jo7lvhxrgmeg-0{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-1{list-style-type:none}ul.lst-kix_jo7lvhxrgmeg-2{list-style-type:none}.lst-kix_rr55haih9k11-7>li:before{content:"\0025cb  "}.lst-kix_yc6oucdrd8k8-0>li:before{content:"\0025cf  "}.lst-kix_yc6oucdrd8k8-4>li:before{content:"\0025cb  "}.lst-kix_k10nxdwmave6-3>li:before{content:"\0025cf  "}.lst-kix_f176rxr8jwn6-3>li:before{content:"\0025cf  "}.lst-kix_7mgbb75pci5l-7>li:before{content:"\0025cb  "}.lst-kix_rr55haih9k11-3>li:before{content:"\0025cf  "}.lst-kix_mwssa4aj5yrr-4>li:before{content:"\0025cb  "}.lst-kix_sezodd14tbn8-3>li:before{content:"\0025cf  "}.lst-kix_sezodd14tbn8-6>li:before{content:"\0025cf  "}.lst-kix_1qx006ze2ww-6>li:before{content:"\0025cf  "}.lst-kix_1qx006ze2ww-8>li:before{content:"\0025a0  "}.lst-kix_sezodd14tbn8-5>li:before{content:"\0025a0  "}.lst-kix_1qx006ze2ww-1>li:before{content:"\0025cb  "}.lst-kix_1qx006ze2ww-3>li:before{content:"\0025cf  "}.lst-kix_sezodd14tbn8-0>li:before{content:"\0025cf  "}ul.lst-kix_42xno5nwptez-6{list-style-type:none}ul.lst-kix_42xno5nwptez-5{list-style-type:none}ul.lst-kix_42xno5nwptez-8{list-style-type:none}ul.lst-kix_42xno5nwptez-7{list-style-type:none}.lst-kix_kipghvp0ii5t-0>li:before{content:"\0025cf  "}.lst-kix_xlk5poyb1w7k-7>li:before{content:"\0025cb  "}.lst-kix_xlk5poyb1w7k-1>li:before{content:"\0025cb  "}.lst-kix_xlk5poyb1w7k-2>li:before{content:"\0025a0  "}.lst-kix_xlk5poyb1w7k-4>li:before{content:"\0025cb  "}.lst-kix_sezodd14tbn8-8>li:before{content:"\0025a0  "}.lst-kix_3rnw7d266aai-5>li:before{content:"\0025a0  "}.lst-kix_kipghvp0ii5t-8>li:before{content:"\0025a0  "}.lst-kix_3rnw7d266aai-8>li:before{content:"\0025a0  "}.lst-kix_kipghvp0ii5t-3>li:before{content:"\0025cf  "}.lst-kix_kipghvp0ii5t-2>li:before{content:"\0025a0  "}.lst-kix_kipghvp0ii5t-5>li:before{content:"\0025a0  "}ul.lst-kix_sezodd14tbn8-7{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-0{list-style-type:none}ul.lst-kix_sezodd14tbn8-8{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-7{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-8{list-style-type:none}ul.lst-kix_sezodd14tbn8-0{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-5{list-style-type:none}ul.lst-kix_sezodd14tbn8-1{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-6{list-style-type:none}ul.lst-kix_sezodd14tbn8-2{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-3{list-style-type:none}ul.lst-kix_sezodd14tbn8-3{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-4{list-style-type:none}ul.lst-kix_sezodd14tbn8-4{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-1{list-style-type:none}ul.lst-kix_sezodd14tbn8-5{list-style-type:none}ul.lst-kix_3vkdlyb1qfl7-2{list-style-type:none}ul.lst-kix_sezodd14tbn8-6{list-style-type:none}.lst-kix_btugr32bfvpf-7>li:before{content:"\0025cb  "}ul.lst-kix_dffublbbtuo-6{list-style-type:none}.lst-kix_btugr32bfvpf-1>li:before{content:"\0025cb  "}ul.lst-kix_dffublbbtuo-7{list-style-type:none}ul.lst-kix_dffublbbtuo-8{list-style-type:none}.lst-kix_jo7lvhxrgmeg-3>li:before{content:"\0025cf  "}.lst-kix_jo7lvhxrgmeg-5>li:before{content:"\0025a0  "}.lst-kix_f176rxr8jwn6-6>li:before{content:"\0025cf  "}ul.lst-kix_ko5ielnjz2sf-6{list-style-type:none}.lst-kix_34siv8i9m0f-1>li:before{content:"\0025cb  "}ul.lst-kix_ko5ielnjz2sf-5{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-4{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-3{list-style-type:none}ul.lst-kix_48ib1dz7zp04-0{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-8{list-style-type:none}ul.lst-kix_48ib1dz7zp04-1{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-7{list-style-type:none}ul.lst-kix_48ib1dz7zp04-2{list-style-type:none}.lst-kix_5gjpir8u93bi-0>li:before{content:"\0025cf  "}.lst-kix_5gjpir8u93bi-8>li:before{content:"\0025a0  "}ul.lst-kix_rr55haih9k11-2{list-style-type:none}ul.lst-kix_48ib1dz7zp04-3{list-style-type:none}ul.lst-kix_rr55haih9k11-1{list-style-type:none}ul.lst-kix_48ib1dz7zp04-4{list-style-type:none}ul.lst-kix_rr55haih9k11-0{list-style-type:none}ul.lst-kix_48ib1dz7zp04-5{list-style-type:none}ul.lst-kix_48ib1dz7zp04-6{list-style-type:none}ul.lst-kix_rr55haih9k11-6{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-2{list-style-type:none}.lst-kix_34siv8i9m0f-3>li:before{content:"\0025cf  "}ul.lst-kix_48ib1dz7zp04-7{list-style-type:none}ul.lst-kix_rr55haih9k11-5{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-1{list-style-type:none}ul.lst-kix_48ib1dz7zp04-8{list-style-type:none}ul.lst-kix_rr55haih9k11-4{list-style-type:none}ul.lst-kix_ko5ielnjz2sf-0{list-style-type:none}ul.lst-kix_rr55haih9k11-3{list-style-type:none}ul.lst-kix_7mgbb75pci5l-3{list-style-type:none}ul.lst-kix_7mgbb75pci5l-4{list-style-type:none}ul.lst-kix_7mgbb75pci5l-5{list-style-type:none}ul.lst-kix_rr55haih9k11-8{list-style-type:none}ul.lst-kix_7mgbb75pci5l-6{list-style-type:none}ul.lst-kix_rr55haih9k11-7{list-style-type:none}ul.lst-kix_7mgbb75pci5l-7{list-style-type:none}ul.lst-kix_7mgbb75pci5l-8{list-style-type:none}.lst-kix_5gjpir8u93bi-2>li:before{content:"\0025a0  "}.lst-kix_3rnw7d266aai-2>li:before{content:"\0025a0  "}.lst-kix_wdeqgbjjk4l4-4>li:before{content:"\0025cb  "}ul.lst-kix_7mgbb75pci5l-0{list-style-type:none}ul.lst-kix_7mgbb75pci5l-1{list-style-type:none}.lst-kix_b06p333wtkc4-1>li:before{content:"\0025cb  "}ul.lst-kix_7mgbb75pci5l-2{list-style-type:none}.lst-kix_3rnw7d266aai-0>li:before{content:"\0025cf  "}.lst-kix_b06p333wtkc4-7>li:before{content:"\0025cb  "}.lst-kix_wdeqgbjjk4l4-2>li:before{content:"\0025a0  "}.lst-kix_f176rxr8jwn6-4>li:before{content:"\0025cb  "}.lst-kix_1qx006ze2ww-0>li:before{content:"\0025cf  "}.lst-kix_k10nxdwmave6-2>li:before{content:"\0025a0  "}.lst-kix_k10nxdwmave6-0>li:before{content:"\0025cf  "}.lst-kix_mwssa4aj5yrr-3>li:before{content:"\0025cf  "}.lst-kix_ixeboairdkos-3>li:before{content:"\0025cf  "}.lst-kix_ixeboairdkos-6>li:before{content:"\0025cf  "}.lst-kix_mwssa4aj5yrr-6>li:before{content:"\0025cf  "}.lst-kix_owldsdw60k96-1>li:before{content:"\0025cb  "}.lst-kix_dffublbbtuo-5>li:before{content:"\0025a0  "}.lst-kix_iu8p9gst7pod-7>li:before{content:"\0025cb  "}.lst-kix_iu8p9gst7pod-4>li:before{content:"\0025cb  "}.lst-kix_yc6oucdrd8k8-5>li:before{content:"\0025a0  "}.lst-kix_hhjcx5eamba3-1>li:before{content:"\0025cb  "}.lst-kix_k10nxdwmave6-5>li:before{content:"\0025a0  "}.lst-kix_5siicw1q5qtw-8>li:before{content:"\0025a0  "}.lst-kix_tw47x1th0rzd-6>li:before{content:"\0025cf  "}.lst-kix_hhjcx5eamba3-4>li:before{content:"\0025cb  "}.lst-kix_k10nxdwmave6-8>li:before{content:"\0025a0  "}.lst-kix_vm44gmi40r2u-1>li:before{content:"\0025cb  "}.lst-kix_pl51pjh7ltk8-4>li:before{content:"\0025cb  "}.lst-kix_tw47x1th0rzd-3>li:before{content:"\0025cf  "}.lst-kix_owldsdw60k96-6>li:before{content:"\0025cf  "}ul.lst-kix_f176rxr8jwn6-5{list-style-type:none}.lst-kix_eb6ic6z2m9sh-2>li:before{content:"\0025a0  "}.lst-kix_pl51pjh7ltk8-7>li:before{content:"\0025cb  "}ul.lst-kix_f176rxr8jwn6-4{list-style-type:none}ul.lst-kix_f176rxr8jwn6-3{list-style-type:none}ul.lst-kix_f176rxr8jwn6-2{list-style-type:none}ul.lst-kix_f176rxr8jwn6-1{list-style-type:none}ul.lst-kix_f176rxr8jwn6-0{list-style-type:none}.lst-kix_eddvhvlfcek7-6>li:before{content:"\0025cf  "}.lst-kix_5siicw1q5qtw-0>li:before{content:"\0025cf  "}ul.lst-kix_f176rxr8jwn6-8{list-style-type:none}ul.lst-kix_f176rxr8jwn6-7{list-style-type:none}ul.lst-kix_f176rxr8jwn6-6{list-style-type:none}.lst-kix_eddvhvlfcek7-3>li:before{content:"\0025cf  "}.lst-kix_5siicw1q5qtw-3>li:before{content:"\0025cf  "}.lst-kix_eb6ic6z2m9sh-5>li:before{content:"\0025a0  "}.lst-kix_vm44gmi40r2u-6>li:before{content:"\0025cf  "}.lst-kix_dffublbbtuo-2>li:before{content:"\0025a0  "}.lst-kix_btugr32bfvpf-4>li:before{content:"\0025cb  "}.lst-kix_8ty6am9dc4ch-8>li:before{content:"\0025a0  "}ul.lst-kix_5siicw1q5qtw-0{list-style-type:none}ul.lst-kix_5siicw1q5qtw-1{list-style-type:none}ul.lst-kix_5siicw1q5qtw-2{list-style-type:none}ul.lst-kix_5siicw1q5qtw-3{list-style-type:none}ul.lst-kix_5siicw1q5qtw-4{list-style-type:none}ul.lst-kix_5siicw1q5qtw-5{list-style-type:none}ul.lst-kix_5siicw1q5qtw-6{list-style-type:none}ul.lst-kix_5siicw1q5qtw-7{list-style-type:none}ul.lst-kix_5siicw1q5qtw-8{list-style-type:none}.lst-kix_m5ey97dhxg5-4>li:before{content:"\0025cb  "}.lst-kix_vuu595jt9mz7-7>li:before{content:"\0025cb  "}.lst-kix_5gjpir8u93bi-5>li:before{content:"\0025a0  "}.lst-kix_42xno5nwptez-4>li:before{content:"\0025cb  "}.lst-kix_jo7lvhxrgmeg-8>li:before{content:"\0025a0  "}.lst-kix_34siv8i9m0f-6>li:before{content:"\0025cf  "}.lst-kix_otwzvwudtuep-2>li:before{content:"\0025a0  "}.lst-kix_wdeqgbjjk4l4-7>li:before{content:"\0025cb  "}.lst-kix_3vkdlyb1qfl7-2>li:before{content:"\0025a0  "}.lst-kix_on37z5v164d5-8>li:before{content:"\0025a0  "}.lst-kix_jr91aycyc992-3>li:before{content:"\0025cf  "}.lst-kix_7mgbb75pci5l-1>li:before{content:"\0025cb  "}.lst-kix_b06p333wtkc4-4>li:before{content:"\0025cb  "}.lst-kix_yc6oucdrd8k8-2>li:before{content:"\0025a0  "}.lst-kix_hze350db386w-1>li:before{content:"\0025cb  "}.lst-kix_8ty6am9dc4ch-0>li:before{content:"\0025cf  "}ul.lst-kix_b06p333wtkc4-5{list-style-type:none}ul.lst-kix_b06p333wtkc4-6{list-style-type:none}.lst-kix_f176rxr8jwn6-1>li:before{content:"\0025cb  "}ul.lst-kix_b06p333wtkc4-7{list-style-type:none}ul.lst-kix_b06p333wtkc4-8{list-style-type:none}.lst-kix_rr55haih9k11-1>li:before{content:"\0025cb  "}.lst-kix_jo7lvhxrgmeg-0>li:before{content:"\0025cf  "}.lst-kix_i64ijt69btn4-1>li:before{content:"\0025cb  "}ul.lst-kix_b06p333wtkc4-0{list-style-type:none}ul.lst-kix_b06p333wtkc4-1{list-style-type:none}ul.lst-kix_b06p333wtkc4-2{list-style-type:none}ul.lst-kix_b06p333wtkc4-3{list-style-type:none}ul.lst-kix_b06p333wtkc4-4{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c5{margin-left:72pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c29{color:#454545;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c57{padding-top:18pt;padding-bottom:2pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c17{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c0{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c68{padding-top:12pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c38{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c19{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c23{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c21{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c14{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c20{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c10{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c36{padding-top:9pt;padding-bottom:9pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c26{padding-top:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c62{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c72{padding-top:0pt;padding-bottom:3pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c47{padding-top:12pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c59{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c40{color:#9900ff;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c13{text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c22{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c51{vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c43{vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c9{font-size:12pt;font-family:"Times New Roman";font-weight:400}.c46{text-decoration:none;vertical-align:baseline;font-style:normal}.c66{vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c25{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c69{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c65{color:#333333;font-size:11pt;font-family:"Courier New"}.c7{font-size:13.5pt;font-family:"Times New Roman";font-weight:400}.c4{font-size:12.5pt;font-family:"Times New Roman";font-weight:400}.c8{font-size:13.5pt;font-family:"Times New Roman";color:#ff0000}.c37{color:#454545;font-size:9pt}.c35{margin-left:144pt;padding-left:0pt}.c30{color:#666666;font-weight:400}.c2{padding:0;margin:0}.c11{color:inherit;text-decoration:inherit}.c15{margin-left:36pt;padding-left:0pt}.c60{font-weight:400;font-style:normal}.c64{font-size:12pt;font-family:"Times New Roman"}.c50{font-size:12.5pt;font-family:"Times New Roman"}.c27{margin-left:108pt;padding-left:0pt}.c39{margin-left:180pt;padding-left:0pt}.c58{margin-left:216pt;padding-left:0pt}.c56{width:33%;height:1px}.c18{background-color:#ffff00}.c3{font-weight:700}.c53{font-size:13.5pt}.c12{color:#ff0000}.c52{margin-left:36pt}.c70{page-break-after:avoid}.c48{background-color:#00ff00}.c41{color:#000000}.c42{color:#9900ff}.c44{margin-left:72pt}.c75{color:#0000ee}.c73{font-size:12pt}.c71{color:#00ff00}.c45{color:#454545}.c67{font-size:16pt}.c33{font-style:italic}.c49{text-decoration:none}.c28{font-size:10pt}.c63{font-size:8pt}.c54{color:#ff00ff}.c32{color:#0000ff}.c55{font-size:9pt}.c74{font-family:"Times New Roman"}.c61{font-weight:400}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c69"><p class="c70 c72 title" id="h.qt7cq82423cu"><span class="c23">CV_Leibe</span></p><h1 class="c38" id="h.cdbxlq3iy73v"><span class="c24">lec1</span></h1><hr><p class="c6"><span class="c1"></span></p><p class="c10"><span class="c43 c25 c3 c45">&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</span></p><p class="c6"><span class="c29"></span></p><h2 class="c57" id="h.omtrrsce18dp"><span>E: </span><span class="c34">Pinhole Camera:</span></h2><p class="c10"><span class="c43 c3 c45 c49">__________________________________________________________________________________________</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c22 c3 c55"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Pinhole_camera&amp;sa=D&amp;source=editors&amp;ust=1642006108517896&amp;usg=AOvVaw3Z0IB-7Np-7KP9qG1Ko57P">https://en.wikipedia.org/wiki/Pinhole_camera</a></span></p><p class="c6"><span class="c29"></span></p><h3 class="c0" id="h.t80p92nr5jsl"><span class="c21">Pinhole Size:</span></h3><p class="c10"><span class="c37">Up to a certain point, the smaller the hole, the sharper the image, but </span><span class="c37 c3">the dimmer the projected image</span><span class="c29">. </span></p><p class="c10"><span class="c29">Optimally, the size of the aperture should be 1/100 or less of the distance between it and the projected image. </span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37">Within limits, a </span><span class="c37 c3">smaller pinhole</span><span class="c29">&nbsp;(with a thinner surface that the hole goes through) will result in a </span></p><p class="c10"><span class="c37 c3">sharper image resolution</span><span class="c37">&nbsp;because the </span><span class="c43 c3 c45 c49">projected circle of confusion at the image plane is practically the </span></p><p class="c10"><span class="c43 c3 c45 c49">same size as the pinhole. (Genauer: s. Bild CoC Effekt #1)</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37">An </span><span class="c37 c3">extremely small hole</span><span class="c29">, however, can produce significant diffraction effects and a less clear image due to </span></p><p class="c10"><span class="c29">the wave properties of light.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">Additionally, vignetting (Abdunklung an den R&auml;ndern) occurs as the diameter of the hole approaches the </span></p><p class="c10"><span class="c29">thickness of the material in which it is punched, because the sides of the hole obstruct the light entering </span></p><p class="c10"><span class="c29">at anything other than 90 degrees.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37">The </span><span class="c37 c3">best pinhole</span><span class="c37">&nbsp;is </span><span class="c37 c3">perfectly round</span><span class="c29">&nbsp;(since irregularities cause higher-order diffraction effects) and in an </span></p><p class="c10"><span class="c37">extremely </span><span class="c37 c3">thin</span><span class="c29">&nbsp;piece of material. Industrially produced pinholes benefit from laser etching, but a hobbyist </span></p><p class="c10"><span class="c29">can still produce pinholes of sufficiently high quality for photographic work.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c3 c45 c49">__________________________________________________________________________________________</span></p><p class="c6"><span class="c43 c3 c45 c49"></span></p><h3 class="c0" id="h.2312rcn0yovm"><span class="c21">Circle of Confusion for Pinhole Camera:</span></h3><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37 c3">Achtung:</span><span class="c29">&nbsp;Es gibt 2 verschiedene CoC Effekte !</span></p><p class="c6"><span class="c29"></span></p><ul class="c2 lst-kix_ko5ielnjz2sf-0 start"><li class="c15 c36 li-bullet-0"><span class="c25 c3 c45 c67">CoC Effekt #1: without lens</span></li></ul><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c22 c3 c55"><a class="c11" href="https://www.google.com/url?q=https://www.quora.com/What-happens-to-the-sharpness-of-a-pinhole-camera-when-the-length-is-increased&amp;sa=D&amp;source=editors&amp;ust=1642006108522685&amp;usg=AOvVaw01uQ8LHOV0F1m5PAOliuCH">https://www.quora.com/What-happens-to-the-sharpness-of-a-pinhole-camera-when-the-length-is-increased</a></span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 418.50px; height: 314.09px;"><img alt="" src="images/image86.png" style="width: 418.50px; height: 314.09px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">Pinhole cameras are always sharp at any length (gemeint ist: object to camera length). The problem is </span></p><p class="c10"><span class="c29">that pinhole cameras have circles of confusion, which is the image is being projected from every point </span></p><p class="c10"><span class="c29">light is passing through the aperture to every opposite point.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c25 c3 c37">The larger the aperture</span><span class="c37">, the more photons can come through, that means </span><span class="c25 c37 c3">the brighter the image</span><span class="c29">&nbsp;but </span></p><p class="c10"><span class="c25 c37 c3">the less &ldquo;sharp&rdquo;</span><span class="c37">&nbsp;the image is </span><span class="c25 c37 c3">due to overlapping images</span><span class="c29">. (dh larger aperture -&gt; CoC w&auml;chst schneller -&gt; smaller DoF (cf lecture))</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">The only way to get rid of circles of confusion is to use a lens which focuses light to a single point. </span></p><p class="c10"><span class="c29">At which point there are not overlapping images.</span></p><p class="c6"><span class="c29"></span></p><ul class="c2 lst-kix_rzjrajl6hx6o-0 start"><li class="c36 c15 li-bullet-0"><span class="c25 c3 c45 c67">CoC Effekt #2: with lens (sensor out of area of AF)</span></li></ul><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c22 c3 c55"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Zerstreuungskreis&amp;sa=D&amp;source=editors&amp;ust=1642006108525618&amp;usg=AOvVaw0KZY-Mrf9SKP5_65PAkVR7">https://de.wikipedia.org/wiki/Zerstreuungskreis</a></span></p><p class="c6"><span class="c29"></span></p><p class="c6"><span class="c29"></span></p><p class="c6"><span class="c29"></span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 508.35px; height: 212.80px;"><img alt="" src="images/image39.png" style="width: 508.35px; height: 212.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10"><span class="c22 c55"><a class="c11" href="https://www.google.com/url?q=https://shotkit.com/circle-of-confusion/&amp;sa=D&amp;source=editors&amp;ust=1642006108526696&amp;usg=AOvVaw0UInEoQZ1LhhRS2WFXu9eY">https://shotkit.com/circle-of-confusion/</a></span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 264.00px;"><img alt="" src="images/image23.png" style="width: 601.70px; height: 264.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37">It&rsquo;s also the case that an </span><span class="c37 c3">area in front and behind the focal plane</span><span class="c37">&nbsp;extends the area of the subject that </span><span class="c37 c3">we perceive as being in focus</span><span class="c37">. In the diagram, this is shown </span><span class="c37 c3">in yellow</span><span class="c29">.</span></p><p class="c10"><span class="c29">Looking side-on, you see that the yellow areas are different for a large aperture (hellblauer Kreis f-2.0) and for a small aperture (hellblauer Kreis f-16).</span></p><p class="c10"><span class="c37">All the </span><span class="c37 c3">areas in red and yellow are seen by the human eye as being in focus</span><span class="c29">, even though they aren&rsquo;t perfectly so. </span></p><p class="c10"><span class="c37">This is what photographers call the &ldquo;</span><span class="c25 c37 c3">Acceptable Focus</span><span class="c37">&rdquo; or the &ldquo;</span><span class="c25 c37 c3">Permissible Circle of Confusion</span><span class="c29">&rdquo;.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c3 c45 c49">In simple terms, this is the area of an image that appears to be in focus.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c3 c45 c49">__________________________________________________________________________________________</span></p><p class="c6"><span class="c29"></span></p><h3 class="c0" id="h.1sdd9thaaup8"><span class="c21">Depth of field</span></h3><p class="c6"><span class="c29"></span></p><ul class="c2 lst-kix_tw47x1th0rzd-0 start"><li class="c36 c15 li-bullet-0"><span class="c29">lecture: large aperture -&gt; small DoF (-&gt; background not visible/blurry)</span></li><li class="c36 c15 li-bullet-0"><span class="c29">this is why many photographers use additional light sources -&gt; allows to use smaller apertures (larger DoF -&gt; more focussed image)</span></li></ul><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c3 c45 c49">------------------------------------------------------------------------------------</span></p><p class="c10"><span class="c25 c3 c45 c67">Definition: DoF</span><span class="c43 c25 c3 c45">&nbsp;</span></p><p class="c10"><span class="c22 c3 c55"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Depth_of_field&amp;sa=D&amp;source=editors&amp;ust=1642006108534375&amp;usg=AOvVaw3jJp1pX1xp_cZTZmhqnadD">https://en.wikipedia.org/wiki/Depth_of_field</a></span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37 c3">depth of field</span><span class="c37">&nbsp;(</span><span class="c37 c3">DOF</span><span class="c37">) is the distance between the nearest and the farthest objects that are in acceptably sharp focus in an image. (area </span><span class="c25 c37 c3">au&szlig;erhalb DoF</span><span class="c29">&nbsp;wichtig f&uuml;r Bokeh s.u.)</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">The depth of field can be calculated based on focal length, distance to subject, the acceptable circle of confusion size, and aperture.</span></p><p class="c10"><span class="c43 c3 c45 c49">------------------------------------------------------------------------------------</span></p><p class="c6 c70 subtitle" id="h.njfrq3i3ieip"><span class="c43 c25 c3 c45"></span></p><p class="c10"><span class="c25 c3 c45 c67">DoF for Pinhole Camera: </span></p><p class="c10"><span class="c29">(wieder von https://en.wikipedia.org/wiki/Pinhole_camera)</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">The depth of field is basically infinite, but this does not mean that no optical blurring occurs. The infinite depth </span></p><p class="c10"><span class="c29">of field means that image blur depends not on object distance but on other factors, such as the distance from </span></p><p class="c10"><span class="c29">the aperture to the film plane, the aperture size, the wavelength(s) of the light source, and motion of the subject </span></p><p class="c10"><span class="c29">or canvas. Additionally, pinhole photography can not avoid the effects of haze (=atmospheric phenomenon in which </span></p><p class="c10"><span class="c29">dust, smoke, and other dry particulates obscure the clarity of the sky). </span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c3 c45 c49">__________________________________________________________________________________________</span></p><p class="c6"><span class="c29"></span></p><h2 class="c20" id="h.sj5o54evpaef"><span>E: </span><span class="c34">Bokeh:</span></h2><p class="c10"><span class="c29">(wiki)</span></p><p class="c6"><span class="c29"></span></p><ul class="c2 lst-kix_wdeqgbjjk4l4-0 start"><li class="c36 c15 li-bullet-0"><span class="c29">is the aesthetic quality of the blur produced in out-of-focus parts of an image.</span></li><li class="c36 c15 li-bullet-0"><span class="c29">blur occurs in all regions of an image which are outside the depth of field.</span></li><li class="c36 c15 li-bullet-0"><span class="c29">Differences in lens aberrations and aperture shape cause very different bokeh effects.</span></li></ul><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37">Bokeh characteristics may be quantified by examining the image&#39;s circle of confusion. In out-of-focus areas, </span><span class="c43 c25 c3 c45">each </span></p><p class="c10"><span class="c25 c37 c3">point of light becomes an image of the aperture,</span><span class="c37">&nbsp;generally a more or less </span><span class="c25 c37 c3">round disc</span><span class="c29">. Depending on how a lens is </span></p><p class="c10"><span class="c29">corrected for spherical aberration, the disc may be uniformly illuminated, brighter near the edge, or brighter near the center.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">Lenses that are poorly corrected for spherical aberration will show one kind of disc for out-of-focus points in front </span></p><p class="c10"><span class="c29">of the plane of focus, and a different kind for points behind. This may actually be desirable, as blur circles that </span></p><p class="c10"><span class="c29">are dimmer near the edges produce less-defined shapes which blend smoothly with the surrounding image.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c29">The shape of the aperture has an influence on the subjective quality of bokeh as well. For conventional lens </span></p><p class="c10"><span class="c29">designs (with bladed apertures), when a lens is stopped down smaller than its maximum aperture size </span></p><p class="c10"><span class="c29">(minimum f-number), out-of-focus points are blurred into the polygonal shape formed by the aperture blades. </span></p><p class="c10"><span class="c29">This is most apparent when a lens produces hard-edged bokeh. For this reason, some lenses have many </span></p><p class="c10"><span class="c29">aperture blades and/or blades with curved edges to make the aperture more closely approximate a circle </span></p><p class="c10"><span class="c29">rather than a polygon.</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c25 c3 c45">&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</span></p><p class="c6"><span class="c29"></span></p><h2 class="c20" id="h.9ex1qf6riap0"><span>E: </span><span class="c34">Thin lens approximation:</span></h2><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c37">d &lt;&lt; R1 </span><span class="c37 c3">und</span><span class="c29">&nbsp;d &lt;&lt; R2 (d: Linsendicke, R: Radii der Linse)</span></p><p class="c6"><span class="c29"></span></p><p class="c10"><span class="c43 c25 c3 c45">&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;&mdash;</span></p><p class="c6"><span class="c29"></span></p><h2 class="c20" id="h.g5huch9gd6jm"><span>E: </span><span class="c34">Gaussian Filter</span></h2><p class="c6"><span class="c1"></span></p><p class="c10"><span class="c1">from scipy.ndimage import gaussian_filter</span></p><p class="c10"><span class="c1">a = np.arange(50, step=2).reshape((5,5))</span></p><p class="c10"><span class="c1">a</span></p><p class="c10"><span class="c13 c12 c61">array([[ 0, &nbsp;2, &nbsp;4, &nbsp;6, &nbsp;8],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[10, 12, 14, 16, 18],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[20, 22, 24, 26, 28],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[30, 32, 34, 36, 38],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[40, 42, 44, 46, 48]])</span></p><p class="c10"><span class="c1">gaussian_filter(a, sigma=1)</span></p><p class="c10"><span class="c13 c12 c61">array([[ 4, &nbsp;6, &nbsp;8, &nbsp;9, 11],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[10, 12, 14, 15, 17],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[20, 22, 24, 25, 27],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[29, 31, 33, 34, 36],</span></p><p class="c10"><span class="c13 c12 c61">&nbsp; &nbsp; &nbsp; &nbsp;[35, 37, 39, 40, 42]])</span></p><p class="c6"><span class="c46 c61 c65"></span></p><p class="c10"><span class="c1">from scipy import misc</span></p><p class="c10"><span class="c1">import matplotlib.pyplot as plt</span></p><p class="c10"><span class="c1">fig = plt.figure()</span></p><p class="c10"><span class="c1">plt.gray() &nbsp;# show the filtered result in grayscale</span></p><p class="c10"><span class="c1">ax1 = fig.add_subplot(121) &nbsp;# left side</span></p><p class="c10"><span class="c1">ax2 = fig.add_subplot(122) &nbsp;# right side</span></p><p class="c10"><span class="c1">ascent = misc.ascent()</span></p><p class="c10"><span class="c1">result = gaussian_filter(ascent, sigma=5)</span></p><p class="c10"><span class="c1">ax1.imshow(ascent)</span></p><p class="c10"><span class="c1">ax2.imshow(result)</span></p><p class="c10"><span class="c1">plt.show()</span></p><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 345.50px; height: 213.99px;"><img alt="" src="images/image37.png" style="width: 345.50px; height: 213.99px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><hr><p class="c6"><span class="c1"></span></p><h1 class="c38" id="h.x3k0xtd0uyez"><span class="c24">lec2</span></h1><hr><p class="c6"><span class="c1"></span></p><h2 class="c20" id="h.pmfwt7880jc2"><span class="c34">E: aliasing</span></h2><ul class="c2 lst-kix_yc6oucdrd8k8-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">aliasing occurs when your sampling rate is not high enough to capture the amount of detail in your image</span></li><li class="c10 c15 li-bullet-0"><span>can give you the wrong signal/image - an &ldquo;</span><span class="c41 c33 c49 c61 c66">alias&rdquo;</span></li></ul><p class="c59 c52"><span>FourierTrafo:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.50px; height: 240.64px;"><img alt="" src="images/image34.png" style="width: 320.50px; height: 240.64px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_yc6oucdrd8k8-0"><li class="c10 c15 li-bullet-0"><span>When the sampled magnitude spectra </span><span class="c25 c3">do not</span><span class="c1">&nbsp;overlap:</span></li></ul><p class="c59 c52"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 497.39px; height: 537.83px;"><img alt="" src="images/image84.png" style="width: 497.39px; height: 537.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_yc6oucdrd8k8-0"><li class="c10 c15 li-bullet-0"><span>When the sampled magnitude spectra </span><span class="c25 c3">do</span><span class="c1">&nbsp;overlap:</span></li></ul><p class="c59 c52"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 484.97px; height: 540.10px;"><img alt="" src="images/image33.png" style="width: 484.97px; height: 540.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_eb6ic6z2m9sh-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">Forsyth, Ponce: the Fourier transform of the sampled signal is the sum of a collection of shifted versions of the Fourier transforms of the signal, i.e.</span></li></ul><p class="c59 c52"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 475.50px; height: 185.62px;"><img alt="" src="images/image27.png" style="width: 475.50px; height: 185.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp;</span></p><p class="c10 c52"><span class="c1">wobei f(x,y) das sampled signal und die { }-brackets die bed-of-nails function ist, die zum samplen benutzt wird; ** ist die convolution operation und varF ist die Fouriertrafo operation.</span></p><hr><p class="c6"><span class="c1"></span></p><h2 class="c20" id="h.nw3ek5gjrljs"><span class="c34">Part 3.1: Multi-Scale Representations</span></h2><ul class="c2 lst-kix_hze350db386w-0 start"><li class="c10 c15 li-bullet-0"><span>in many CV tasks we need to perform a quick </span><span class="c25 c3">search across the space</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_hze350db386w-1 start"><li class="c5 li-bullet-0"><span class="c1">eg. object detection: </span></li></ul><ul class="c2 lst-kix_hze350db386w-2 start"><li class="c10 c27 li-bullet-0"><span>eg find occurrences of a particular house in an image with an object detector that looks for the house </span><span class="c25 c3">at a particular size</span><span class="c1">&nbsp;in the image</span></li></ul><ul class="c2 lst-kix_hze350db386w-3 start"><li class="c10 c35 li-bullet-0"><span class="c3">problem</span><span class="c1">: at the original image resolution the detector window is too small and only covers part of the house</span></li><li class="c10 c35 li-bullet-0"><span class="c3">sol</span><span class="c1">: consider multiple downscaled versions of the original image =&gt; at some lower resolution the object detector will match the size of the house in the image =&gt; house will be detected</span></li></ul><ul class="c2 lst-kix_hze350db386w-2"><li class="c10 c27 li-bullet-0"><span class="c3">General sol</span><span>: Build up a so-called </span><span class="c25 c3 c41 c51">image pyramid</span></li></ul><ul class="c2 lst-kix_hze350db386w-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">original image at its base resolution at the starting level</span></li><li class="c10 c35 li-bullet-0"><span class="c1">multiple downscaled versions on top&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></li><li class="c10 c35 li-bullet-0"><span class="c1">process all those versions in parallel</span></li></ul><ul class="c2 lst-kix_hze350db386w-4 start"><li class="c10 c39 li-bullet-0"><span class="c1">perform a range of CV tasks at different scales simultaneously</span></li></ul><ul class="c2 lst-kix_hze350db386w-3"><li class="c10 c35 li-bullet-0"><span class="c1">Note: this does not need much more computational resources and memory space, if we use a constant downscaling factor of 2 in both directions (=image at next level only &frac14; of the size of the original image =&gt; image pyramid contains not more than 1.5 times the number of pixels of the original image =&gt; image pyramid comes with very little overhead)</span></li></ul><ul class="c2 lst-kix_hze350db386w-0"><li class="c10 c15 li-bullet-0"><span>checkerboard: in the bottom two cases the </span><span class="c25 c3">resampling rate</span><span class="c1">&nbsp;is too low to preserve the checkerboard structure</span></li></ul><h1 class="c38" id="h.41cpghd5h5hi"><span class="c24">lec3</span></h1><hr><p class="c6"><span class="c1 c18"></span></p><h2 class="c20" id="h.xrb8ly6opv2i"><span>E: </span><span class="c34">3.3 image gradients</span></h2><ul class="c2 lst-kix_k10nxdwmave6-0 start"><li class="c10 c15 li-bullet-0"><span>&ldquo;1 -1&rdquo;-filter ausprobieren: </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://setosa.io/ev/image-kernels/&amp;sa=D&amp;source=editors&amp;ust=1642006108549419&amp;usg=AOvVaw3TVyTgJ_Ps4xCHEvugBuEo">https://setosa.io/ev/image-kernels/</a></span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-1 start"><li class="c5 li-bullet-0"><span class="c1">ganz unten auf der website ist ein 3x3 Filter in dem man selbst Werte eingeben kann mit dem live update Bild eines r&ouml;mischen Geb&auml;udes daneben: hier 7 Werte gleich 0 setzen und 2 nebeneinanderliegende Werte (egal wo) auf 1 -1 (oder -1 1)</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-0"><li class="c10 c15 li-bullet-0"><span>numpy.convolve </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://colab.research.google.com/drive/1WL0xTt5l5yEzyfZhBt3bSV6K7ZMlZ6H_&amp;sa=D&amp;source=editors&amp;ust=1642006108550294&amp;usg=AOvVaw3BPs0vEgx0_EXLgezJwKCG">https://colab.research.google.com/drive/1WL0xTt5l5yEzyfZhBt3bSV6K7ZMlZ6H_</a></span></li><li class="c10 c15 li-bullet-0"><span>versch. Filter: </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Kernel_(image_processing)&amp;sa=D&amp;source=editors&amp;ust=1642006108550723&amp;usg=AOvVaw02XXRm_A6mnxv1Aybgcg5K">https://en.wikipedia.org/wiki/Kernel_(image_processing)</a></span></li><li class="c10 c15 li-bullet-0"><span class="c1">1 -1 und -1 1 sind im Prinzip gleich, nur umgekehrte Farben (schwarz, wo wei&szlig; ist und vice versa)</span></li><li class="c10 c15 li-bullet-0"><span>warum 1 0 -1 besser als 1 -1 ? </span><span class="c9 c12">als Klausurfrage ungeeignet</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-1 start"><li class="c5 li-bullet-0"><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://www.cse.unr.edu/~bebis/CS791E/Notes/EdgeDetection.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108551562&amp;usg=AOvVaw1t9lsSAMwHWcd1SfW0LE4C">https://www.cse.unr.edu/~bebis/CS791E/Notes/EdgeDetection.pdf</a></span></li><li class="c5 li-bullet-0"><span class="c9">Mx is the approximation at (i,j</span><span class="c73">+</span><span class="c9">1/2) and My is the approximation at (i</span><span class="c73">+</span><span class="c9 c46 c41">1/2,j)</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-2 start"><li class="c10 c27 li-bullet-0"><span class="c9 c41 c46">dh. 1 -1 rechnet Gradienten &ldquo;zwischen betrachtetem Pixel und Nachbarpixel&rdquo; aus, statt (wie gewollt) &ldquo;genau auf&rdquo; dem betrachteten Pixel </span></li><li class="c10 c27 li-bullet-0"><span class="c9">-&gt; dadurch ist Ableitung (=gefiltertes Bild) &frac12; Pixel nach </span><span class="c64 c3">links</span><span class="c9 c46 c41">&nbsp;verschoben </span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-3 start"><li class="c10 c35 li-bullet-0"><span class="c9">denn der 1. berechnete Gradient (also der bei (0,0) im gefilterten Bild) ist </span><span class="c64 c3">tats&auml;chlich</span><span class="c9">&nbsp;&frac12; Pixel weiter </span><span class="c3 c64">rechts</span><span class="c9 c46 c41">, dh. bei Pixel (0,0+1/2) statt bei (0,0)</span></li><li class="c10 c35 li-bullet-0"><span class="c9 c46 c41">dh wir wollen den Gradienten auf index (i,j) und nicht auf (i,j+&frac12;)</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-4 start"><li class="c10 c39 li-bullet-0"><span class="c9 c46 c41">das geht nur mit 3x3 Filter, weil dieser ist immer auf einem Pixel zentriert (sowohl mit als auch ohne padding)</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-1"><li class="c5 li-bullet-0"><span class="c9 c46 c12">als Klausurfrage ungeeignet</span></li></ul><ul class="c2 lst-kix_k10nxdwmave6-0"><li class="c6 c15 li-bullet-0"><span class="c9 c46 c41"></span></li></ul><hr><p class="c6"><span class="c1"></span></p><h1 class="c38" id="h.b1sxfdfhblyq"><span class="c24">lec6 - Segmentation</span></h1><h2 class="c20" id="h.rb1zksuehz2f"><span class="c34">The Gestalt School</span></h2><ul class="c2 lst-kix_jo7lvhxrgmeg-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">Elements in a collection can have properties that result from relationships</span></li></ul><ul class="c2 lst-kix_jo7lvhxrgmeg-1 start"><li class="c5 li-bullet-0"><span class="c1">&ldquo;the whole is greater than the sum of its parts</span></li></ul><ul class="c2 lst-kix_jo7lvhxrgmeg-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">illusory/subjective contours (pacman triangle)</span></li><li class="c10 c27 li-bullet-0"><span class="c1">occlusion (ein Teil vom &ldquo;S&rdquo; verdeckt)</span></li><li class="c10 c27 li-bullet-0"><span class="c1">familiar configuration (morgenstern, Nessi snake)</span></li></ul><h3 class="c0" id="h.50wlx3gfz18m"><span class="c21">Gestalt Theory</span></h3><ul class="c2 lst-kix_pl51pjh7ltk8-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">Gestalt: whole or group ?</span></li></ul><ul class="c2 lst-kix_pl51pjh7ltk8-1 start"><li class="c5 li-bullet-0"><span class="c1">whole is greater than sum of its parts</span></li><li class="c5 li-bullet-0"><span class="c1">relationships among parts can yield new properties/features</span></li></ul><ul class="c2 lst-kix_pl51pjh7ltk8-0"><li class="c10 c15 li-bullet-0"><span class="c1">psychologists (psychophysicists) identified series of factors that predispose set of elements to be grouped (by human visual system) eg. Max Wertheimer </span></li></ul><h3 class="c0" id="h.gaohlecds0f2"><span class="c21">Gestalt factors</span></h3><ul class="c2 lst-kix_34siv8i9m0f-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">proximity</span></li><li class="c10 c15 li-bullet-0"><span class="c1">similarity (Farbe: zB schwarze Punkte und wei&szlig;e Punkte)</span></li><li class="c10 c15 li-bullet-0"><span class="c1">similarity (Ausrichtung: zB hochkant Ellipsen und liegende Ellipsen)</span></li><li class="c10 c15 li-bullet-0"><span class="c1">Common Fate (motion)</span></li><li class="c10 c15 li-bullet-0"><span class="c1">Common Region</span></li><li class="c10 c15 li-bullet-0"><span class="c1">parallelism</span></li><li class="c10 c15 li-bullet-0"><span class="c1">symmetry</span></li><li class="c10 c15 li-bullet-0"><span class="c1">continuity</span></li><li class="c10 c15 li-bullet-0"><span class="c1">closure</span></li><li class="c10 c15 li-bullet-0"><span class="c3 c12 c13">Gestalt factors are difficult to translate into algorithms</span></li></ul><h2 class="c20" id="h.79kinixq65vk"><span class="c34">Continuity through Occlusion Cues</span></h2><ul class="c2 lst-kix_34siv8i9m0f-0"><li class="c10 c15 li-bullet-0"><span class="c1">continuity, explanation by occlusion (W&uuml;rfel verdeckt mit 3 Stangen, f&uuml;nf verdeckte &ldquo;9&rdquo;en, Hund)</span></li></ul><h2 class="c20" id="h.9h6r1ez9w4yg"><span class="c34">Summary</span></h2><ul class="c2 lst-kix_34siv8i9m0f-0"><li class="c10 c15 li-bullet-0"><span class="c1">when we look at images, there is a lot of grouping going on in our human visual system. Those are not just based on local measurements/not just based on individual image pixel colors. Instead they are based on larger scale grouping effects, based on recognition of known structure, based on finding visual explanations for e.g. occlusion effects. Those all have influence on what we finally perceive in the scene.</span></li></ul><h2 class="c20" id="h.mcccqwjhnmn9"><span class="c34">Image Segmentation</span></h2><p class="c10"><span class="c3 c12">Goal</span><span class="c1">: identify groups of pixels that go together</span></p><h1 class="c38" id="h.eal8sf5xugnw"><span class="c24">lec7 - Segmentation as Energy Minimization</span></h1><p class="c10"><span>Sachen, die schwer zu merken sind:</span></p><ul class="c2 lst-kix_34siv8i9m0f-0"><li class="c10 c15 li-bullet-0"><span class="c3 c12">ACHTUNG typo</span><span class="c13 c3 c41">:</span></li></ul><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 128.00px;"><img alt="" src="images/image54.png" style="width: 601.70px; height: 128.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_34siv8i9m0f-0"><li class="c10 c15 li-bullet-0"><span class="c1">the energy function is submodular, iff ...</span></li><li class="c10 c15 li-bullet-0"><span class="c1">Leibe: &ldquo;Briefly stated, this [&ldquo;submodularity&rdquo;] constraint has the effect that we prefer labeling pixels to be in the same class rather than in different classes.&rdquo;</span></li></ul><ul class="c2 lst-kix_34siv8i9m0f-1 start"><li class="c5 li-bullet-0"><span>Energiefunktion f&uuml;r 2 Pixel i und j E(Label_i, Label_j) hat die Eigenschaft, dass die Energie (=cost) </span><span class="c25 c3">niedriger</span><span>&nbsp;ist, wenn beide Pixel das </span><span class="c25 c3">gleiche</span><span>&nbsp;Label haben, ie Label_i = label_j, statt unterschiedliche Label. Da wir E minimieren wollen (wie Physik), ist </span><span class="c25 c3">letzteres &auml;quivalent dazu, dass wir gleiche Label f&uuml;r die Pixel i und j bei der Segmentation (=Labelzuordnung) pr&auml;ferieren</span><span>. </span><span class="c13 c3 c12">Als Klausurfrage ungeeignet ! (lern erstmal den Rest/die Basics !)</span></li></ul><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 392.00px;"><img alt="" src="images/image68.png" style="width: 601.70px; height: 392.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c38" id="h.x4wsknbv7lq6"><span class="c24">lec8 - Object detection</span></h1><h2 class="c20" id="h.l8q1fjhnz7mg"><span class="c34">GrabCut</span></h2><ul class="c2 lst-kix_owldsdw60k96-0 start"><li class="c10 c15 li-bullet-0"><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108560505&amp;usg=AOvVaw1g-X7wR1-kFcglyEr7J8F6">https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf</a></span><span>&nbsp;</span></li><li class="c10 c15 li-bullet-0"><span class="c1">Leibe: &ldquo;by getting better or more compact color models for the different classes we are also able to reduce the energy, so this basically shows you this &ldquo;meta-optimization&rdquo; by iterating also on the foreground and background color models&rdquo;</span></li></ul><p class="c59"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 204.50px; height: 187.46px;"><img alt="" src="images/image76.png" style="width: 204.50px; height: 187.46px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_42xno5nwptez-0 start"><li class="c10 c15 li-bullet-0"><span class="c3">GMM f&uuml;r foreground </span><span>mit 5 Komponenten (s. 5 rote Ellipsen; sind egtl Ellipsoidoberfl&auml;chen im RGB Raum, die Fl&auml;chen bzw Konturen gleicher Ws darstellen, die auf RG Raum projiziert sind) und </span><span class="c3">GMM f&uuml;r background</span><span class="c1">&nbsp;mit weiteren 5 Komponenten (s. blaue Ellipsen) &auml;ndern sich nach jeder Iteration. Dieser Vorgang ist sozusagen eine &ldquo;Meta-optimization&rdquo; des color model, neben der egtl optimization der energy function.</span></li></ul><p class="c59"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 170.50px; height: 158.57px;"><img alt="" src="images/image44.png" style="width: 170.50px; height: 158.57px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c59"><span>im </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://cvg.ethz.ch/teaching/cvl/2012/grabcut-siggraph04.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108561899&amp;usg=AOvVaw0BmnMprGdCPw2F2NBUra7h">Original-Paper</a></span><span class="c1">:</span></p><p class="c59"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 446.00px; height: 298.00px;"><img alt="" src="images/image48.png" style="width: 446.00px; height: 298.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_eddvhvlfcek7-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">Can we draw any parallels of MRF or CRF to context in semantic segmentation?</span></li></ul><ul class="c2 lst-kix_eddvhvlfcek7-1 start"><li class="c5 li-bullet-0"><span class="c1">Leibe: &ldquo;The answer is yes, when you look at what we had before, we can set up per pixel classifiers, this corresponds to the unary potentials in MRFs or CRFs. What the MRF/CRF formalism gives us is the possibility to also integrate neighborhood constraints and this corresponds to context information. In a deep learning context with CNNs the CNN will take into account context information automatically through its processing chain. Therefore, it also can be understood as a form of classification or segmentation problem that takes into account context information. But here the MRF or CRF, basically, provides us with the opportunity to explicitly put in context, a type of context that we can control, where we know exactly what kind of assumptions we make about the context.&rdquo;</span></li></ul><h2 class="c20" id="h.s78pmegjoj1s"><span class="c34">Object detection</span></h2><h3 class="c0" id="h.cae4wsynbgvl"><span class="c21">Challenges</span></h3><ul class="c2 lst-kix_7mgbb75pci5l-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">Leibe zu &ldquo;Scale change&rdquo;: Was er meint ist, dass das receptive field des Filters gleichzeitig ma&szlig;getreu angepasst werden muss, wenn man ein Bild verkleinert/vergr&ouml;&szlig;ert, sonst extracted der Filter andere Features (zB Edges, &hellip;)</span></li></ul><h3 class="c0" id="h.puvlqpxg6bnt"><span class="c21">Gradient-based Representations</span></h3><ul class="c2 lst-kix_48ib1dz7zp04-0 start"><li class="c10 c15 li-bullet-0"><span>Localized histograms offer </span><span class="c3">more spatial information</span><span>&nbsp;than a single global histogram &ldquo;</span><span class="c3">tradeoff invariant vs discriminative</span><span class="c1">&rdquo; (folie 35 unter &ldquo;Summarize local distribution of gradients with histograms&rdquo;) soll hei&szlig;en, dass </span></li></ul><ul class="c2 lst-kix_48ib1dz7zp04-1 start"><li class="c5 li-bullet-0"><span>von </span><span class="c22 c3"><a class="c11" href="https://www.google.com/url?q=https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108564070&amp;usg=AOvVaw2nKrFNYgnidT8Npi8FOs33">paper</a></span><span>&nbsp;(vgl. n&auml;chste Vorlesung): </span><span class="c7">The keypoint descriptor is shown on the right side of </span><span class="c22 c7"><a class="c11" href="#id.mvshh7hmpj1r">Figure 7</a></span><span class="c7">. It allows for significant shift in gradient positions by creating orientation histograms over 4x4 </span><span class="c8 c3">sample regions</span><span class="c7">. The figure shows eight directions for each </span><span class="c8 c3">orientation histogram</span><span class="c7">, with the length of each arrow corresponding to the magnitude of that histogram entry. A gradient sample on the left can shift up to 4 sample positions while still contributing to the same histogram on the right, </span><span class="c18 c3 c53 c74">thereby achieving the objective of allowing for larger local positional shifts.</span></li><li class="c5 li-bullet-0"><span class="c7">aus </span><span class="c22 c7"><a class="c11" href="https://www.google.com/url?q=https://cs.gmu.edu/~kosecka/cs482/grauman-recognition-draft-27-01-11.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108565774&amp;usg=AOvVaw1HPNuoohefdKrc4m1lNbSJ">Grauman Buch</a></span><span class="c7">&nbsp;(3.3.1 THE SIFT DESCRIPTOR): Sampling is performed in a regular grid of 16</span><span class="c53">&times;</span><span class="c7">16 locations covering the interest region. For each sampled location, the gradient orientation is entered into a </span><span class="c3 c8">coarser 4</span><span class="c3 c53 c12">&times;</span><span class="c8 c3">4 grid of gradient orientation histograms</span><span class="c7">&nbsp;with 8 orientation bins each [...]. [...] This procedure is visualized for a smaller 2</span><span class="c53">&times;</span><span class="c7">2 grid in Figure 3.8. The motivation for this choice of representation is that the </span><span class="c8 c3">coarse</span><span class="c7">&nbsp;spatial binning allows for small shifts due to registration errors without overly affecting the descriptor. At the same time, the high-dimensional representation provides enough discriminative power to reliably distinguish a large number of keypoints.</span></li><li class="c5 li-bullet-0"><span>zu </span><span class="c3">tradeoff</span><span class="c1">: in diesem Sinne [The motivation for this choice of representation is that the coarse spatial binning allows for small shifts due to registration errors without overly affecting the descriptor. At the same time, the high-dimensional representation provides enough discriminative power to reliably distinguish a large number of keypoints.] gibt es einen tradeoff zwischen invariant [gg&uuml; small shifts/rotations] und discriminative: </span></li></ul><ul class="c2 lst-kix_48ib1dz7zp04-2 start"><li class="c10 c27 li-bullet-0"><span class="c3">discriminative </span><span>ist hier das Gegenteil von </span><span class="c3">invariant </span></li></ul><ul class="c2 lst-kix_48ib1dz7zp04-3 start"><li class="c10 c35 li-bullet-0"><span>je discriminativer umso &ouml;fter sagt der classifier &ldquo;das ist nicht das gleiche Bild&rdquo;, auch wenn beide Bilder &auml;hnlich sind !</span><span class="c12 c33">&nbsp; </span></li></ul><ul class="c2 lst-kix_48ib1dz7zp04-1"><li class="c5 li-bullet-0"><span class="c13 c3 c41">Merke einfach: </span></li></ul><ul class="c2 lst-kix_48ib1dz7zp04-2 start"><li class="c10 c27 li-bullet-0"><span>localized histograms (also 4 sub-histograms statt 1 globales) (also das &ldquo;coarser 4&times;4 grid of gradient orientation histograms&rdquo;) sind f&uuml;r beides gut, die </span><span class="c3">spatial information</span><span>&nbsp;</span><span class="c25">UND</span><span>&nbsp;die </span><span class="c3">shift/rotation invariance</span><span class="c1">&nbsp;gut.</span></li></ul><ul class="c2 lst-kix_48ib1dz7zp04-1"><li class="c5 li-bullet-0"><span class="c13 c3 c12">unwahrsch. als Klausurfrage ! </span></li></ul><h1 class="c38" id="h.knegyo7un0uf"><span class="c24">lec9 - Object detection II</span></h1><h2 class="c20" id="h.vgyurnlp3f7y"><span>HOG detector</span></h2><h3 class="c0" id="h.tl7ex5x9mzrt"><span class="c21">Descriptor representation</span></h3><a id="id.mvshh7hmpj1r"></a><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 342.67px;"><img alt="" src="images/image77.png" style="width: 601.70px; height: 342.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c59"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 551.50px; height: 475.15px;"><img alt="" src="images/image40.png" style="width: 551.50px; height: 475.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c59"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 532.65px; height: 380.60px;"><img alt="" src="images/image82.png" style="width: 532.65px; height: 380.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_m5ey97dhxg5-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">zu &ldquo;affine changes in illumination&rdquo;:</span></li></ul><ul class="c2 lst-kix_m5ey97dhxg5-1 start"><li class="c5 li-bullet-0"><span>zu </span><span class="c3 c12">affine transformations</span><span class="c1">: </span></li></ul><ul class="c2 lst-kix_m5ey97dhxg5-2 start"><li class="c10 c27 li-bullet-0"><span>every affine transformation on </span><span class="c33">X</span><span>&nbsp;can be represented as the</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Composition_of_functions&amp;sa=D&amp;source=editors&amp;ust=1642006108570170&amp;usg=AOvVaw2T6jmJFFGVXXOBqAz9Epvq">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Composition_of_functions&amp;sa=D&amp;source=editors&amp;ust=1642006108570629&amp;usg=AOvVaw1SWiX4bIAM2_qbJKz6yy8i">composition</a></span><span>&nbsp;of a</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Linear_transformation&amp;sa=D&amp;source=editors&amp;ust=1642006108570931&amp;usg=AOvVaw3F7QGWkyGX9-SkqF32yTTx">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Linear_transformation&amp;sa=D&amp;source=editors&amp;ust=1642006108571226&amp;usg=AOvVaw17z4ZHz58BRo_7KRKYSzkO">linear transformation</a></span><span>&nbsp;on </span><span class="c33">X</span><span>&nbsp;and a</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Translation_(geometry)&amp;sa=D&amp;source=editors&amp;ust=1642006108571596&amp;usg=AOvVaw0RR_UGm7HWDScbCzEryVyP">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Translation_(geometry)&amp;sa=D&amp;source=editors&amp;ust=1642006108571863&amp;usg=AOvVaw3EAqkE1tMMhRKmji7JPD7n">translation</a></span><span>&nbsp;of </span><span class="c33">X</span><span>. Unlike a purely linear transformation, an affine transformation need not preserve the </span><span class="c3">origin</span><span>&nbsp;of the affine space. Thus, </span><span class="c33">every linear transformation is affine, but not every affine transformation is linear</span><span class="c1">.</span></li><li class="c10 c27 li-bullet-0"><span>Examples of affine transformations include translation,</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Scaling_(geometry)&amp;sa=D&amp;source=editors&amp;ust=1642006108572740&amp;usg=AOvVaw0VQMBkx_nVDItXktiZxidO">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Scaling_(geometry)&amp;sa=D&amp;source=editors&amp;ust=1642006108573027&amp;usg=AOvVaw0FLrR0cN6kiDenEPfgQ7-N">scaling</a></span><span>,</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Homothetic_transformation&amp;sa=D&amp;source=editors&amp;ust=1642006108573332&amp;usg=AOvVaw3M1GTd-zPKXEmFpfD8Q_2T">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Homothetic_transformation&amp;sa=D&amp;source=editors&amp;ust=1642006108573622&amp;usg=AOvVaw3CIB-TTcR9J8pLRVnl4FYq">homothety</a></span><span>,</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Similarity_transformation_(geometry)&amp;sa=D&amp;source=editors&amp;ust=1642006108573937&amp;usg=AOvVaw07qAX8mXoJ_byeqcrJmOIm">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Similarity_transformation_(geometry)&amp;sa=D&amp;source=editors&amp;ust=1642006108574216&amp;usg=AOvVaw0kDRCLWfMKbaPCTGSOXqsq">similarity</a></span><span>,</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Reflection_(mathematics)&amp;sa=D&amp;source=editors&amp;ust=1642006108574549&amp;usg=AOvVaw296XJeShv282kJNi-d1_gE">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Reflection_(mathematics)&amp;sa=D&amp;source=editors&amp;ust=1642006108574830&amp;usg=AOvVaw20lhId7oMAUHInDb2XPiJo">reflection</a></span><span>,</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Rotation_(mathematics)&amp;sa=D&amp;source=editors&amp;ust=1642006108575135&amp;usg=AOvVaw1h_jBnnJQvGQo650tLjkE_">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Rotation_(mathematics)&amp;sa=D&amp;source=editors&amp;ust=1642006108575390&amp;usg=AOvVaw02Jfgk-tYI-A_YydwL7yQW">rotation</a></span><span>,</span><span><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Shear_mapping&amp;sa=D&amp;source=editors&amp;ust=1642006108575666&amp;usg=AOvVaw0pi3Srcyd8khysLj5TrEpy">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Shear_mapping&amp;sa=D&amp;source=editors&amp;ust=1642006108575950&amp;usg=AOvVaw0uv3ZyMA4k8Dovhl_KlTbc">shear mapping</a></span><span class="c1">, and compositions of them in any combination and sequence.</span></li></ul><ul class="c2 lst-kix_m5ey97dhxg5-3 start"><li class="c10 c35 li-bullet-0"><span>zu </span><span class="c3 c12">homothety</span><span>: Eine </span><span class="c22 c3"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Zentrische_Streckung&amp;sa=D&amp;source=editors&amp;ust=1642006108576649&amp;usg=AOvVaw26awjZ5vy9Brg8loJXdz2P">zentrische Streckung</a></span><span>&nbsp;ist in der</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108576960&amp;usg=AOvVaw19lMtACU0DH_-HK1BrCoVs">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108577284&amp;usg=AOvVaw2H9GGmgj9qfvEYItVVG6L_">Geometrie</a></span><span>&nbsp;eine</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Abbildung_(Mathematik)&amp;sa=D&amp;source=editors&amp;ust=1642006108577647&amp;usg=AOvVaw1vjdYoqL8y_OirpL6dYfO7">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Abbildung_(Mathematik)&amp;sa=D&amp;source=editors&amp;ust=1642006108577940&amp;usg=AOvVaw2grCyAjH25HP8UC9L02SpL">Abbildung</a></span><span>, die alle</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Strecke_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108578277&amp;usg=AOvVaw20NhqpxQAhf4x4Erz-JXQA">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Strecke_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108578578&amp;usg=AOvVaw1O8CaqMM27stFvaAdzzVW_">Strecken</a></span><span>&nbsp;in einem bestimmten, gegebenen</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Verh%25C3%25A4ltnis_(Mathematik)&amp;sa=D&amp;source=editors&amp;ust=1642006108578905&amp;usg=AOvVaw2HE4KRKrNFRMo0_7cDd7y6">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Verh%25C3%25A4ltnis_(Mathematik)&amp;sa=D&amp;source=editors&amp;ust=1642006108579193&amp;usg=AOvVaw0HAw--9Ri6Gev9BWk9FfVa">Verh&auml;ltnis</a></span><span>&nbsp;vergr&ouml;&szlig;ert oder verkleinert, wobei die Bildstrecken jeweils zu den urspr&uuml;nglichen Strecken</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Parallel_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108579511&amp;usg=AOvVaw1roqNDRapXD4_NXHPO6WDN">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Parallel_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108579730&amp;usg=AOvVaw32QKF1viE45D3Y_Ou399Xi">parallel</a></span><span>&nbsp;sind. Zentrische Streckungen sind spezielle</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/%25C3%2584hnlichkeitsabbildung&amp;sa=D&amp;source=editors&amp;ust=1642006108579944&amp;usg=AOvVaw0gvvC02xvQ9Fg5_qp0Zo0x">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/%25C3%2584hnlichkeitsabbildung&amp;sa=D&amp;source=editors&amp;ust=1642006108580207&amp;usg=AOvVaw3KAScO6o1T_sbUfoObmFm9">&Auml;hnlichkeitsabbildungen</a></span><span>, in der</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Synthetische_Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108580426&amp;usg=AOvVaw3y797Hs4jL-qIoLPBv8Coq">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Synthetische_Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108580638&amp;usg=AOvVaw1kscEwbw48bkdU9UlT9rqL">synthetischen Geometrie</a></span><span>&nbsp;nennt man sie auch </span><span class="c3">Homothetien.</span></li></ul><ul class="c2 lst-kix_m5ey97dhxg5-4 start"><li class="c10 c39 li-bullet-0"><span>Als </span><span class="c3">&Auml;hnlichkeitsabbildung</span><span>&nbsp;(oder </span><span class="c3">&Auml;hnlichkeit</span><span>) wird in der</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108581208&amp;usg=AOvVaw24k3YbHhOiA7BrYU4w9nTd">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108581422&amp;usg=AOvVaw2qzwPEZ_GBqlldt9Hv7tb6">Geometrie</a></span><span>, einem Teilgebiet der</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Mathematik&amp;sa=D&amp;source=editors&amp;ust=1642006108581678&amp;usg=AOvVaw1cgqYYVLioCkFpX8-xLOM_">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Mathematik&amp;sa=D&amp;source=editors&amp;ust=1642006108581883&amp;usg=AOvVaw2sZIeZ3s_nGGZOyLpA-hSo">Mathematik</a></span><span>, eine</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Affinit%25C3%25A4t_(Mathematik)&amp;sa=D&amp;source=editors&amp;ust=1642006108582220&amp;usg=AOvVaw0rpAHb5Fwsp-3OW3sDC5aL">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Affinit%25C3%25A4t_(Mathematik)&amp;sa=D&amp;source=editors&amp;ust=1642006108582464&amp;usg=AOvVaw2Ho3mmUJpONSww2B_9X1Vq">Affinit&auml;t</a></span><span>&nbsp;bezeichnet, die Streckenverh&auml;ltnisse und</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Winkel&amp;sa=D&amp;source=editors&amp;ust=1642006108582715&amp;usg=AOvVaw0lXkteV3ls9VWt4u5jrZii">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Winkel&amp;sa=D&amp;source=editors&amp;ust=1642006108582900&amp;usg=AOvVaw1vFCVXDrovyw2KNWTXSEho">Winkelgr&ouml;&szlig;en</a></span><span>&nbsp;unver&auml;ndert l&auml;sst, aber im Allgemeinen die L&auml;ngen von</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Strecke_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108583191&amp;usg=AOvVaw3HhmuKBqXPkQfBY869GkYI">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Strecke_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108583407&amp;usg=AOvVaw0c_AaqO1zmLWbgx9WxD6C8">Strecken</a></span><span class="c1">&nbsp;&auml;ndert.</span></li></ul><ul class="c2 lst-kix_m5ey97dhxg5-3"><li class="c10 c35 li-bullet-0"><span>zu </span><span class="c3 c12">similarity</span><span>: (</span><span class="c22 c3"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/%25C3%2584hnlichkeit_(Geometrie)&amp;sa=D&amp;source=editors&amp;ust=1642006108584036&amp;usg=AOvVaw1k1y85dEgH9zilQupBmCD1">&Auml;hnlichkeit</a></span><span class="c3">) </span><span>In der</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108584358&amp;usg=AOvVaw0XQC0lA0RUt-un79Lhfwmg">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/Geometrie&amp;sa=D&amp;source=editors&amp;ust=1642006108584603&amp;usg=AOvVaw1jQAzj1PYw-0iqn2jueIJI">Geometrie</a></span><span>&nbsp;sind zwei Figuren genau dann zueinander </span><span class="c3">&auml;hnlich</span><span>, wenn sie durch eine</span><span><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/%25C3%2584hnlichkeitsabbildung&amp;sa=D&amp;source=editors&amp;ust=1642006108585012&amp;usg=AOvVaw21VgxzivXAAdh1ddbrOlUo">&nbsp;</a></span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://de.wikipedia.org/wiki/%25C3%2584hnlichkeitsabbildung&amp;sa=D&amp;source=editors&amp;ust=1642006108585294&amp;usg=AOvVaw3nziPEv9o78IlrHyN80e9e">&Auml;hnlichkeitsabbildung</a></span><span>&nbsp;(auch diese Abbildung wird h&auml;ufig als </span><span class="c33">&Auml;hnlichkeit</span><span class="c1">&nbsp;bezeichnet) ineinander &uuml;berf&uuml;hrt werden k&ouml;nnen.</span></li></ul><ul class="c2 lst-kix_m5ey97dhxg5-0"><li class="c10 c15 li-bullet-0"><span>zu </span><span class="c3">L2-normalization</span><span>: from Dalal </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108586082&amp;usg=AOvVaw11yfP9fe51brajB49B1SKc">paper</a></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 409.00px; height: 168.00px;"><img alt="" src="images/image57.png" style="width: 409.00px; height: 168.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><p class="c59"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 435.00px; height: 225.00px;"><img alt="" src="images/image62.png" style="width: 435.00px; height: 225.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 441.00px; height: 366.00px;"><img alt="" src="images/image70.png" style="width: 441.00px; height: 366.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_kipghvp0ii5t-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">auch aus text: </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span>&ldquo;The detector window is </span><span class="c3">tiled</span><span class="c1">&nbsp;with a grid of overlapping blocks in which Histogram of Oriented Gradient feature vectors are extracted.&rdquo;</span></li><li class="c5 li-bullet-0"><span>Figure 4 (d) Using </span><span class="c3">overlapping descriptor blocks</span><span class="c1">&nbsp;decreases the miss rate by around 5%.</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c1">wichtiges im obigen Text: </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span class="c1">dieses &ldquo;overlapping grid of HOG descriptor blocks&rdquo; entsteht also bei der contrast-normalization (wie auf Leibe Folie steht) und verbessert die &ldquo;miss rate&rdquo;</span></li></ul><h2 class="c20" id="h.edlokq69jdja"><span>Viola-Jones detector</span></h2><a id="id.bg0uvgq16ux6"></a><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c3">intuition</span><span>: two-rectangle features can be used to measure the difference in intensity between the region of the eyes and a region across the upper cheeks (</span><span class="c22"><a class="c11" href="#id.g6bm3mjuir6n">fig 3</a></span><span class="c1">)</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span>paper: &ldquo;For the task of face detection, the initial rectangle features selected by AdaBoost are meaningful and easily interpreted. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks (see </span><span class="c22"><a class="c11" href="#id.g6bm3mjuir6n">Figure 3</a></span><span class="c1">). This feature is relatively large in comparison with the detection subwindow, and should be somewhat insensitive to size and location of the face. The second feature selected relies on the property that the eyes are darker than the bridge of the nose.</span></li></ul><a id="id.g6bm3mjuir6n"></a><p class="c59 c52"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 339.45px; height: 356.19px;"><img alt="" src="images/image45.png" style="width: 339.45px; height: 356.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c3 c12">haar basis function features = rectangle features </span><span>(von </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108588547&amp;usg=AOvVaw0-TyJw5vYA3rvq8jk0QRCG">paper</a></span><span>):</span><a id="id.bf1axytqftth"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 426.00px; height: 313.00px;"><img alt="" src="images/image56.png" style="width: 426.00px; height: 313.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span>&ldquo;we use </span><span class="c3">three</span><span>&nbsp;kinds of features&rdquo; - sie benutzen alle drei Varianten </span><span class="c3">gleichzeitig ! </span><span class="c1">(sie haben 3 versch. Varianten benutzt, aber das sind vlt tats&auml;chlich mehr als 3 features, zB feature A und B in fig 1 geh&ouml;ren zu einer Variante, sind aber tats&auml;chlich verschieden) </span></li><li class="c5 li-bullet-0"><span class="c1">in vio-jones implementation: </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">24x24 detection window</span></li><li class="c10 c27 li-bullet-0"><span class="c1">pool of &gt;180k rectangle features</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c3 c12">rectangle features</span><span class="c3">: </span><span>the </span><span class="c3 c12">value of a rectangle feature</span><span>&nbsp;is the calculated difference (between the sum of the pixels in the white and grey regions) # s. </span><span class="c22"><a class="c11" href="#id.bf1axytqftth">text</a></span><span>&nbsp;</span><a id="id.ariv61hdbinw"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 427.00px; height: 394.00px;"><img alt="" src="images/image79.png" style="width: 427.00px; height: 394.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span class="c3 c12">integral image</span><span class="c13 c3 c41">:</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span>Rectangle features can be computed very rapidly using an </span><span class="c25 c3">intermediate representation</span><span>&nbsp;for the image which we call the </span><span class="c25 c3">integral image</span><span>. The integral image at location (x, y) </span><span class="c3 c25">contains the sum of the pixels</span><span>&nbsp;</span><span class="c32">above and to the left of (x, y) [s Bild]</span><span class="c1">, inclusive:</span></li></ul><p class="c59 c44"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 211.80px; height: 219.31px;"><img alt="" src="images/image52.png" style="width: 211.80px; height: 219.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_kipghvp0ii5t-1"><li class="c5 li-bullet-0"><span class="c4">&ldquo;the integral image can be computed </span><span class="c25 c3 c50">in one pass</span><span class="c4">&nbsp;over the original image</span><span>&rdquo; (aus paper)</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c3">rectangle calculation: </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 437.00px; height: 368.00px;"><img alt="" src="images/image59.png" style="width: 437.00px; height: 368.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span class="c1">independent of size: egal wie gro&szlig; detection window D ist, die calculation ist gleich expensive =&gt; no downsampling needed anymore, scale features directly instead (which is equally expensive)</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span>computational </span><span class="c3">complexity</span><span>:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 446.00px; height: 169.00px;"><img alt="" src="images/image55.png" style="width: 446.00px; height: 169.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span class="c25 c3">six</span><span>&nbsp;array references for &ldquo;</span><span class="c25 c3">two</span><span>-rectangle features&rdquo;:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 478.40px; height: 228.07px;"><img alt="" src="images/image31.png" style="width: 478.40px; height: 228.07px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">Vorfaktor +2 und -2 weil die &ldquo;borderpixel&rdquo; 2 mal in die Differenz eingehen (einmal f&uuml;r wei&szlig;es Rechteck und einmal f&uuml;r schwarzes) </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span>Leibe: &ldquo;key idea of Viola-Jones detector is that we present the detector with a large library/pool of such features. Each feature is one configuration of those rectangles at a certain position and with a certain aspect ratio (wiki: the </span><span class="c3">aspect ratio</span><span>&nbsp;of an image is the </span><span class="c3">ratio</span><span class="c1">&nbsp;of its width to its height). we can vary the top left corner, we can vary the bottom right corner and we can vary the type of the feature (eg horizontal subdivision, vertical subdivision, three-rectangle feature, four-rectangle (diagonal) feature, etc)&rdquo;</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span>Leibe: &ldquo;we simply define different types of features and for each type we define a number of top left corner positions and bottom right corner positions and from those possibilities we span up a huge </span><span class="c25 c3">pool of possible feature configurations</span><span>&rdquo; (</span><span class="c3">Achtung: </span><span>Im folgenden </span><span class="c22"><a class="c11" href="#id.li1qdv5rctxz">Bild</a></span><span>&nbsp;sind nur die 24x24 (sliding) </span><span class="c3">detection windows</span><span>&nbsp;- und </span><span class="c25 c3">in</span><span class="c1">&nbsp;diesen die rectangle features - dargestellt und nicht die Bilder &uuml;ber die sie geschoben werden !) </span></li></ul><a id="id.li1qdv5rctxz"></a><p class="c10 c44"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 426.50px; height: 235.96px;"><img alt="" src="images/image24.png" style="width: 426.50px; height: 235.96px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c0" id="h.nmsfodcad6fk"><span class="c21">AdaBoost algo</span></h3><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 418.50px; height: 297.00px;"><img alt="" src="images/image36.png" style="width: 418.50px; height: 297.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span>&ldquo;Table 1: The AdaBoost algorithm for classifier learning. </span><span class="c18">Each round of boosting selects one feature</span><span class="c1">&nbsp;from the 180,000 potential features.&rdquo;</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span>What do the </span><span class="c3">initially</span><span class="c1">&nbsp;selected features of AdaBoost mean ? </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2 start"><li class="c10 c27 li-bullet-0"><span>s. </span><span class="c22"><a class="c11" href="#id.bg0uvgq16ux6">intuition</a></span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c3 c12">cascade of AdaBoost classifiers</span><span class="c1">:</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span class="c25 c3">in paper</span><span class="c1">: </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2 start"><li class="c10 c27 li-bullet-0"><span>in stage 1</span><span class="c1">&nbsp;im Grunde dasselbe wie ohne cascade, aber nur mit 2 (von AdaBoost gew&auml;hlten) features =&gt; less expensive </span></li><li class="c10 c27 li-bullet-0"><span class="c1">in stage 2 auch dasselbe wie ohne cascade, aber mit anderem - schwierigeren - training set (set mit klaren negatives ausgesiebt)</span></li><li class="c10 c27 li-bullet-0"><span class="c1">Hyperparameter of the cascade: </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">i) the number of classifier stages</span></li><li class="c10 c35 li-bullet-0"><span class="c1">ii) the number of features in each stage</span></li><li class="c10 c35 li-bullet-0"><span>iii) the threshold of each stage (ie. for face-no face decision; we want to detect </span><span class="c3">almost all faces</span><span>, s. </span><span class="c22"><a class="c11" href="#id.h0hw542jngx">hier</a></span><span class="c1">)</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2"><li class="c10 c27 li-bullet-0"><span class="c3">first stage</span><span>: 2 rectangle features (s. </span><span class="c22"><a class="c11" href="#id.g6bm3mjuir6n">fig 3</a></span><span>), die </span><span class="c25 c3">von AdaBoost ausgew&auml;hlt</span><span class="c1">&nbsp;werden !</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-3 start"><li class="c10 c35 li-bullet-0"><span>&ldquo;</span><span class="c3 c32">Given</span><span>&nbsp;a </span><span class="c3 c32">feature set</span><span>&nbsp;and a </span><span class="c3 c32">training set</span><span>&nbsp;of positive and negative images, any number of machine learning approaches could be used to learn a classification function. In our system a variant of AdaBoost is used </span><span class="c33">both</span><span class="c1">&nbsp;to select a small set of features and train the classifier&rdquo;</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2"><li class="c10 c27 li-bullet-0"><span>diese beiden features werden </span><span class="c25 c3">mit</span><span>&nbsp;dem sliding detection window &uuml;ber </span><span class="c3">alle</span><span>&nbsp;</span><span class="c3">Bilder im training set</span><span class="c1">&nbsp;geschoben </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-3 start"><li class="c10 c35 li-bullet-0"><span class="c3">Achtung</span><span>: jedes einzelne der 180000+ rectangle features hat eine feste Position IM DETECTION WINDOW (vgl </span><span class="c22"><a class="c11" href="#id.ariv61hdbinw">fig 1</a></span><span class="c1">) </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2"><li class="c10 c27 li-bullet-0"><span>Adaboost Classifier 1 wertet </span><span class="c3">jeden</span><span class="c1">&nbsp;dieser sliding windows aus (geht schnell, weil nur 2 rect features)</span></li><li class="c10 c27 li-bullet-0"><span class="c1">positive sliding window (face) wird weiter von AdaBoost Classifier 2 ausgewertet, negatives (no face) werden rejected </span></li><li class="c10 c27 li-bullet-0"><span>T: True face detections, F: False face detections</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 397.50px; height: 213.76px;"><img alt="" src="images/image49.png" style="width: 397.50px; height: 213.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><a id="id.h0hw542jngx"></a><ul class="c2 lst-kix_kipghvp0ii5t-2"><li class="c10 c27 li-bullet-0"><span>adjust </span><span class="c3">each</span><span>&nbsp;AdaBoost classifier&rsquo;s threshold such that </span><span class="c3">almost all </span><span class="c1">faces are detected (dh maximiere TPs und minimiere FNs), dh. FNR close to 0.</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span class="c1">TPR=recall=TP/(TP+FN) (dh Verh&auml;ltnis TPs zu alle die &ldquo;tats&auml;chlich&rdquo; positives sein sollten; denn FNs sollten ja &ldquo;tats&auml;chlich&rdquo; positives sein!)</span></li><li class="c10 c15 li-bullet-0"><span>Muster f&uuml;r die rates TPR, FPR, TNR, FNR: </span><span class="c3 c12">x/(N oder P)</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1 start"><li class="c5 li-bullet-0"><span class="c1">merke: </span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">1. was im Z&auml;hler steht, steht auch im Nenner</span></li><li class="c10 c27 li-bullet-0"><span class="c1">2. im Nenner entweder TP+FN oder TN+FP</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-1"><li class="c5 li-bullet-0"><span class="c1">merke: es gibt also nur 2 m&ouml;gliche Nenner (n&auml;mlich P=TP+FN und N=TN+FP)</span></li></ul><ul class="c2 lst-kix_kipghvp0ii5t-0"><li class="c10 c15 li-bullet-0"><span>Cascades: A frequently used method for speeding up classifiers is to split them up into a sequence of simpler classifiers. By having the first stages prune most of the false positives (&auml;quivalent zu &ldquo;FNR close to 0&rdquo;, weil wir lassen &ldquo;almost all&rdquo; positives durch, dh es gibt fast keine false positives mehr nach den ersten stages [=&gt;wir &ldquo;prunen&rdquo; die false positives]), the average computation time is significantly reduced (aus </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=http://people.ee.ethz.ch/~timofter/publications/Benenson-CVPR-2012.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108599242&amp;usg=AOvVaw2msVhr6DZs8iFPLTvF9Q1v">paper</a></span><span class="c1">)</span></li></ul><h1 class="c38" id="h.vvu1416c83wp"><span class="c24">lec13 - CNNs</span></h1><h2 class="c20" id="h.lrojhfjb5mqw"><span class="c34">Convolutional layers</span></h2><h2 class="c20" id="h.e5q1rzw8zruj"><span class="c34">Pooling Layers</span></h2><h2 class="c20" id="h.c3vhce7q7ccx"><span class="c34">Nonlinearities</span></h2><ul class="c2 lst-kix_ixeboairdkos-0 start"><li class="c10 c15 li-bullet-0"><span class="c3">Question</span><span class="c1">: What do ReLU layers accomplish?</span></li></ul><ul class="c2 lst-kix_ixeboairdkos-1 start"><li class="c5 li-bullet-0"><span class="c3">Answer</span><span class="c1">: Piece-wise linear tiling: mapping is locally linear.</span></li><li class="c5 li-bullet-0"><span class="c3">Long Answer</span><span>: ReLU layers do local linear approximation. Number of planes grows exponentially &nbsp;with &nbsp;number &nbsp;of &nbsp;hidden &nbsp;units. &nbsp;Multiple &nbsp;layers &nbsp;yield &nbsp;exponential savings in number of parameters (parameter sharing). (Montufar et al. &nbsp;&ldquo;On the number of linear regions of DNNs&rdquo; arXiv 2014, </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1402.1869.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108601142&amp;usg=AOvVaw1eAfKNolMRA_kc5ived6lq">https://arxiv.org/pdf/1402.1869.pdf</a></span><span>&nbsp;)</span></li></ul><h1 class="c38" id="h.ghq5lsn7i7in"><span class="c24">lec14 - ML Recap, Backbones</span></h1><h2 class="c20" id="h.776slog1lkf3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 424.00px;"><img alt="" src="images/image38.png" style="width: 601.70px; height: 424.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h2><h2 class="c20" id="h.gekyv6932mpy"><span>Stanford - Recap</span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup></h2><h3 class="c0" id="h.1v9vvz7q7ci6"><span class="c21">Recap: AlexNet, VGG, GoogleNet, ResNet, DenseNet, FractalNet</span></h3><p class="c10"><span>As a reminder, last time we were talking about CNN Architectures. We kind of walked through the timeline of some of the various winners of the </span><span class="c3 c12">Imagenet classification challenge</span><span class="c1">: </span></p><ul class="c2 lst-kix_hhjcx5eamba3-0 start"><li class="c10 c15 li-bullet-0"><span>Kind of the breakthrough result, as we saw, was the </span><span class="c3 c12">AlexNet</span><span class="c1">&nbsp;architecture in 2012, which was a nine layer convolutional network. It did amazingly well, and it sort of kick started this whole deep learning revolution in computer vision, and kind of brought a lot of these models into the mainstream. </span></li><li class="c10 c15 li-bullet-0"><span>Then we skipped ahead a couple years, and saw that in 2014 image net challenge, we had these two really interesting models, </span><span class="c3 c12">VGG and GoogLeNet</span><span class="c3">, which were much deeper</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-1 start"><li class="c5 li-bullet-0"><span>So </span><span class="c3">VGG</span><span>&nbsp;was, they had a 16 and a 19 layer model, and </span><span class="c3">GoogLeNet</span><span class="c1">&nbsp;was, I believe, a 22 layer model. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-0"><li class="c10 c15 li-bullet-0"><span>Although one thing that is kind of interesting about these models is that the 2014 image net challenge was right </span><span class="c3">before batch normalization was invented</span><span>. So at this time, before the invention of batch normalization, training these relatively deep models of </span><span class="c3">roughly twenty layers was very challenging</span><span>. So, in fact, </span><span class="c18 c3">both of these two models</span><span>&nbsp;had to resort to a little bit of </span><span class="c18 c3">hackery in order to get their deep models to converge</span><span class="c1">: </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-1 start"><li class="c5 li-bullet-0"><span>So </span><span class="c3 c42">for VGG</span><span>, they had the 16 and 19 layer models, but </span><span class="c3">actually they first trained an 11 layer model</span><span>, because that was what they could get to converge. And </span><span class="c3">then added some extra random layers in the middle and then continued training</span><span class="c1">, actually training the 16 and 19 layer models. So, managing this training process was very challenging in 2014 before the invention of batch normalization. </span></li><li class="c5 li-bullet-0"><span>Similarly, for </span><span class="c3 c42">GoogLeNet</span><span>, we saw that GoogLeNet has these </span><span class="c3">auxiliary classifiers</span><span>&nbsp;that were </span><span class="c3">stuck into lower layers</span><span class="c1">&nbsp;of the network. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-2 start"><li class="c10 c27 li-bullet-0"><span>And these were </span><span class="c3">not really needed</span><span class="c1">&nbsp;for the class to, to get good classification performance. </span></li><li class="c10 c27 li-bullet-0"><span>This was just sort of </span><span class="c3">a way to cause extra gradient to be injected directly into the lower layers</span><span class="c1">&nbsp;of the network. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-1"><li class="c5 li-bullet-0"><span>And this sort of, this again was before the invention of batch normalization and now </span><span class="c18 c3">once you have these networks with batch normalization, then you no longer need these slightly ugly hacks</span><span class="c1">&nbsp;in order to get these deeper models to converge. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-0"><li class="c10 c15 li-bullet-0"><span>Then we also saw in the 2015 image net challenge was this really cool model called </span><span class="c3 c12">ResNet</span><span>, these residual networks that now have these shortcut connections that actually have these little </span><span class="c3">residual blocks</span><span>&nbsp;where we&#39;re going to take our input, pass it through the residual blocks, and then add our input to the block to the output from these convolutional layers. This is kind of a funny architecture, but it actually has two really nice </span><span class="c3">properties</span><span class="c1">: </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-1 start"><li class="c5 li-bullet-0"><span>One is that if we just set all the weights in this residual block to zero, then this block is computing the identity. So in some way, </span><span class="c3">it&#39;s relatively easy for this model to learn not to use the layers that it doesn&#39;t need</span><span class="c1">. In addition, it kind of adds this interpretation to L2 regularization in the context of these neural networks, cause once you put L2 regularization, remember, on the weights of your network, that&#39;s going to drive all the parameters towards zero.</span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-2 start"><li class="c10 c27 li-bullet-0"><span>If your </span><span class="c3">standard</span><span>&nbsp;convolutional architecture [ie an architecture </span><span class="c3">w/o</span><span class="c1">&nbsp;skip connections] is driving towards zero, it [a layer with weights=0] doesn&#39;t make sense. </span></li><li class="c10 c27 li-bullet-0"><span>But in the context of a </span><span class="c3">residual network</span><span>, if you drive </span><span class="c25 c3">all</span><span class="c1">&nbsp;the parameters towards zero, that&#39;s kind of encouraging the model to not use layers that it doesn&#39;t need, because it will just drive those, the residual blocks towards the identity, whether or not needed for classification. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-1"><li class="c5 li-bullet-0"><span>The other really useful property of these residual networks has to do with the </span><span class="c3">gradient flow in the backward paths</span><span>. If you remember what happens at these addition gates in the backward pass, when upstream gradient is coming in through an addition gate, then it will split and fork along these two different paths. So then, when upstream gradient comes in, it&#39;ll take one path through these convolutional blocks, but it will also have a direct connection of the gradient through this residual connection. So then when you look at, when you imagine stacking many of these residual blocks on top of each other, and our network ends up with hundreds of, potentially hundreds of layers. Then, these residual connections give a sort of </span><span class="c3">gradient super highway</span><span>&nbsp;for gradients to flow backward through the entire network. And this allows it to train much easier and much faster. And actually </span><span class="c3">allows these things to converge reasonably well, even when the model is potentially hundreds of layers deep</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-2 start"><li class="c10 c27 li-bullet-0"><span>And this idea of </span><span class="c3 c42">managing gradient flow</span><span class="c1">&nbsp;in your models is actually super important everywhere in machine learning. And super prevalent in recurrent networks as well. So we&#39;ll definitely revisit this idea of gradient flow later in today&#39;s lecture. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-0"><li class="c10 c15 li-bullet-0"><span>So then, we kind of also saw a couple other more exotic, more recent CNN architectures last time, including </span><span class="c3 c12">DenseNet</span><span>&nbsp;and </span><span class="c3 c12">FractalNet</span><span>, and once you think about these architectures in terms of gradient flow, they make a little bit more sense. These things like DenseNet and FractalNet are adding these </span><span class="c3">additional shortcut or identity connections</span><span>&nbsp;inside the model. And if you think about what happens in the backwards pass for these models, these additional funny topologies are basically </span><span class="c3">providing direct paths for gradients to flow from the loss at the end of the network more easily into all the different layers</span><span class="c1">&nbsp;of the network. </span></li></ul><ul class="c2 lst-kix_hhjcx5eamba3-1 start"><li class="c5 li-bullet-0"><span>So I think that, again, this idea of </span><span class="c3 c42">managing gradient flow</span><span class="c1">&nbsp;properly in your CNN Architectures is something that we&#39;ve really seen a lot more in the last couple of years. And will probably see more moving forward as more exotic architectures are invented. </span></li></ul><h3 class="c0" id="h.cacmll660glk"><span class="c21">Recap: plot: flops vs #params vs runtime</span></h3><p class="c10"><span>We also saw this kind of nice </span><span class="c18 c3">plot</span><span>, plotting </span><span class="c18 c3">performance of the number of flops versus the number of parameters versus the run time</span><span class="c1">&nbsp;of these various models. And there&#39;s some interesting characteristics that you can dive in and see from this plot: </span></p><ul class="c2 lst-kix_3vkdlyb1qfl7-0 start"><li class="c10 c15 li-bullet-0"><span>One idea is that </span><span class="c3">VGG and AlexNet</span><span>&nbsp;have a </span><span class="c3">huge number of parameters</span><span>, and these parameters actually come </span><span class="c3">almost entirely from the fully connected layers</span><span class="c1">&nbsp;of the models. </span></li></ul><ul class="c2 lst-kix_3vkdlyb1qfl7-1 start"><li class="c5 li-bullet-0"><span class="c1">So AlexNet has something like roughly 62 million parameters, and if you look at that last fully connected layer, the final fully connected layer in AlexNet is going from an activation volume of six by six by 256 into this fully connected vector of 496. </span></li><li class="c5 li-bullet-0"><span class="c1">So if you imagine what the weight matrix needs to look like at that layer, the weight matrix is gigantic. It&#39;s number of entries is six by six, six times six times 256 times 496. And if you multiply that out, you see that that single layer has 38 million parameters. </span></li><li class="c5 li-bullet-0"><span>So </span><span class="c3">more than half of the parameters of the entire AlexNet model are just sitting in that last fully connected layer</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And if you add up all the parameters in just the fully connected layers of AlexNet, including these other fully connected layers, you see something like </span><span class="c3">59 of the 62 million parameters in AlexNet are sitting in these fully connected layers</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_3vkdlyb1qfl7-0"><li class="c10 c15 li-bullet-0"><span>So then when we move to other architectures, like </span><span class="c3">GoogLeNet and ResNet</span><span>, they </span><span class="c3">do away with a lot of these large fully connected layers</span><span>&nbsp;in favor of </span><span class="c3">global average pooling at the end</span><span class="c1">&nbsp;of the network. And this allows these nicer architectures, to really cut down the parameter count in these architectures.</span></li></ul><h2 class="c20" id="h.b5v9fktzvftw"><span class="c34">[ST] Classification + Localization</span></h2><ul class="c2 lst-kix_xlk5poyb1w7k-0 start"><li class="c10 c15 li-bullet-0"><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://youtu.be/nDPWywWRIRo?list%3DPLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk%26t%3D1964&amp;sa=D&amp;source=editors&amp;ust=1642006108611094&amp;usg=AOvVaw0g4n0sXJp9zvNaRXMllJX1">ab 32:44</a></span><span>&nbsp;(Lecture 11 | Detection and Segmentation)</span></li><li class="c10 c15 li-bullet-0"><span>So the next task that I want to talk about is this idea of </span><span class="c3 c12">classification plus localization</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span>So we&#39;ve talked about </span><span class="c3 c12">image classification</span><span class="c1">&nbsp;a lot where we want to just assign a category label to the input image but sometimes you might want to know a little bit more about the image. </span></li><li class="c10 c15 li-bullet-0"><span>In addition to predicting </span><span class="c3">what</span><span>&nbsp;the category is, in this case the cat, you might also want to know </span><span class="c3">where</span><span class="c1">&nbsp;is that object in the image? </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1 start"><li class="c5 li-bullet-0"><span>So in addition to predicting the </span><span class="c3">category label</span><span>&nbsp;cat, you might also want to draw a </span><span class="c3">bounding box</span><span class="c1">&nbsp;around the region of the cat in that image. </span></li></ul><h3 class="c0" id="h.ot62j4q3zqub"><span class="c21">C&amp;L vs Object detection</span></h3><ul class="c2 lst-kix_xlk5poyb1w7k-0"><li class="c10 c15 li-bullet-0"><span>And classification plus localization, the distinction here between this and </span><span class="c3 c12">object detection</span><span>&nbsp;is that in the localization scenario you assume ahead of time that you know there&#39;s </span><span class="c3">exactly one object in the image</span><span>&nbsp;that you&#39;re looking for or maybe more than one but you know ahead of time that we&#39;re going to make some classification decision about this image and we&#39;re going to produce </span><span class="c3">exactly one bounding box</span><span class="c1">&nbsp;that&#39;s going to tell us where that object is located in the image so we sometimes call that task classification plus localization.</span></li></ul><h3 class="c0" id="h.pl13snvvm9l4"><span class="c21">Basic architecture </span></h3><ul class="c2 lst-kix_xlk5poyb1w7k-0"><li class="c10 c15 li-bullet-0"><span class="c1">And again, we can reuse a lot of the same machinery that we&#39;ve already learned from image classification in order to tackle this problem. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">So kind of a basic architecture for this problem looks something like this: </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1 start"><li class="c5 li-bullet-0"><span>So again, we have our input image, we feed our input image through some giant convolutional network, this is Alex, this is </span><span class="c3">AlexNet</span><span class="c1">&nbsp;for example, which will give us some final vector summarizing the content of the image.</span></li><li class="c5 li-bullet-0"><span class="c1">[2 fully connected layers:] &nbsp;</span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2 start"><li class="c10 c27 li-bullet-0"><span>Then just like before we&#39;ll have some </span><span class="c3">fully connected layer</span><span>&nbsp;that goes from that final vector to our </span><span class="c3 c12">class scores</span><span class="c1">. </span></li><li class="c10 c27 li-bullet-0"><span>But now we&#39;ll also have </span><span class="c3">another fully connected layer</span><span>&nbsp;that goes from that vector to four numbers. Where the four numbers are something like the </span><span class="c3 c12">height</span><span>, the </span><span class="c3 c12">width</span><span>, and the </span><span class="c3 c12">x and y positions </span><span>of that </span><span class="c3 c12">bounding box</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1"><li class="c5 li-bullet-0"><span>And now our network will produce these </span><span class="c3">two different outputs</span><span class="c1">, </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">one is this set of class scores, and </span></li><li class="c10 c27 li-bullet-0"><span>the other are these four numbers giving the </span><span class="c3">coordinates of the bounding box</span><span class="c1">&nbsp;in the input image. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1"><li class="c5 li-bullet-0"><span>And now </span><span class="c3">during training time</span><span>, when we train this network we&#39;ll actually have </span><span class="c22 c3"><a class="c11" href="#id.ivyfqm9p8urc">two losses</a></span><span>&nbsp;so in this scenario we&#39;re sort of assuming a fully supervised setting so we assume that each of our </span><span class="c3">training images</span><span>&nbsp;is annotated with both a </span><span class="c3">category label</span><span>&nbsp;and also a </span><span class="c3">ground truth bounding box</span><span class="c1">&nbsp;for that category in the image. </span></li><li class="c5 li-bullet-0"><span>So now we have </span><span class="c3">two loss functions</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2 start"><li class="c10 c27 li-bullet-0"><span>We have our favorite </span><span class="c18 c3">softmax loss</span><span>&nbsp;that we compute using the </span><span class="c3 c54">ground truth category label</span><span>&nbsp;and the </span><span class="c3 c54">predicted class scores</span><span class="c1">, and </span></li><li class="c10 c27 li-bullet-0"><span>we also have some kind of loss that gives us some measure of dissimilarity between our </span><span class="c3 c54">predicted coordinates for the bounding box</span><span>&nbsp;and our </span><span class="c3 c54">actual coordinates for the bounding box</span><span>. So one very simple thing is to just take an </span><span class="c18 c3">L2 loss</span><span>&nbsp;between those two and that&#39;s kind of the simplest thing that you&#39;ll see in practice although sometimes people play around with this and maybe use </span><span class="c18 c3">L1</span><span>&nbsp;or </span><span class="c18 c3">smooth L1</span><span>&nbsp;or they </span><span class="c3">parametrize the bounding box a little bit differently</span><span class="c1">&nbsp;but the idea is always the same, that you have some regression loss between your predicted bounding box coordinates and the ground truth bounding box coordinates. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-0"><li class="c10 c15 li-bullet-0"><span class="c1">Questions: </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1 start"><li class="c5 li-bullet-0"><span class="c1">Is this a good idea to do all at the same time? Like what happens if you misclassify, should you even look at the box coordinates? </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">So sometimes people get fancy with it, so in general it works okay. It&#39;s not a big problem, you can actually train a network to do both of these things at the same time and it&#39;ll figure it out </span></li><li class="c10 c27 li-bullet-0"><span class="c3 c12">but</span><span class="c1">&nbsp;sometimes things can get tricky in terms of misclassification. </span></li><li class="c10 c27 li-bullet-0"><span class="c1">So, sometimes what you&#39;ll see for example is that rather than predicting a single box you might </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-3 start"><li class="c10 c35 li-bullet-0"><span>make </span><span class="c3">a separate prediction of the box for each category</span><span class="c1">&nbsp;and </span></li><li class="c10 c35 li-bullet-0"><span class="c1">then only apply loss to the predicted box corresponding to the ground truth category. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2"><li class="c10 c27 li-bullet-0"><span class="c1">So people do get a little bit fancy with these things that sometimes helps a bit in practice. But at least this basic setup, it might not be perfect or it might not be optimal but it will work and it will do something. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1"><li class="c5 li-bullet-0"><span class="c1">Do these losses have different units, do they dominate the gradient? </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2 start"><li class="c10 c27 li-bullet-0"><span>So this is what we call a </span><a id="id.ivyfqm9p8urc"></a><span class="c3 c12">multi-task loss</span><span>&nbsp;so whenever we&#39;re taking derivatives we always want to take derivative of a scalar with respect to our network parameters and use that derivative to take gradient steps. But now we&#39;ve got two scalars that we want to both minimize so what you tend to do </span><span class="c3">in practice</span><span>&nbsp;is have some </span><span class="c3">additional hyperparameter</span><span>&nbsp;that gives you some weighting between these two losses so you&#39;ll take a weighted sum of these two different loss functions to give our final scalar loss. And then you&#39;ll take your gradients with respect to this </span><span class="c3">weighted sum of the two losses</span><span>. And this ends up being really really tricky because this weighting parameter is a hyperparameter that you need to set but it&#39;s kind of different from some of the other hyperparameters that we&#39;ve seen so far in the past right because this </span><span class="c3">weighting hyperparameter actually changes the value of the loss function.</span><span>&nbsp;So, one thing that you might often look at when you&#39;re trying to set hyperparameters is you might make different hyperparameter choices and see what happens to the loss under different choices of hyperparameters. But in this case because the loss actually, because the hyperparameter affects the absolute value of the loss making those comparisons becomes kind of tricky. So </span><span class="c3">setting that hyperparameter</span><span class="c1">&nbsp;is somewhat difficult. And in practice, you kind of need to take it on a case by case basis for exactly the problem you&#39;re solving but my general strategy for this is to have some other metric of performance that you care about other than the actual loss value which then you actually use that final performance metric to make your cross validation choices rather than looking at the value of the loss to make those choices. </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-1"><li class="c5 li-bullet-0"><span class="c1">Why do we do this all at once? Why not do this separately? Why don&#39;t we fix the big network and then just only learn separate fully connected layers for these two tasks? </span></li></ul><ul class="c2 lst-kix_xlk5poyb1w7k-2 start"><li class="c10 c27 li-bullet-0"><span>People do do that sometimes and in fact that&#39;s probably the first thing you should try if you&#39;re faced with a situation like this but in general </span><span class="c3">whenever you&#39;re doing transfer learning you always get better performance if you fine tune the whole system jointly</span><span>&nbsp;because there&#39;s probably some mismatch between the features, if you train on ImageNet and then you use that network for your data set you&#39;re going to get better performance on your data set if you can also change the network. But </span><span class="c3">one trick</span><span class="c1">&nbsp;you might see in practice sometimes is that you might freeze that network then train those two things separately until convergence and then after they converge then you go back and jointly fine tune the whole system. So that&#39;s a trick that sometimes people do in practice in that situation. And as I&#39;ve kind of alluded to this big network is often a pre-trained network that is taken from ImageNet for example.</span></li></ul><h3 class="c0" id="h.3xziiwx3q24k"><span class="c21">Human Pose estimation</span></h3><ul class="c2 lst-kix_otwzvwudtuep-0 start"><li class="c10 c15 li-bullet-0"><span>So a bit of an aside, this </span><span class="c3 c54">idea of predicting some fixed number of positions in the image</span><span>&nbsp;can be applied to a lot of different problems beyond just classification plus localization (vgl. </span><span class="c22"><a class="c11" href="#id.3xj1dem1a8da">Stichpunkt</a></span><span>). One kind of cool example is </span><span class="c3 c12">human pose estimation</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span>So here we want to take an </span><span class="c3">input</span><span>&nbsp;image is a </span><span class="c3">picture of a person</span><span>. We want to </span><span class="c3">output</span><span>&nbsp;the </span><span class="c3">positions of the joints</span><span class="c1">&nbsp;for that person and this actually allows the network to predict what is the pose of the human. </span></li></ul><ul class="c2 lst-kix_otwzvwudtuep-1 start"><li class="c5 li-bullet-0"><span>Where are his arms, where are his legs, stuff like that, and generally most </span><span class="c3">people have the same number of joints</span><span>. That&#39;s a bit of a simplifying </span><span class="c3">assumption</span><span class="c1">, it might not always be true but it works for the network. </span></li><li class="c5 li-bullet-0"><span>So for example one parameterization that you might see in some data sets is define a person&#39;s pose by </span><span class="c3">14 joint positions</span><span class="c1">. Their feet and their knees and their hips and something like that </span></li><li class="c5 li-bullet-0"><span>and now when we train the network then we&#39;re going to input this image of a person and now we&#39;re going to output 14 numbers in this case giving the </span><span class="c3">x and y coordinates for each of those 14 joints</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And then you </span><span class="c3">apply some kind of regression loss on each of those 14</span><span class="c1">&nbsp;different predicted points and just train this network with back propagation again. </span></li><li class="c5 li-bullet-0"><span>Yeah, so you might see an </span><span class="c3">L2 loss</span><span>&nbsp;but people play around with </span><span class="c3">other regression losses</span><span class="c1">&nbsp;here as well. </span></li></ul><ul class="c2 lst-kix_otwzvwudtuep-0"><li class="c10 c15 li-bullet-0"><span class="c1">Questions: </span></li></ul><ul class="c2 lst-kix_otwzvwudtuep-1 start"><li class="c5 li-bullet-0"><span>What do I mean when I say </span><span class="c3 c12">regression loss</span><span class="c1">? </span></li></ul><ul class="c2 lst-kix_otwzvwudtuep-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">So I mean something other than cross entropy or softmax right. When I say regression loss I usually mean like an L2 Euclidean loss or an L1 loss or sometimes a smooth L1 loss. </span></li><li class="c10 c27 li-bullet-0"><span>But in general classification versus regression is whether your </span><span class="c3 c12">output</span><span>&nbsp;is </span><span class="c3 c12">categorical</span><span>&nbsp;or </span><span class="c3 c12">continuous</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_otwzvwudtuep-3 start"><li class="c10 c35 li-bullet-0"><span>so if you&#39;re expecting a </span><span class="c18 c3 c12">categorical output</span><span>&nbsp;like you ultimately want to make a classification decision over some fixed number of categories then you&#39;ll think about a </span><span class="c3">cross entropy loss</span><span>, </span><span class="c3">softmax loss</span><span>&nbsp;or these </span><span class="c3">SVM margin type losses</span><span class="c1">&nbsp;that we talked about already in the class. </span></li><li class="c10 c35 li-bullet-0"><span>But if your expected output is to be some </span><span class="c18 c3 c12">continuous value</span><span>, in this case the position of these points, then your output is continuous so you tend to use different types of losses in those situations. Typically an </span><span class="c3">L2, L1,</span><span class="c1">&nbsp;different kinds of things there. So sorry for not clarifying that earlier. </span></li></ul><a id="id.3xj1dem1a8da"></a><ul class="c2 lst-kix_otwzvwudtuep-0"><li class="c10 c15 li-bullet-0"><span>But the bigger point here is that for </span><span class="c3 c54">any time you know that you want to make some fixed number of outputs</span><span>&nbsp;from your network, (if you know for example. Maybe you knew that you always are going to have pictures of a cat and a dog and you want to predict both the bounding box of the cat and the bounding box of the dog in that case you&#39;d know that you have a </span><span class="c3 c54">fixed number of outputs for each input</span><span>&nbsp;so) you might imagine hooking up this type of </span><span class="c3 c54">regression classification plus localization framework</span><span class="c1">&nbsp;for that problem as well.</span></li></ul><h2 class="c20" id="h.eoibu0d25kmu"><span class="c34">[ST] Object detection</span></h2><ul class="c2 lst-kix_vuu595jt9mz7-0 start"><li class="c10 c15 li-bullet-0"><span>The idea in </span><span class="c3 c12">object detection</span><span>&nbsp;is that we again start with some </span><span class="c3">fixed set of categories</span><span>&nbsp;that we care about, maybe cats and dogs and fish or whatever but some fixed set of categories that we&#39;re interested in. And now our task is that given our input image, every time one of those categories appears in the image, we want to </span><span class="c3">draw a box around it</span><span>&nbsp;and we want to </span><span class="c3">predict the category of that box</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-1 start"><li class="c5 li-bullet-0"><span>so this is different from </span><span class="c3 c12">classification plus localization</span><span>&nbsp;because there might be a </span><span class="c3">varying number of outputs</span><span class="c1">&nbsp;for every input image. You don&#39;t know ahead of time how many objects you expect to find in each image so that&#39;s, this ends up being a pretty challenging problem. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-0"><li class="c10 c15 li-bullet-0"><span>So we&#39;ve seen </span><span class="c3">graphs</span><span>, so this is kind of interesting. We&#39;ve seen this graph many times of the ImageNet classification performance as a function of years and we saw that it just got better and better every year and there&#39;s been a similar </span><span class="c3">trend with object detection</span><span class="c1">&nbsp;because object detection has again been one of these core problems in computer vision that people have cared about for a very long time. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-1 start"><li class="c5 li-bullet-0"><span>So this slide is due to Ross Girshick who&#39;s worked on this problem a lot and it shows the progression of object detection performance on this one particular data set called </span><span class="c3 c12">PASCAL VOC</span><span class="c1">&nbsp;which has been relatively used for a long time in the object detection community. </span></li><li class="c5 li-bullet-0"><span>And you can see that up until about 2012 performance on object detection started to stagnate and slow down a little bit and then in </span><span class="c3">2013</span><span>&nbsp;was when some of the </span><span class="c3">first deep learning approaches to object detection</span><span class="c1">&nbsp;came around and you could see that performance just shot up very quickly getting better and better year over year. </span></li><li class="c5 li-bullet-0"><span>One thing you might notice is that this plot ends in 2015 and it&#39;s actually continued to go up since then so the current state of the art in this data set is well over 80 and in fact a lot of recent papers don&#39;t even report results on this data set anymore because it&#39;s considered too easy. So it&#39;s a little bit hard to know, I&#39;m not actually sure what is the </span><span class="c3">state of the art</span><span class="c1">&nbsp;number on this data set but it&#39;s off the top of this plot.</span></li></ul><a id="id.ydmhn25ztlxf"></a><ul class="c2 lst-kix_vuu595jt9mz7-0"><li class="c10 c15 li-bullet-0"><span class="c1">Okay, so as I already said this is different from localization because there might be differing numbers of objects for each image. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-1 start"><li class="c5 li-bullet-0"><span>So for example in this cat on the upper left there&#39;s only </span><span class="c3">one object</span><span>&nbsp;so we only need to </span><span class="c3">predict four numbers</span><span>&nbsp;but now for this image in the middle there&#39;s </span><span class="c3">three animals</span><span>&nbsp;there so we need our network to </span><span class="c3">predict 12 numbers</span><span class="c1">, four coordinates for each bounding box. Or in this example of many many ducks then you want your network to predict a whole bunch of numbers. Again, four numbers for each duck. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-0"><li class="c10 c15 li-bullet-0"><span>So as a result [of </span><span class="c22"><a class="c11" href="#id.ydmhn25ztlxf">Stichpunkt</a></span><span class="c1">], it&#39;s kind of tricky if you want to think of object detection as a regression problem. So instead, people tend to work, use kind of a different paradigm when thinking about object detection. </span></li><li class="c10 c15 li-bullet-0"><span>So one approach that&#39;s very common and has been used for a long time in computer vision is this idea of </span><span class="c3 c12">sliding window approaches</span><span class="c1">&nbsp;to object detection. So this is kind of similar to this idea of taking small patches and applying that for semantic segmentation and we can apply a similar idea for object detection. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-1 start"><li class="c5 li-bullet-0"><span class="c1">So the idea is that we&#39;ll </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">take different crops from the input image, in this case we&#39;ve got this crop in the lower left hand corner of our image and now we take that crop, </span></li><li class="c10 c27 li-bullet-0"><span class="c1">feed it through our convolutional network and our convolutional network does a classification decision on that input crop. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-3 start"><li class="c10 c35 li-bullet-0"><span>It&#39;ll say that there&#39;s no dog here, there&#39;s no cat here, and then in addition to the categories that we care about we&#39;ll add an </span><span class="c3">additional category called </span><span class="c3 c12">background</span><span class="c1">&nbsp;and now our network can predict background in case it doesn&#39;t see any of the categories that we care about, </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-1"><li class="c5 li-bullet-0"><span>[</span><span class="c3 c12">Problem:</span><span>] so then when we take this crop from the lower left hand corner here then our network would hopefully predict background and say that no, there&#39;s no object here. Now if we take a different crop then our network would predict dog yes, cat no, background no. We take a different crop we get dog yes, cat no, background no. Or a different crop, dog no, cat yes, background no. Does anyone see a </span><span class="c3">problem</span><span>&nbsp;here? Yeah, the question is </span><span class="c3">how do you choose the crops</span><span class="c1">? So this is a huge problem right. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">Because there could be </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-4 start"><li class="c10 c39 li-bullet-0"><span class="c3">any number of objects</span><span class="c1">&nbsp;in this image, </span></li><li class="c10 c39 li-bullet-0"><span>these objects could appear at </span><span class="c3">any location</span><span class="c1">&nbsp;in the image,</span></li><li class="c10 c39 li-bullet-0"><span>these objects could appear at </span><span class="c3">any size</span><span class="c1">&nbsp;in the image,</span></li><li class="c10 c39 li-bullet-0"><span>these objects could also appear at </span><span class="c3">any aspect ratio</span><span class="c1">&nbsp;in the image, </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-3"><li class="c10 c35 li-bullet-0"><span>so if you want to do kind of a brute force sliding window approach you&#39;d </span><span class="c3">end up having to test thousands, tens of thousands</span><span>, many many many many different </span><span class="c3">crops</span><span>&nbsp;in order to tackle this problem with a brute force sliding window approach. And in the case where every one of those crops is going to be </span><span class="c3">fed through a giant convolutional network</span><span>, this would be completely </span><span class="c3 c12">computationally intractable</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_vuu595jt9mz7-1"><li class="c5 li-bullet-0"><span class="c1">So in practice people don&#39;t ever do this sort of brute force sliding window approach for object detection using convolutional networks.</span></li></ul><h2 class="c20" id="h.z1jna3hysj5d"><span class="c34">[ST] Region-based methods for object detection</span></h2><h3 class="c0" id="h.hhfjeyxz5byb"><span class="c21">R-CNN</span></h3><ul class="c2 lst-kix_mwssa4aj5yrr-0 start"><li class="c10 c15 li-bullet-0"><span>Instead there&#39;s this cool line of work called </span><span class="c3 c12">region proposals</span><span class="c1">&nbsp;that comes from, this is not using deep learning typically. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>These are slightly </span><span class="c3">more traditional computer vision techniques</span><span class="c1">&nbsp;but the idea is that a region proposal network kind of uses more traditional signal processing, image processing type things to make some list of proposals </span></li><li class="c5 li-bullet-0"><span>given an input image, a region proposal network will then give you something like a thousand boxes where an object might be present. We look for edges in the image and try to draw boxes that contain closed edges or something like that. These are various types of image processing approaches, but these region proposal networks will basically </span><span class="c3">look for blobby regions</span><span>&nbsp;in our input image and </span><span class="c3">then give us some set of candidate proposal regions</span><span>&nbsp;where objects might be potentially found. </span></li><li class="c5 li-bullet-0"><span>And these are </span><span class="c3">relatively fast-ish to run</span><span class="c1">&nbsp;so </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c3">one common example</span><span>&nbsp;of a region proposal method: that you might see is something called </span><span class="c3 c12">Selective Search</span><span class="c1">&nbsp;which I think actually gives you 2000 region proposals, not the 1000 that it says on the slide. So you kind of run this thing and then after about two seconds of turning on your CPU it&#39;ll spit out 2000 region proposals in the input image where objects are likely to be found </span></li><li class="c10 c27 li-bullet-0"><span>so there&#39;ll be a lot of noise in those. Most of them will not be true objects but there&#39;s a </span><span class="c3">pretty high recall</span><span class="c1">. If there is an object in the image then it does tend to get covered by these region proposals from Selective Search. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So now rather than applying our classification network to every possible location and scale in the image instead what we can do is first apply one of these region proposal networks to get some set of proposal regions where objects are likely located and now </span><span class="c3">apply a convolutional network for classification to each of these proposal regions</span><span>&nbsp;and this will end up being </span><span class="c3">much more computationally tractable</span><span class="c1">&nbsp;than trying to do all possible locations and scales. </span></li><li class="c10 c15 li-bullet-0"><span>And this idea all came together in this paper called </span><span class="c3 c12">R-CNN</span><span class="c1">&nbsp;from a few years ago that does exactly that. </span></li><li class="c10 c15 li-bullet-0"><span>So given our input image in this case we&#39;ll run some region proposal network to get our proposals, these are also sometimes called </span><span class="c3 c12">regions of interest</span><span>&nbsp;or </span><span class="c3 c12">ROI&#39;s</span><span class="c1">&nbsp;so again Selective Search gives you something like 2000 regions of interest. </span></li><li class="c10 c15 li-bullet-0"><span>Now one of the problems here is that these input, these regions in the input image could have different sizes but if we&#39;re going to run them all through a convolutional network our classification, our </span><span class="c3">convolutional networks for classification all want images of the same input size</span><span>&nbsp;typically due to the fully connected net layers and whatnot so we need to take each of these region proposals and warp them to that fixed square size that is expected as input to our downstream network. So we&#39;ll </span><span class="c3">crop out those regions</span><span>&nbsp;corresponding to the region proposals, we&#39;ll </span><span class="c3">warp them to that fixed size</span><span class="c1">&nbsp;</span></li><li class="c10 c15 li-bullet-0"><span class="c3">run each of them through a convolutional network</span><span class="c1">&nbsp;which will then </span></li><li class="c10 c15 li-bullet-0"><span class="c3">use in this case an SVM to make a classification decision for each of those</span><span class="c1">, to predict categories for each of those crops. </span></li><li class="c10 c15 li-bullet-0"><span>And then in addition </span><span class="c3">R-CNN also predicts a regression, like a correction to the bounding box in addition for each of these input region proposals</span><span>&nbsp;because the problem is that your input region proposals are kind of generally in the right position for an object but they might not be perfect so in addition R-CNN will, in addition to category labels for each of these proposals, it&#39;ll also predict four numbers that are kind of an offset or a correction to the box that was predicted at the region proposal stage. So then again, </span><span class="c3">this is a multi-task loss</span><span>&nbsp;and </span><span class="c3">you would train this whole thing</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">questions: </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">How much does the change in aspect ratio impact accuracy? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">It&#39;s a little bit hard to say. I think there&#39;s some controlled experiments in some of these papers but I&#39;m not sure I can give a generic answer to that. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">Is it necessary for regions of interest to be rectangles? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">So they typically are because it&#39;s tough to warp these non-region things but once you move to something like instant segmentation then you sometimes get proposals that are not rectangles. If you actually do care about predicting things that are not rectangles. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">Are the region proposals learned? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">so in R-CNN it&#39;s a traditional thing. These are not learned, this is kind of some fixed algorithm that someone wrote down but we&#39;ll see in a couple minutes that we can actually, we&#39;ve changed that a little bit in the last couple of years. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">Is the offset always inside the region of interest? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">The answer is no, it doesn&#39;t have to be. You might imagine that suppose the region of interest put a box around a person but missed the head then you could imagine the network inferring that oh this is a person but people usually have heads so the network showed the box should be a little bit higher. So sometimes the final predicted boxes will be outside the region of interest. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">What is the output of the SVM, if there is no object in the ROI?</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">You have a lot of ROI&#39;s that don&#39;t correspond to true objects. And like we said, in addition to the classes that you actually care about you add an additional background class so your class scores can also predict background to say that there was no object here. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">What kind of data do we need?</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">This is fully supervised in the sense that our training data has each image, consists of images. Each image has all the object categories marked with bounding boxes for each instance of that category. There are definitely papers that try to approach this like oh what if you don&#39;t have the data. What if you only have that data for some images? Or what if that data is noisy but at least in the generic case you assume full supervision of all objects in the images at training time.</span></li></ul><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 338.67px;"><img alt="" src="images/image58.png" style="width: 601.70px; height: 338.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp;</span></p><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Okay, so I think we&#39;ve kind of alluded to this but there&#39;s kind of </span><span class="c3">a lot of problems with this R-CNN framework</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>this is kind of </span><span class="c3">still computationally pretty expensive</span><span>&nbsp;because if we&#39;ve got 2000 region proposals, we&#39;re running each of those proposals independently [through </span><span class="c3 c71">the</span><span class="c1">&nbsp;CNN] [note: use the same CNN for all ROIs in the input image [unterschiedliche CNN f&uuml;r jeden ROI macht keinen Sinn, da alle ROIs unterschiedliche Pos und Form haben]], that can be pretty expensive.</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>[ expensive weil wir </span><span class="c3 c54">jeden ROI einzeln</span><span>&nbsp;durch das CNN schicken und </span><span class="c3 c71">nicht weil wir viele CNNs haben!</span><span class="c1">&nbsp;]</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-3 start"><li class="c10 c35 li-bullet-0"><span>Runtime &#x1d6c2; Number of [expensive] forward and backward passes &#x1d6c2; &nbsp;</span><span class="c3 c54">#ROIs *</span><span class="c1">&nbsp;#input_images</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>There&#39;s also this question of relying on this </span><span class="c3">fixed region proposals</span><span>, we&#39;re </span><span class="c3">not learning them</span><span class="c1">&nbsp;so that&#39;s kind of a problem. </span></li><li class="c5 li-bullet-0"><span>[</span><span class="c18 c3">At training time:</span><span>] And just in practice it ends up being </span><span class="c3">pretty slow</span><span>&nbsp;so in the original implementation R-CNN would actually dump all the features to disk so it&#39;d take </span><span class="c3">hundreds of gigabytes of disk space</span><sup class="c3"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c3">&nbsp;to store all these features</span><span>. [nachdem ROI durch das CNN durch ist, wird das Ergebnis dieser &ldquo;input_image x Matrix x &hellip; x Matrix&rdquo;-multiplikation gespeichert </span><span class="c3">f&uuml;r alle ROIs</span><span>. Diese Ergebnisse werden dann n&auml;mlich benutzt, um die </span><span class="c3">post-hoc</span><sup class="c3"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span>&nbsp;SVM</span><span class="c25 c3 c12">s</span><sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span>&nbsp;zu trainieren (s. &ldquo;cached region features&rdquo; auf </span><span class="c22"><a class="c11" href="#id.vmf8rtvg7abk">slide</a></span><span>)! ] Then </span><span class="c3">training would be super slow</span><span>&nbsp;since you have to make all these different forward and backward passes through the image and it took something like 84 hours is one number they&#39;ve recorded for training time so this is super super slow. </span><a id="id.fnzd99e9kkj2"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 474.37px; height: 267.13px;"><img alt="" src="images/image85.png" style="width: 474.37px; height: 267.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><a id="id.vmf8rtvg7abk"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 473.24px; height: 266.49px;"><img alt="" src="images/image50.png" style="width: 473.24px; height: 266.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup></li></ul><p class="c10 c44"><span>Die positive/negative Einteilung sieht beim </span><span class="c3">dog</span><span>&nbsp;SVM nat&uuml;rlich anders aus:</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 488.53px; height: 275.10px;"><img alt="" src="images/image74.png" style="width: 488.53px; height: 275.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 490.95px; height: 276.47px;"><img alt="" src="images/image35.png" style="width: 490.95px; height: 276.47px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><a id="id.62hwt07x920b"></a></p><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>And now </span><span class="c18 c3">at test time</span><span class="c3">&nbsp;[&ldquo;inference&rdquo; time] it&#39;s also super slow</span><span>, something like roughly 30 seconds </span><span class="c25">per image</span><span class="c1">&nbsp;because you need to run thousands [2000 in der original implementation] of forward passes through the convolutional network for each of these region proposals so this ends up being pretty slow.</span></li></ul><h3 class="c0" id="h.3uhx93bq6h8n"><span>Fast R-CNN</span></h3><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 338.67px;"><img alt="" src="images/image63.png" style="width: 601.70px; height: 338.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp;</span><a id="id.5jaq8zvvgan2"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 338.67px;"><img alt="" src="images/image80.png" style="width: 601.70px; height: 338.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Thankfully we have </span><span class="c3 c12">fast R-CNN</span><span class="c1">&nbsp;that fixed a lot of these problems so when we do fast R-CNN then it&#39;s going to look kind of the same. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>We&#39;re going to start with our input image but now rather than processing each region of interest separately </span><span class="c3">instead we&#39;re going to run the entire image through some convolutional layers all at once</span><span>&nbsp;to give this high resolution convolutional </span><span class="c3">feature map corresponding to the entire image</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And now we </span><span class="c3 c48">still are using</span><span class="c3">&nbsp;some region proposals from some fixed thing like </span><span class="c3 c48">Selective Search</span><span>&nbsp;but rather than cropping out the pixels of the image corresponding to the region proposals, instead we imagine </span><span class="c3">projecting those region proposals onto this convolutional feature map and then taking crops from the convolutional feature map corresponding to each proposal</span><span class="c1">&nbsp;rather than taking crops directly from the image. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>And this allows us to </span><span class="c3">reuse a lot of this expensive convolutional computation</span><span class="c1">&nbsp;across the entire image when we have many many crops per image. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>But again, if we have some fully connected layers downstream those </span><span class="c3">fully connected layers are expecting some fixed-size input [a.k.a. low-resolution input]</span><span>&nbsp;so now we need to do some </span><span class="c3">reshaping of those crops from the convolutional feature map</span><span>&nbsp;and they do that in a differentiable way using something they call an </span><span class="c3 c12">ROI pooling layer</span><sup class="c3 c12"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup><span class="c1">. (This ROI pooling, it looks kind of like max pooling. I don&#39;t really want to get into the details of that right now.)</span></li><li class="c5 li-bullet-0"><span>Once you have these </span><span class="c3">warped crops</span><span>&nbsp;from the convolutional feature map then you can </span><span class="c3">run these things through some fully connected layers</span><span class="c1">&nbsp;and predict your classification scores and your linear regression offsets to the bounding boxes. </span></li><li class="c5 li-bullet-0"><span>And now when we train this thing then we again have a </span><span class="c3">multi-task loss</span><span>&nbsp;that trades off between these two constraints and during back propagation we can back prop through this entire thing and </span><span class="c3">learn it all jointly</span><sup class="c3"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>And in terms of </span><span class="c3">speed</span><span>&nbsp;if we look at </span><span class="c3">R-CNN versus fast R-CNN versus this other model called </span><span class="c3 c12">SPP net</span><span class="c1">&nbsp;which is kind of in between the two, </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>then you can see that </span><span class="c18 c3">at training time</span><span class="c3">&nbsp;fast R-CNN is something like 10 times faster to train</span><span class="c1">&nbsp;because we&#39;re sharing all this computation between different feature maps. </span></li><li class="c5 li-bullet-0"><span>And now </span><span class="c3">at test time</span><span>&nbsp;fast R-CNN is super fast and in fact </span><span class="c3">fast R-CNN is so fast </span><span class="c18 c3">at test time</span><span class="c3">&nbsp;that its computation time is actually dominated by computing region proposals</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>So we said that computing these 2000 region proposals using Selective Search takes something like two seconds and now once we&#39;ve got all these region proposals then because we&#39;re </span><span class="c3">processing them [the region proposals] all sort of in a shared way by sharing these expensive convolutions across the entire image</span><span class="c1">&nbsp;that we can process all of these region proposals in less than a second altogether. </span></li><li class="c10 c27 li-bullet-0"><span class="c3 c12">Problem:</span><span>&nbsp;So fast R-CNN ends up being </span><span class="c3">bottlenecked by just the computing of these region proposals</span><span class="c1">. </span></li></ul><h3 class="c0" id="h.8b8ug3njkoty"><span>Faster R-CNN</span></h3><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 338.67px;"><img alt="" src="images/image64.png" style="width: 601.70px; height: 338.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Thankfully we&#39;ve solved this problem with </span><span class="c3 c12">faster R-CNN</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>So the idea in faster R-CNN is to just make, so the problem was the computing the region proposals using this fixed function was a bottleneck. So instead we&#39;ll just </span><span class="c3">make the network itself predict its own region proposals</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And so the way that this sort of works is that again, we take our input image, run the entire input image altogether through some convolutional layers to get some convolutional feature map representing the entire high resolution image and </span><span class="c3">now there&#39;s a separate </span><span class="c3 c12">region proposal network (RPN)</span><span class="c3">&nbsp;which works on top of those convolutional features and predicts its own region proposals</span><span class="c1">&nbsp;inside the network. </span></li><li class="c5 li-bullet-0"><span>Now once we have those predicted region proposals </span><span class="c3">then it looks just like fast R-CNN</span><span class="c1">&nbsp;where now we take crops from those region proposals from the convolutional features, pass them up to the rest of the network. </span></li><li class="c5 li-bullet-0"><span>And now we talked about </span><span class="c3">multi-task losses</span><span>&nbsp;and </span><span class="c3">multi-task training networks</span><span>&nbsp;to do multiple things at once. Well now we&#39;re telling the network to do four things all at once so balancing out this </span><span class="c3">four-way multi-task loss</span><span class="c1">&nbsp;is kind of tricky. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>But because the </span><span class="c3">region proposal network needs to do two things</span><span class="c1">:</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">it needs to say for each potential proposal is it an object or not an object, </span></li><li class="c10 c35 li-bullet-0"><span class="c1">it needs to actually regress the bounding box coordinates for each of those proposals, </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2"><li class="c10 c27 li-bullet-0"><span>and now </span><span class="c3">the final network at the end needs to do these two things again</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">Make final classification decisions for what are the class scores for each of these proposals, </span></li><li class="c10 c35 li-bullet-0"><span class="c1">and also have a second round of bounding box regression to again correct any errors that may have come from the region proposal stage. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span class="c1">Questions: </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">So the question is that sometimes multi-task learning might be seen as regularization and are we getting that effect here? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">I&#39;m not sure if there&#39;s been super controlled studies on that but actually in the original version of the faster R-CNN paper they did a little bit of experimentation like what if we share the region proposal network, what if we don&#39;t share? What if we learn separate convolutional networks for the region proposal network versus the classification network? And I think there were minor differences but it wasn&#39;t a dramatic difference either way. So in practice it&#39;s kind of nicer to only learn one because it&#39;s computationally cheaper. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">How do you train this region proposal network because you don&#39;t have ground truth region proposals for the region proposal network? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">So that&#39;s a little bit hairy. I don&#39;t want to get too much into those details but the idea is that at any time you have a region proposal which has more than some threshold of overlap with any of the ground truth objects then you say that that is the positive region proposal and you should predict that as the region proposal and any potential proposal which has very low overlap with any ground truth objects should be predicted as a negative. But there&#39;s a lot of dark magic hyperparameters in that process and that&#39;s a little bit hairy. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">What is the classification loss on the region proposal network? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>and the answer is that it&#39;s making a binary [classification], so I didn&#39;t want to get into too much of the details of that architecture &#39;cause it&#39;s a little bit hairy but it&#39;s making binary decisions. So it has some set of potential regions that it&#39;s considering and it&#39;s making a binary decision for each one. Is this an object or not an object? </span><span class="c3">So it&#39;s like a binary classification loss</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So once you train this thing then faster R-CNN ends up being pretty darn fast. So now </span><span class="c3">because we&#39;ve eliminated this overhead from computing region proposals outside the network</span><span class="c1">, now faster R-CNN ends up being very very fast compared to these other alternatives. </span></li><li class="c10 c15 li-bullet-0"><span>Also, one interesting thing is that because we&#39;re learning the region proposals here you might imagine maybe what if there was some </span><span class="c3">mismatch between this fixed region proposal algorithm and my data</span><span class="c1">? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>So in this case once you&#39;re learning your own region proposals then </span><span class="c3">you can overcome that mismatch</span><span class="c1">&nbsp;if your region proposals are somewhat weird or different than other data sets. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span class="c1">So this whole family of R-CNN methods, R stands for region, so these are all region-based methods because there&#39;s some kind of region proposal and then we&#39;re doing some processing, some independent processing for each of those potential regions. So this whole family of methods are called these region-based methods for object detection. </span></li></ul><h2 class="c20" id="h.51y0nq1hr9s4"><span>[ST] Single Shot methods</span></h2><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>But there&#39;s another family of methods that you sometimes see for object detection which is sort of all feed forward in a single pass. So one of these is </span><span class="c3 c12">YOLO</span><span>&nbsp;for </span><span class="c3 c12">You Only Look Once</span><span>. And another is </span><span class="c3 c12">SSD</span><span>&nbsp;for </span><span class="c3 c12">Single Shot Detection</span><span>&nbsp;and these two came out somewhat around the same time. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 477.88px; height: 269.10px;"><img alt="" src="images/image30.png" style="width: 477.88px; height: 269.10px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp;</span></li><li class="c10 c15 li-bullet-0"><span>But </span><span class="c3">the idea</span><span class="c1">&nbsp;is that rather than doing independent processing for each of these potential regions instead we want to try to treat this like a regression problem and just make all these predictions all at once with some big convolutional network. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>So now given our input image you imagine dividing that input image into some </span><span class="c18 c3">coarse grid</span><span class="c1">, </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">in this case it&#39;s a seven by seven grid </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>and now within each of those grid cells you imagine some </span><span class="c3">set of base bounding boxes</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">Here I&#39;ve drawn three base bounding boxes like a tall one, a wide one, and a square one but in practice you would use more than three. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>So now for each of these grid cells and </span><span class="c3">for each</span><span>&nbsp;of these base </span><span class="c3">bounding boxes</span><span class="c1">&nbsp;you want to predict several things. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>1. One, you want to </span><span class="c3">predict an offset</span><span class="c1">&nbsp;off the base bounding box to predict what is the true location of the object off this base bounding box. </span></li><li class="c10 c27 li-bullet-0"><span>2. And you also want to </span><span class="c3">predict classification scores</span><span class="c1">&nbsp;so maybe a classification score for each of these base bounding boxes. How likely is it that an object of this category appears in this bounding box. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>So then at the end we end up predicting from our input image, we end up predicting this giant tensor of seven by seven grid by </span><span class="c3">5B + C</span><span class="c1">. [ ie tensor shape: 7 x 7 x (5B+C) ]</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>So that&#39;s just where we have B base bounding boxes, we have five numbers for each [bounding box] giving our </span><span class="c3">offset</span><span>&nbsp;[4 values: b_x, b_y, b_w, b_h] and our </span><span class="c3">confidence</span><sup class="c3"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup><sup class="c3"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span class="c3">&nbsp;</span><span class="c1">[1 value] for that base bounding box and C classification scores for our C categories. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span class="c1">So then we kind of see object detection as this input of an image, output of this three dimensional tensor and you can imagine just training this whole thing with a giant convolutional network.</span></li><li class="c5 li-bullet-0"><span>And that&#39;s kind of what these </span><span class="c3">single shot methods</span><span>&nbsp;[gemeint als Oberbegriff f&uuml;r YOLO und SSD] do where they just, and again </span><span class="c42">matching the ground truth objects into these potential base boxes</span><sup class="c42"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span class="c1">&nbsp;becomes a little bit hairy but that&#39;s what these methods do. </span></li></ul><h3 class="c0" id="h.4dd3qvx9qgg6"><span class="c21">YOLOv1 architecture, speed</span></h3><ul class="c2 lst-kix_on37z5v164d5-0 start"><li class="c10 c15 li-bullet-0"><span>The YOLO consists of 24 conv layers and 2 FC layers, of which some conv layers construct ensembles of </span><span class="c3">inception modules</span><span class="c1">&nbsp;with 1 &times; 1 reduction layers followed by 3 &times; 3 conv layers. </span></li><li class="c10 c15 li-bullet-0"><span>The network can process images in real-time at </span><span class="c3">45 FPS</span><span>&nbsp;and a simplified version </span><span class="c3 c12">Fast YOLO</span><span>&nbsp;can reach </span><span class="c3">155 FPS</span><span class="c1">&nbsp;with better results than other real-time detectors. </span></li></ul><h3 class="c0" id="h.f39h6bntwt3u"><span>YOLOv2</span></h3><ul class="c2 lst-kix_on37z5v164d5-0"><li class="c10 c15 li-bullet-0"><span>An improved version, </span><span class="c3">YOLOv2</span><span>, was later proposed in [72], which adopts several impressive strategies, such as BN, anchor boxes, dimension cluster and multi-scale training.</span></li></ul><h3 class="c0" id="h.ikjbpcjjdus3"><span class="c21">Difference YOLO vs SSD </span></h3><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>(</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.05511.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108657650&amp;usg=AOvVaw39BzBYv4EYexDq5O6bnOza">source</a></span><span>)</span></li></ul><h4 class="c14" id="h.onx7zp16frfv"><span class="c17">Problems of YOLO</span></h4><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>YOLO has a </span><span class="c3">difficulty in dealing with small objects in groups</span><span class="c1">, which is caused by strong spatial constraints imposed on bounding box predictions [17]. </span></li><li class="c10 c15 li-bullet-0"><span>Meanwhile, YOLO </span><span class="c3">struggles to generalize to objects in new/unusual aspect ratios/ configurations</span><span class="c1">&nbsp;and </span></li><li class="c10 c15 li-bullet-0"><span class="c3">produces relatively coarse features due to multiple downsampling operations</span><span class="c1">. </span></li></ul><h4 class="c14" id="h.5y3csydenu7a"><span>Solution: SSD</span></h4><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Aiming at these problems, Liu et al. proposed a </span><span class="c3">Single Shot MultiBox Detector (SSD)</span><span class="c1">&nbsp;[71], which was inspired by the anchors adopted in MultiBox [68], RPN [18] and multi-scale representation [95]. </span></li><li class="c10 c15 li-bullet-0"><span>Given a specific feature map, </span><span class="c3">instead of fixed grids adopted in YOLO</span><span class="c1">, the SSD takes advantage of a set of default anchor boxes with different aspect ratios and scales [dh. die BBoxes sind nicht an einer bestimmten Stelle im Bild, sondern werden ganz allgemein (nur &uuml;ber Gr&ouml;&szlig;e und Form, aber nicht location) festgelegt] to discretize the output space of bounding boxes. </span></li><li class="c10 c15 li-bullet-0"><span>To </span><span class="c3">handle objects with various sizes</span><span class="c1">, the network fuses predictions from multiple feature maps with different resolutions. </span></li><li class="c10 c15 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 148.00px;"><img alt="" src="images/image83.png" style="width: 601.70px; height: 148.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span>The architecture of SSD is demonstrated in Figure 10. Given the </span><span class="c3">VGG16 backbone</span><span>&nbsp;architecture, SSD </span><span class="c3">adds several feature layers to the end</span><span>&nbsp;of the network, which are responsible for </span><span class="c3">predicting the offsets to default boxes</span><sup class="c3"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span>&nbsp;with different scales and aspect ratios </span><span class="c3">and their associated confidences</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">The network is trained with a weighted sum of localization loss (e.g. Smooth L1) and confidence loss (e.g. Softmax), which is similar to (1). </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Final detection results are obtained by conducting </span><span class="c3 c12">NMS</span><span class="c1">&nbsp;[=non-max suppression] on multi-scale refined bounding boxes.</span></li></ul><h3 class="c0" id="h.vosqedl8jcjw"><span>Similarity to faster R-CNN [</span><span class="c12">egal bisschen vage</span><span>]</span></h3><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>[Johnson] </span><span>And by the way, the </span><span class="c3">region proposal network</span><span>&nbsp;that gets used in faster R-CNN ends up looking quite similar to these where they have some set of base bounding boxes over some gridded image, </span><span class="c3">another region proposal network does some regression</span><span class="c1">&nbsp;plus some classification. So there&#39;s kind of some overlapping ideas here. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>So </span><span class="c3">in faster R-CNN</span><span class="c1">&nbsp;we&#39;re kind of treating the region proposal step as kind of this fixed end-to-end regression problem and then we do the separate per region processing </span></li><li class="c10 c27 li-bullet-0"><span>but now with these </span><span class="c3">single shot methods</span><span class="c1">&nbsp;we only do that first step [=region proposal step] and just do all of our object detection with a single forward pass. </span></li></ul><h3 class="c0" id="h.hsk7qdk3298r"><span>Similarity to R-CNN</span></h3><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>[</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640v5.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108666183&amp;usg=AOvVaw3sXX27xSa7LNePfnkbZivf">paper</a></span><span class="c1">] YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features.</span></li><li class="c5 li-bullet-0"><span>[Differences:] [</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640v5.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108667298&amp;usg=AOvVaw3BfAA3ehj1mhULX7WydZaT">paper</a></span><span class="c1">]</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>However, our system puts </span><span class="c3">spatial constraints on the grid cell proposals</span><span class="c1">&nbsp;which helps mitigate multiple detections of the same object. </span></li><li class="c10 c27 li-bullet-0"><span>Our system also </span><span class="c3">proposes far fewer bounding boxes</span><span>, only 98 per image compared to about 2000 from </span><span class="c3 c12">Selective Search</span><span class="c1">. </span></li><li class="c10 c27 li-bullet-0"><span>Finally, our system </span><span class="c3">combines these individual components into a single, jointly optimized model</span><span>&nbsp;[bei R-CNN wird Selective Search </span><span class="c3">extern</span><span class="c1">&nbsp;performt, also nicht jointly optimized].</span></li></ul><h3 class="c0" id="h.9f3bli8a28o3"><span class="c21">Fast R-CNN vs YOLO</span></h3><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Second, YOLO reasons globally about the image when making predictions. Unlike sliding window and region proposal-based techniques, </span><span class="c3 c12">YOLO sees the entire image</span><span>&nbsp;during training and test time so it implicitly encodes contextual information about classes as well as their appearance. [</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640v5.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108669024&amp;usg=AOvVaw3ZXHU3gXzr5kBkpdWoRcnu">paper</a></span><span class="c1">]</span></li><li class="c10 c15 li-bullet-0"><span class="c3 c12">Fast R-CNN</span><span>, a top detection method [14], </span><span class="c3">mistakes background patches in an image for objects</span><span>&nbsp;</span><span class="c3 c12">because it can&rsquo;t see the larger context</span><span>. YOLO makes less than half the number of background errors compared to Fast R-CNN. [</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640v5.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108669721&amp;usg=AOvVaw1SRppQ0Un_KfN0_SCQKzG4">paper</a></span><span class="c1">]</span></li><li class="c10 c15 li-bullet-0"><span>Furthermore, YOLO produces </span><span class="c3">fewer false positives on background</span><span>, which makes the </span><span class="c3 c42">cooperation with Fast R-CNN</span><span>&nbsp;[=model combination] become possible. [</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.05511.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108670310&amp;usg=AOvVaw2Ik9zcFbR2qlHW1xO0I0um">paper</a></span><span class="c1">]</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c63">[</span><span class="c22 c63"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02640v5.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108670788&amp;usg=AOvVaw3u35zF5ILAFfSRqudVjMAa">paper</a></span><span class="c63">] YOLO makes far fewer background mistakes than Fast R-CNN. By </span><span class="c3 c42 c63">using YOLO to eliminate background detections from Fast R-CNN</span><span class="c31">&nbsp;we get a significant boost in performance. For every bounding box that R-CNN predicts we check to see if YOLO predicts a similar box. If it does, we give that prediction a boost based on the probability predicted by YOLO and the overlap between the two boxes. </span></li><li class="c5 li-bullet-0"><span class="c31">The best Fast R-CNN model achieves a mAP of 71.8% on the VOC 2007 test set. When combined with YOLO, its mAP increases by 3.2% to 75.0%. </span></li><li class="c5 li-bullet-0"><span class="c31">We also tried combining the top Fast R-CNN model with several other versions of Fast R-CNN. Those ensembles produced small increases in mAP between .3 and .6%, see Table 2 for details. </span></li><li class="c5 li-bullet-0"><span class="c31">The boost from YOLO is not simply a byproduct of model ensembling since there is little benefit from combining different versions of Fast R-CNN. Rather, it is precisely because YOLO makes different kinds of mistakes at test time that it is so effective at boosting Fast R-CNN&rsquo;s performance. </span></li><li class="c5 li-bullet-0"><span class="c63">Unfortunately, this combination doesn&rsquo;t benefit from the speed of YOLO since we run each model seperately and then combine the results. However, since YOLO is so fast it doesn&rsquo;t add any significant computational time compared to Fast R-CNN.</span></li></ul><h2 class="c20" id="h.gamcn36erasm"><span>[ST] Obj det variables &amp; hyperparameters</span></h2><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So </span><span class="c18 c3">object detection</span><span>&nbsp;has a ton of different </span><span class="c18 c3">variables</span><span>:</span><span>&nbsp;There could be different </span><span class="c3 c48">base networks</span><span>&nbsp;like </span><span class="c3">VGG, ResNet,</span><span>&nbsp;we&#39;ve seen different </span><span class="c3 c48">metastrategies</span><span>&nbsp;for object detection including this faster R-CNN type </span><span class="c3">region based</span><span>&nbsp;family of methods, this </span><span class="c3">single shot detection</span><span class="c1">&nbsp;family of methods. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>(There&#39;s kind of a hybrid that I didn&#39;t talk about called </span><span class="c3 c12">R-FCN</span><span class="c1">&nbsp;which is somewhat in between.) </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>There&#39;s a lot of different </span><span class="c3 c48">hyperparameters</span><span>&nbsp;like what is the </span><span class="c3">image size</span><span>, </span><span class="c3">how many region proposals</span><span class="c1">&nbsp;do you use. </span></li><li class="c10 c15 li-bullet-0"><span>And there&#39;s actually this really cool </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.05511.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108673528&amp;usg=AOvVaw36ktijnwFw_VXONTutALv0">paper</a></span><span class="c1">&nbsp;that will appear at CVPR this summer that does a really controlled experimentation around a lot of these different variables and tries to tell you how do these methods all perform under these different variables. So if you&#39;re interested I&#39;d encourage you to check it out </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>but kind of </span><span class="c3">one of the key takeaways</span><span class="c1">&nbsp;is that </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>the </span><span class="c3">faster R-CNN</span><span>&nbsp;style of region based methods tends to give </span><span class="c3">higher accuracies</span><span>&nbsp;</span><span class="c3 c12">but</span><span>&nbsp;ends up being </span><span class="c3">much slower</span><span>&nbsp;than the </span><span class="c3">single shot methods</span><span class="c1">&nbsp;because the single shot methods don&#39;t require this per region processing. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>Also as a bit of aside, I had this fun </span><span class="c22 c3"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/abs/1511.07571&amp;sa=D&amp;source=editors&amp;ust=1642006108674922&amp;usg=AOvVaw2hmA2vUYfoDMhheA6wJbJP">paper with Andre</a></span><span>&nbsp;a couple years ago that kind of combined object detection with image captioning and did this problem called </span><span class="c3">dense captioning</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">so now the idea is that rather than predicting a fixed category label for each region, instead we want to write a caption for each region. </span></li><li class="c5 li-bullet-0"><span class="c1">And again, we had some data set that had this sort of data where we had a data set of regions together with captions and then we sort of trained this giant end-to-end model that just predicted these captions all jointly. </span></li><li class="c5 li-bullet-0"><span class="c1">And this ends up looking somewhat like faster R-CNN where you have some region proposal stage then a bounding box, then some per region processing.</span></li><li class="c5 li-bullet-0"><span class="c1">But rather than a SVM or a softmax loss instead those per region processing has a whole RNN language model that predicts a caption for each region. </span></li><li class="c5 li-bullet-0"><span class="c1">So that ends up looking quite a bit like faster R-CNN. </span></li><li class="c5 li-bullet-0"><span class="c1">There&#39;s a video here but I think we&#39;re running out of time so I&#39;ll skip it. </span></li><li class="c5 li-bullet-0"><span>But </span><span class="c18 c3">the idea here</span><span>&nbsp;is that once you have this, </span><span class="c3">you can kind of tie together a lot of these ideas</span><span>&nbsp;and if you have some </span><span class="c3">new problem</span><span>&nbsp;that you&#39;re interested in tackling like dense captioning, you can </span><span class="c18 c3">recycle</span><span class="c3">&nbsp;a lot of the components that you&#39;ve learned from other problems</span><span>&nbsp;like object detection and image captioning and kind of </span><span class="c18 c3">stitch together</span><span class="c1">&nbsp;one end-to-end network that produces the outputs that you care about for your problem. </span></li></ul><h2 class="c20" id="h.xhixnfscin1l"><span>[ST] Mask R-CNN</span></h2><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span class="c1">So the last task that I want to talk about is this idea of instance segmentation. So here instance segmentation is in some ways like the full problem: We&#39;re given an input image and we want to predict </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>1. the locations and identities of objects in that image similar to </span><span class="c3">object detection</span><span class="c1">, </span></li><li class="c5 li-bullet-0"><span>2. but rather than just predicting a bounding box for each of those objects, instead we want to predict a whole </span><span class="c3">segmentation mask</span><span class="c1">&nbsp;for each of those objects and predict which pixels in the input image corresponds to each object instance. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So this is kind of like a </span><span class="c3">hybrid</span><span>&nbsp;between </span><span class="c3">semantic segmentation</span><span>&nbsp;and </span><span class="c3">object detection</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>because </span><span class="c3">like object detection</span><span class="c1">&nbsp;we can handle multiple objects and we differentiate the identities of different instances </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>so in this example since there are two dogs in the image and instance segmentation method actually </span><span class="c3">distinguishes between the two dog instances in the output</span><span class="c1">&nbsp;</span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1"><li class="c5 li-bullet-0"><span>and kind of </span><span class="c3">like semantic segmentation</span><span class="c1">&nbsp;we have this pixel wise accuracy where for each of these objects we want to say which pixels belong to that object. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So there&#39;s been a lot of different methods that people have tackled, for instance segmentation as well, but the current state of the art is this new paper called </span><span class="c3 c12">Mask R-CNN</span><span class="c1">&nbsp;that actually just came out on archive about a month ago so this is not yet published, this is like super fresh stuff. And this ends up looking a lot like faster R-CNN. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">So it has this multi-stage processing approach where we take our whole input image, that whole input image goes into some convolutional network and some learned region proposal network that&#39;s exactly the same as faster R-CNN and now once we have our learned region proposals then we project those proposals onto our convolutional feature map just like we did in fast and faster R-CNN. </span></li><li class="c10 c15 li-bullet-0"><span>But now rather than just making a classification and a bounding box for regression decision for each of those boxes </span><span class="c3">we in addition want to predict a segmentation mask for each of those region proposals</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">So now it kind of looks like a mini, like a semantic segmentation problem inside each of the region proposals that we&#39;re getting from our region proposal network. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So now </span><span class="c3">after we do this ROI aligning</span><span>&nbsp;to warp our features corresponding to the region of proposal into the right shape, then we have </span><span class="c3">two different branches</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span>1. One branch will come up that looks exact, and this first branch at the top looks just </span><span class="c3">like faster R-CNN</span><span class="c1">&nbsp;and it will predict classification scores telling us what is the category corresponding to that region of proposal or alternatively whether or not it&#39;s background. And we&#39;ll also predict some bounding box coordinates that regressed off the region proposal coordinates. </span></li><li class="c5 li-bullet-0"><span>2. And now in addition we&#39;ll have this branch at the bottom which looks basically </span><span class="c3">like a semantic segmentation mini network</span><span class="c1">&nbsp;which will classify for each pixel in that input region proposal whether or not it&#39;s an object </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>so this mask R-CNN problem, this </span><span class="c3">mask R-CNN architecture just kind of unifies all of these different problems</span><span class="c1">&nbsp;that we&#39;ve been talking about today into one nice jointly end-to-end trainable model. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">And it&#39;s really cool and it actually works really really well so when you look at the examples in the paper they&#39;re kind of amazing. They look kind of indistinguishable from ground truth. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">So in this example on the left you can see that there are these two people standing in front of motorcycles, it&#39;s drawn the boxes around these people, it&#39;s also gone in and labeled all the pixels of those people and it&#39;s really small but actually in the background on that image on the left there&#39;s also a whole crowd of people standing very small in the background. It&#39;s also drawn boxes around each of those and grabbed the pixels of each of those images. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>And you can see that this is just, it ends up working really really well and </span><span class="c3">it&#39;s a relatively simple addition on top of</span><span>&nbsp;the existing </span><span class="c3">faster R-CNN</span><span class="c1">&nbsp;framework. </span></li></ul><h3 class="c0" id="h.dxkzunk9iusx"><span>Pose estimation</span></h3><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>So I told you that mask R-CNN unifies everything we talked about today and it also does </span><span class="c3 c12">pose estimation</span><span class="c1">&nbsp;by the way. You can do pose estimation by predicting these joint coordinates for each of the joints of the person so you can do mask R-CNN to do joint object detection, pose estimation, and instance segmentation. </span></li><li class="c10 c15 li-bullet-0"><span>And the only addition we need to make is that </span><span class="c3">for each of these region proposals we add an additional little branch that predicts these coordinates of the joints</span><span class="c1">&nbsp;for the instance of the current region proposal. </span></li><li class="c10 c15 li-bullet-0"><span>So now this is </span><span class="c3">just another loss</span><span>, like </span><span class="c3">another layer</span><span>&nbsp;that we add, </span><span class="c3">another head</span><span>&nbsp;coming out of the network </span><span class="c3">and an additional term in our multi-task loss</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">But once we add this one little branch then you can do all of these different problems jointly and you get results looking something like this: Where now this network, like a single feed forward network is deciding </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">how many people are in the image, </span></li><li class="c5 li-bullet-0"><span class="c1">detecting where those people are, </span></li><li class="c5 li-bullet-0"><span class="c1">figuring out the pixels corresponding to each of those people </span></li><li class="c5 li-bullet-0"><span class="c1">and also drawing a skeleton estimating the pose of those people </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span>and this works really well </span><span class="c3">even in crowded scenes</span><span class="c1">&nbsp;like this classroom where there&#39;s a ton of people sitting and they all overlap each other and it just seems to work incredibly well. </span></li><li class="c10 c15 li-bullet-0"><span>And because it&#39;s </span><span class="c3">built on the faster R-CNN</span><span>&nbsp;framework it also </span><span class="c3">runs relatively close to real time</span><span>&nbsp;so this is running something like </span><span class="c3">five frames per second</span><span class="c1">&nbsp;on a GPU because this is all sort of done in the single forward pass of the network. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">So this is again, a super new paper but I think that this will probably get a lot of attention in the coming months. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">Questions: </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-1 start"><li class="c5 li-bullet-0"><span class="c1">How much training data do you need? </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-2 start"><li class="c10 c27 li-bullet-0"><span>So all of these instant segmentation results were trained on the Microsoft Coco data set so Microsoft Coco is roughly 200,000 training images. It has 80 categories that it cares about so in each of those 200,000 training images it has all the instances of those 80 categories labeled. So there&#39;s something like 200,000 images for training and there&#39;s something like I think an average of fivee or six instances per image. So it actually is quite a lot of data. And for Microsoft Coco for all the people in Microsoft Coco they also have all the joints annotated as well so this actually does have </span><span class="c3 c12">quite a lot of supervision at training time</span><span>&nbsp;you&#39;re right, and actually is trained with </span><span class="c3 c12">quite a lot of data</span><span>. So I think one really </span><span class="c18 c3">interesting topic to study moving forward</span><span>&nbsp;is that we kind of know that if you have a lot of data to solve some problem, at this point we&#39;re relatively confident that you can stitch up some convolutional network that can probably do a reasonable job at that problem but </span><span class="c18 c3">figuring out ways to get performance like this with less training data</span><span>&nbsp;is a super interesting and </span><span class="c18 c3">active area of research</span><span class="c1">&nbsp;and I think that&#39;s something people will be spending a lot of their efforts working on in the next few years. </span></li></ul><ul class="c2 lst-kix_mwssa4aj5yrr-0"><li class="c10 c15 li-bullet-0"><span class="c1">So just to recap, today we had kind of a whirlwind tour of a whole bunch of different computer vision topics and we saw how a lot of the machinery that we built up from image classification can be applied relatively easily to tackle these different computer vision topics. And next time we&#39;ll talk about, we&#39;ll have a really fun lecture on visualizing CNN features. Well also talk about DeepDream and neural style transfer.</span></li></ul><h1 class="c38" id="h.36fdrrxgbgzy"><span class="c24">lec15</span></h1><h2 class="c20" id="h.24ga9c1ozpeu"><span class="c34">Practical advice on CNN training</span></h2><h3 class="c0" id="h.4ic7uxrldg7o"><span>Data Augmentation</span></h3><h2 class="c20" id="h.3fy3a983rtvw"><span class="c34">CNNs for Segmentation</span></h2><h3 class="c0" id="h.ji5l9jogfdew"><span class="c21">FCN</span></h3><h3 class="c0" id="h.qracyekotcqs"><span class="c21">Encoder-Decoder architecture</span></h3><h3 class="c0" id="h.8wmq5h10jj2v"><span class="c21">Transpose convolutions (Learnable Upsampling)</span></h3><ul class="c2 lst-kix_5siicw1q5qtw-0 start"><li class="c10 c15 li-bullet-0"><span class="c1">Transposed convolutions suffer from chequered board effects as shown below</span></li></ul><ul class="c2 lst-kix_5siicw1q5qtw-1 start"><li class="c5 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 445.59px; height: 205.03px;"><img alt="" src="images/image75.png" style="width: 445.59px; height: 205.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>The </span><span class="c3">main cause</span><span class="c1">&nbsp;of this is uneven overlap at some parts of the image causing artifacts. </span></li><li class="c5 li-bullet-0"><span>This </span><span class="c3">can be fixed or reduced by</span><span class="c1">&nbsp;using kernel-size divisible by the stride, for e.g taking a kernel size of 2x2 or 4x4 when having a stride of 2.</span></li></ul><h2 class="c20" id="h.k4e5bkukoes6"><span class="c34">Extensions</span></h2><h3 class="c0" id="h.y2z9mwizjbzn"><span class="c21">(Long) Skip Connections</span></h3><ul class="c2 lst-kix_ujrw2gdovb97-0 start"><li class="c10 c15 li-bullet-0"><span>Long skip connections often exist in architectures that are symmetrical, where the </span><span class="c3">spatial dimensionality is reduced in the encoder part</span><span class="c1">&nbsp;and is gradually increased in the decoder part</span></li><li class="c10 c15 li-bullet-0"><span>The aforementioned architecture of the encoder-decoder scheme along with long skip connections is often referred as U-shape (</span><span class="c3">Unet</span><span class="c1">). </span></li><li class="c10 c15 li-bullet-0"><span>It is utilized for tasks that the prediction has the same spatial dimension as the input such as </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://theaisummer.com/Semantic_Segmentation/&amp;sa=D&amp;source=editors&amp;ust=1642006108689630&amp;usg=AOvVaw2seTsPsyg0xML8BdZXleZx">image segmentation</a></span><span class="c1">, optical flow estimation, video prediction, etc.</span></li><li class="c10 c15 li-bullet-0"><span>By introducing skip connections in the encoder-decoded architecture, </span><span class="c3">fine-grained details can be recovered in the prediction</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span>Even though there is no theoretical justification, symmetrical long skip connections work incredibly effectively in dense prediction tasks (</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://theaisummer.com/medical-image-deep-learning/&amp;sa=D&amp;source=editors&amp;ust=1642006108690572&amp;usg=AOvVaw0hnuD2Kvx8ScneEfVAiTXB">medical image segmentation</a></span><span class="c1">).</span></li><li class="c10 c15 li-bullet-0"><span class="c3">long skip connections are used to pass features from the encoder path to the decoder path in order to recover spatial information lost during downsampling</span><span>.</span></li></ul><h3 class="c0" id="h.nx0gf0gnnzh0"><span class="c21">Dilated/atrous convolution</span></h3><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 394.67px; height: 381.33px;"><img alt="" src="images/image61.gif" style="width: 394.67px; height: 381.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c2 lst-kix_rr55haih9k11-0 start"><li class="c15 c26 li-bullet-0"><span>Dilated convolutions introduce another parameter to convolutional layers called the </span><span class="c3 c12">dilation rate</span><span class="c1">. This defines a spacing between the values in a kernel. A 3x3 kernel with a dilation rate of 2 will have the same field of view as a 5x5 kernel, while only using 9 parameters. Imagine taking a 5x5 kernel and deleting every second column and row.</span></li><li class="c15 c62 li-bullet-0"><span>This delivers a </span><span class="c3">wider field of view [=receptive field] at the same computational cost [=same # parameters]</span><span class="c1">. </span></li><li class="c62 c15 li-bullet-0"><span>Dilated convolutions are particularly popular in the field of </span><span class="c3 c12">real-time segmentation</span><span class="c1">. </span></li><li class="c62 c15 li-bullet-0"><span class="c3">Use them if you need a wide field of view</span><span>&nbsp;and cannot afford multiple convolutions or larger kernels.</span></li></ul><h3 class="c0" id="h.itt9n7udcypj"><span class="c21">SPP</span></h3><ul class="c2 lst-kix_sezodd14tbn8-0 start"><li class="c10 c15 li-bullet-0"><span>[</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://paperswithcode.com/method/spatial-pyramid-pooling&amp;sa=D&amp;source=editors&amp;ust=1642006108692872&amp;usg=AOvVaw2IloOnLyZySj-9sgyJkmH-">paperswithcode</a></span><span>] </span><span class="c3 c12">Spatial Pyramid Pooling (SPP)</span><span>&nbsp;is a pooling layer that removes the fixed-size constraint of the network, i.e. </span><span class="c3">a CNN does not require a fixed-size input image</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_sezodd14tbn8-1 start"><li class="c5 li-bullet-0"><span class="c1">Specifically, we add an SPP layer on top of the last convolutional layer. </span></li><li class="c5 li-bullet-0"><span>The SPP layer pools the features and </span><span class="c3">generates fixed-length outputs</span><span class="c1">, which are then fed into the fully-connected layers (or other classifiers). </span></li><li class="c5 li-bullet-0"><span>In other words, we perform some information aggregation at a deeper stage of the network hierarchy (between convolutional layers and fully-connected layers) </span><span class="c3">to avoid the need for cropping or warping</span><span class="c1">&nbsp;at the beginning.</span></li></ul><h1 class="c38" id="h.i2ckbkvh9rxg"><span>lec16 - Human Pose Est; Matching; RNNs</span></h1><h2 class="c20" id="h.drh9ogy8dn9j"><span class="c34">FCNs for Human Pose Est</span></h2><h3 class="c0" id="h.i2isao6t151"><span class="c21">Definitions</span></h3><p class="c10"><span>[</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://paperswithcode.com/task/keypoint-detection&amp;sa=D&amp;source=editors&amp;ust=1642006108695013&amp;usg=AOvVaw2MPMvx1Ax4ugVhpnDCbTtQ">source</a></span><span class="c1">]</span></p><ul class="c2 lst-kix_1qx006ze2ww-0 start"><li class="c10 c15 li-bullet-0"><span class="c3 c12">Keypoints / interest points</span></li></ul><ul class="c2 lst-kix_1qx006ze2ww-1 start"><li class="c5 li-bullet-0"><span class="c3 c12">Keypoints</span><span>&nbsp;are the same thing as </span><span class="c3 c12">interest points</span><span class="c1">. They are spatial locations, or points in the image that define what is interesting or what stand out in the image. They are invariant to image rotation, shrinkage, translation, distortion, and so on.</span></li></ul><ul class="c2 lst-kix_1qx006ze2ww-0"><li class="c10 c15 li-bullet-0"><span class="c3 c12">Keypoint Detection</span></li></ul><ul class="c2 lst-kix_1qx006ze2ww-1 start"><li class="c5 li-bullet-0"><span class="c1">Keypoint detection involves simultaneously detecting people and localizing their keypoints. </span></li></ul><h2 class="c20" id="h.5fa895jj1spi"><span class="c34">Matching: SiameseNN + Triplet loss</span></h2><h3 class="c0" id="h.sg3hvvveuvsi"><span class="c21">Triplet Generation for SiameseNN</span></h3><h4 class="c14" id="h.ds1v9w5bztc6"><span class="c17">Online hard triplet mining</span></h4><h5 class="c68" id="h.djdnqzoal6o8"><span class="c13 c30">FaceNet</span></h5><ul class="c2 lst-kix_vm44gmi40r2u-0 start"><li class="c10 c15 li-bullet-0"><span>In order to ensure fast convergence it is crucial to </span><span class="c3">select triplets that violate the </span><span class="c3 c12">triplet constraint</span><span class="c1">&nbsp;</span></li></ul><p class="c10 c52"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 350.50px; height: 42.50px;"><img alt="" src="images/image26.png" style="width: 350.50px; height: 42.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp;</span></p><ul class="c2 lst-kix_dffublbbtuo-0 start"><li class="c10 c15 li-bullet-0"><span>This means that, given </span><img src="images/image1.png"><span>, we want to select an </span><img src="images/image2.png"><span>&nbsp;(</span><span class="c3 c12">hard positive</span><span>) such that </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 148.33px; height: 20.00px;"><img alt="" src="images/image43.png" style="width: 148.33px; height: 20.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>&nbsp;and similarly </span><img src="images/image3.png"><span>&nbsp;(</span><span class="c3 c12">hard negative</span><span>) such that </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 144.62px; height: 20.00px;"><img alt="" src="images/image29.png" style="width: 144.62px; height: 20.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c1">&nbsp;</span></li><li class="c10 c15 li-bullet-0"><span>[Problems with </span><span class="c3">Full batch training</span><span class="c1">:]</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1 start"><li class="c5 li-bullet-0"><span>It is </span><span class="c3">infeasible to compute the argmin and argmax across the whole training set</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>Additionally, it </span><span class="c3">might lead to poor training</span><span class="c1">, as mislabelled and poorly imaged faces would dominate the hard positives and negatives. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-0"><li class="c10 c15 li-bullet-0"><span class="c1">There are two obvious choices that avoid this issue:</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1 start"><li class="c5 li-bullet-0"><span>Generate triplets </span><span class="c3 c12">offline</span><span class="c1">&nbsp;[offline, weil (fast) full batch]</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-2 start"><li class="c10 c27 li-bullet-0"><span class="c18 c3">every n steps [ie. n mini-batches]</span><span>, </span><span class="c3 c33 c40">[reduces computation]</span></li><li class="c10 c27 li-bullet-0"><span class="c18 c3">using the most recent network checkpoint</span><span class="c1">&nbsp;and </span></li><li class="c10 c27 li-bullet-0"><span class="c18 c3">computing the argmin and argmax on a subset of the data [embeddings]</span><span>. </span><span class="c3 c42 c33">[reduces computation]</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1"><li class="c5 li-bullet-0"><span>Generate triplets </span><span class="c3 c12">online </span><span>[online, weil mini-batch (~1800 s. </span><span class="c22"><a class="c11" href="#id.ql8zosqwg6eb">hier</a></span><span>; full training set &gt; 10M s. </span><span class="c22"><a class="c11" href="#id.omdk29xs222v">hier</a></span><span class="c1">)]. This can be done by </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-2 start"><li class="c10 c27 li-bullet-0"><span class="c18 c3">selecting the hard positive/negative exemplars from within a mini-batch</span><span class="c1">.</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-0"><li class="c10 c15 li-bullet-0"><span>Here, we focus on the </span><span class="c3 c12">online generation</span><span>&nbsp;and </span><span class="c3">use large mini-batches in the order of a few thousand exemplars</span><span class="c1">&nbsp;and only compute the argmin and argmax within a mini-batch.</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1 start"><li class="c5 li-bullet-0"><span>[</span><span class="c18">A.</span><span class="c1">&nbsp;Build the mini-batch:] </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-2 start"><li class="c10 c27 li-bullet-0"><span>[</span><span class="c18">1.</span><span>&nbsp;Sample (easy and hard) Positives:] To have a meaningful representation of the anchor-positive distances, it needs to be </span><span class="c3">ensured that a minimal number of [positive] exemplars of any one identity is present in each mini-batch</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">In our experiments we sample the training data such that around 40 faces are selected per identity per mini-batch. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-2"><li class="c10 c27 li-bullet-0"><span>[</span><span class="c18">2.</span><span class="c1">&nbsp;Sample (easy and hard) Negatives:] Additionally, randomly sampled negative faces are added to each mini-batch.</span></li></ul><a id="id.phb6w55bwhtg"></a><ul class="c2 lst-kix_dffublbbtuo-1"><li class="c5 li-bullet-0"><span>[</span><span class="c18">B.</span><span>&nbsp;Generate triplets </span><span class="c3">from mini-batch from </span><span class="c18">A</span><span class="c1">:]</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-2 start"><li class="c10 c27 li-bullet-0"><span>Instead of picking the </span><span class="c25">hardest</span><span>&nbsp;positive, </span><span class="c3">we use </span><span class="c25 c3">all</span><span class="c3">&nbsp;anchor-positive pairs in a mini-batch</span><span>&nbsp;while still selecting the </span><span class="c25">hard</span><span class="c1">&nbsp;negatives. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-3 start"><li class="c10 c35 li-bullet-0"><span class="c42">[dh for each anchor (1800), </span><span class="c25 c42">all</span><span class="c42">&nbsp;positives (39), </span><span class="c25 c42">hard</span><span class="c40 c60">&nbsp;negative (1) (because one argmin per anchor); but avoid doubling anchor-positive pairs, dh. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-4 start"><li class="c10 c39 li-bullet-0"><span class="c42">\# triplets used for training = </span><img src="images/image4.png"><span class="c40 c60">&nbsp;a-p pairs * 1 = 780 &lt; 1800 triplets]</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-3"><li class="c10 c35 li-bullet-0"><span>[Begr&uuml;ndung f&uuml;r diese Entscheidung:] We don&rsquo;t have a side-by-side comparison of </span><span class="c3 c42">hard anchor-positive pairs</span><span class="c3">&nbsp;versus </span><span class="c3 c42">all anchor-positive pairs</span><span>&nbsp;within a mini-batch [dh die haben das verglichen, aber nicht im paper aufgeschrieben], but we found in practice that the &ldquo;</span><span class="c25 c3">all</span><span class="c3">&nbsp;anchor-positive method&rdquo;</span><span>&nbsp;was </span><span class="c3">more stable</span><span>&nbsp;and </span><span class="c3">converged slightly faster at the beginning</span><span class="c1">&nbsp;of training. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-0"><li class="c10 c15 li-bullet-0"><span>We also explored the </span><span class="c3 c12">offline generation</span><span class="c3">&nbsp;</span><span class="c1">in conjunction with the online generation and it may allow the use of smaller batch sizes, but the experiments were inconclusive.</span></li><li class="c10 c15 li-bullet-0"><span class="c3 c12">Use semi-hard exemplars: </span><span>Selecting the </span><span class="c3">hardest negatives</span><span>&nbsp;can in practice lead to </span><span class="c3">bad local minima</span><span>&nbsp;early on in training, specifically it </span><span class="c3">can result in a collapsed model</span><span>&nbsp;(i.e. </span><img src="images/image5.png"><span class="c1">). </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1 start"><li class="c5 li-bullet-0"><span>In order to mitigate this, it helps to select </span><img src="images/image3.png"><span>&nbsp;such that</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 247.24px; height: 36.15px;"><img alt="" src="images/image47.png" style="width: 247.24px; height: 36.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>We call these negative exemplars </span><span class="c3 c12">semi-hard</span><span class="c1">, as they are further away from the anchor than the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. Those negatives lie inside the margin &alpha;.</span></li></ul><ul class="c2 lst-kix_dffublbbtuo-0"><li class="c10 c15 li-bullet-0"><span>As mentioned before, correct </span><span class="c3">triplet selection</span><span>&nbsp;is </span><span class="c3">crucial for fast convergence</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1 start"><li class="c5 li-bullet-0"><span>On the one hand we would like to </span><span class="c3">use small mini-batches</span><span class="c1">&nbsp;as these tend to improve convergence during Stochastic Gradient Descent (SGD) [20]. </span></li><li class="c5 li-bullet-0"><span>On the other hand, </span><span class="c3">implementation</span><span>&nbsp;details make </span><span class="c3">batches of tens to hundreds of exemplars more efficient</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>The main constraint with regards to the batch size, however, is the </span><span class="c3">way we select hard relevant triplets from within the mini-batches</span><span class="c1">. </span></li></ul><a id="id.ql8zosqwg6eb"></a><ul class="c2 lst-kix_dffublbbtuo-1"><li class="c5 li-bullet-0"><span class="c1">In most experiments we use a batch size of around 1,800 exemplars.</span></li></ul><a id="id.omdk29xs222v"></a><ul class="c2 lst-kix_dffublbbtuo-0"><li class="c10 c15 li-bullet-0"><span>Table 6 shows the impact of large </span><span class="c3">amounts of training data</span><span class="c1">. Due to time constraints this evaluation was run on a smaller model [ie. less parameter]; the effect may be even larger on larger models. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 366.64px; height: 173.57px;"><img alt="" src="images/image22.png" style="width: 366.64px; height: 173.57px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_dffublbbtuo-1 start"><li class="c5 li-bullet-0"><span>It is clear that using tens of millions of exemplars results in a clear boost of accuracy on our personal photo test set from section 4.2. Compared to only millions of images the relative reduction in error is 60%. Using another order of magnitude more images (hundreds of millions) still gives a small boost, but the improvement tapers off.</span></li></ul><h5 class="c68" id="h.nxxzqefnb696"><span class="c13 c30">Hermans, Leibe</span></h5><ul class="c2 lst-kix_iu8p9gst7pod-0 start"><li class="c10 c15 li-bullet-0"><span>A major </span><span class="c3">caveat of the triplet loss</span><span class="c1">, though, is </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-1 start"><li class="c5 li-bullet-0"><span>that as the dataset gets larger, the </span><span class="c3">possible number of triplets grows cubically</span><span class="c1">, rendering a long enough training impractical. </span></li><li class="c5 li-bullet-0"><span class="c1">To make matters worse, f&theta; relatively quickly learns to correctly map most trivial triplets, rendering a large fraction of all triplets uninformative. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-0"><li class="c10 c15 li-bullet-0"><span class="c1">Thus mining hard triplets becomes crucial for learning. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-1 start"><li class="c5 li-bullet-0"><span class="c3">Intuitively</span><span>, being told over and over again that people with differently colored clothes are different persons does not teach one anything, whereas seeing similarly-looking but different people (</span><span class="c3">hard negatives</span><span>), or pictures of the same person in wildly different poses (</span><span class="c3">hard positives</span><span class="c1">) dramatically helps understanding the concept of &ldquo;same person&rdquo;. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-0"><li class="c10 c15 li-bullet-0"><span>On the other hand, </span><span class="c3">being shown only the hardest triplets</span><span class="c1">&nbsp;would select outliers in the data unproportionally often and make f&theta; unable to learn &ldquo;normal&rdquo; associations, as will be shown in Table 1. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-1 start"><li class="c5 li-bullet-0"><span class="c1">Examples of typical hard positives, hard negatives, and outliers are shown in the Supplementary Material. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-0"><li class="c10 c15 li-bullet-0"><span>Hence it is common to only </span><span class="c3">mine </span><span class="c3 c12">moderate negatives</span><span>&nbsp;[29] </span><span class="c3">and/or </span><span class="c3 c12">moderate positives</span><span class="c1">&nbsp;[31]. </span></li><li class="c10 c15 li-bullet-0"><span>Regardless of which type of </span><span class="c3">mining</span><span>&nbsp;is being done, it is a separate step from training and adds </span><span class="c3">considerable overhead</span><span class="c1">, as it requires embedding a large fraction of the data with the most recent f&theta; and computing all pairwise distances between those data points.</span></li><li class="c10 c15 li-bullet-0"><span>In a </span><span class="c3">classical implementation</span><span>, once a certain set of B triplets has been chosen [dh. erst </span><span class="c25 c3">nach</span><span>&nbsp;</span><span class="c22"><a class="c11" href="#id.phb6w55bwhtg">B</a></span><span>&nbsp;! nicht davor !</span><span>], their images are stacked into a </span><span class="c3 c42">batch of size 3B</span><span class="c1">, for which the 3B embeddings are computed, which are in turn used to create B terms contributing to the loss. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-1 start"><li class="c5 li-bullet-0"><span class="c3">Problem</span><span>: Given the fact that there are up to 6B**2 &minus;4B [</span><span class="c3 c12">unwahrscheinlich in Klausur</span><span>] possible combinations of these 3B images that are valid triplets, using only B of them seems </span><span class="c3">wasteful</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-2 start"><li class="c10 c27 li-bullet-0"><span>With this realization, </span><span class="c3">we [Leibe] propose</span><span class="c1">&nbsp;an organizational modification to the classic way of using the triplet loss: </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">the core idea is to </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-4 start"><li class="c10 c39 li-bullet-0"><span>1. [sozusagen Schritt </span><span class="c18 c3">A</span><span>&nbsp;bezogen auf </span><span class="c22"><a class="c11" href="#id.phb6w55bwhtg">oben</a></span><span>] form batches by randomly sampling P classes (person identities), and then randomly sampling K images of each class (person), thus resulting in a </span><span class="c3 c42">batch of PK images</span><span>.</span><sup><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span class="c1">&nbsp;[dh K-1 positives und Rest vom mini-batch wird als negatives behandelt]</span></li><li class="c10 c39 li-bullet-0"><span>2. [sozusagen Schritt </span><span class="c18 c3">B</span><span>&nbsp;bezogen auf </span><span class="c22"><a class="c11" href="#id.phb6w55bwhtg">oben</a></span><span>] Now, for each sample </span><img src="images/image6.png"><span>&nbsp;in the batch, we can </span><span class="c3">select the hardest positive and the hardest negative</span><span>&nbsp;samples </span><span class="c18 c3">within the batch</span><span>&nbsp;when forming the triplets for computing the loss, which we call </span><span class="c3 c12">Batch Hard</span><span class="c1">&nbsp;[...].</span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-5 start"><li class="c10 c58 li-bullet-0"><span>This results in </span><span class="c3">PK terms contributing to the loss</span><span>, a threefold</span><sup><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span class="c1">&nbsp;increase over the traditional formulation. </span></li><li class="c10 c58 li-bullet-0"><span>Additionally, the selected triplets can be considered </span><span class="c3 c12">moderate triplets</span><span class="c1">, since they are the hardest within a small subset of the data, which is exactly what is best for learning with the triplet loss</span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-4"><li class="c10 c39 li-bullet-0"><span class="c3">alternative to 2.</span><span>: This new formulation of sampling a batch immediately suggests another alternative, that is to simply </span><span class="c3">use all possible PK(PK &minus;K)(K &minus;1) combinations</span><span>&nbsp;of triplets, which corresponds to the strategy chosen in [9] and which we call </span><span class="c3 c12">Batch All</span><span class="c1">&nbsp;[...].</span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-3"><li class="c10 c35 li-bullet-0"><span>At this point, it is important to note that </span><span class="c3">both</span><span>&nbsp;LBH and LBA still </span><span class="c18 c3">exactly</span><span class="c3">&nbsp;correspond to the standard triplet loss in the limit of infinite training</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-4 start"><li class="c10 c39 li-bullet-0"><span>Both the max and min functions [in the LBH loss and LBA loss] are continuous and differentiable almost everywhere, meaning they </span><span class="c18 c3">can be used in a model trained by stochastic (sub-)gradient descent</span><span class="c1">&nbsp;without concern. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-5 start"><li class="c10 c58 li-bullet-0"><span class="c1">In fact, they are already widely available in popular deep-learning frameworks for the implementation of max-pooling and the ReLU [11] non-linearity.</span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-3"><li class="c10 c35 li-bullet-0"><span>Most similar to our batch hard and batch all losses is the </span><span class="c3 c12">Lifted Embedding loss</span><span class="c1">&nbsp;[32], which fills the batch with triplets but considers all but the anchor-positive pair as negatives: [...]</span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-4 start"><li class="c10 c39 li-bullet-0"><span>While [32] motivates a &ldquo;hard&rdquo;-margin loss similar to LBH and LBA, they end up optimizing the smooth bound of it given in the above equation. Additionally, traditional </span><span class="c3">3B batches</span><span>&nbsp;are considered, thus using all possible negatives, but </span><span class="c3">only one positive pair</span><span class="c1">&nbsp;per triplet. </span></li></ul><ul class="c2 lst-kix_iu8p9gst7pod-3"><li class="c10 c35 li-bullet-0"><span>This leads us to propose a </span><span class="c3 c12">generalization of the Lifted Embedding loss</span><span>&nbsp;</span><span class="c3">based on PK batches which considers all anchor-positive pairs</span><span class="c1">&nbsp;as follows: [...]</span></li></ul><h2 class="c20" id="h.mrhn30ytd8kq"><span class="c34">RNN [Stanford]</span></h2><h3 class="c0" id="h.4szjjhufyiw1"><span class="c21">Motivation, &ldquo;Pitch&rdquo;</span></h3><ul class="c2 lst-kix_i64ijt69btn4-0 start"><li class="c10 c15 li-bullet-0"><span>So that was kind of our brief recap of the CNN architectures that we saw last lecture, and then today, we&#39;re going to move to one of my favorite topics to talk about, which is recurrent neural networks. </span></li></ul><h4 class="c14" id="h.za52fk19jg03"><span class="c17">So far ...</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0 start"><li class="c10 c15 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 336.50px; height: 189.49px;"><img alt="" src="images/image53.png" style="width: 336.50px; height: 189.49px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span class="c3">So far</span><span>&nbsp;in this class, we&#39;ve seen, what I like to think of as kind of a vanilla feed forward network, all of our network architectures have this flavor, where we receive some input and that </span><span class="c3">input is a fixed size object</span><span>, like an image or vector. That input is fed through some set of hidden layers and produces a </span><span class="c3">single output</span><span class="c1">, like a set of classifications scores over a set of categories. </span></li></ul><h4 class="c14" id="h.eecplpuzhvvz"><span>RNN model types</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>But in some context in machine learning, </span><span class="c3">we want</span><span>&nbsp;to have </span><span class="c3">more flexibility in the types of data</span><span>&nbsp;that our models can process. So once we move to this idea of </span><span class="c3 c12">recurrent neural networks</span><span>, we have </span><span class="c3">a lot more</span><span>&nbsp;opportunities to play around with the </span><span class="c3">types of input and output data</span><span class="c1">&nbsp;that our networks can handle: </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 327.03px; height: 184.56px;"><img alt="" src="images/image42.png" style="width: 327.03px; height: 184.56px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>So once we have recurrent neural networks, we can do what we call these </span><span class="c3 c12">one to many models</span><span class="c1">:</span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">where maybe our input is some object of fixed size, like an image, but now our output is a sequence of variable length, such as a caption. Where different captions might have different numbers of words, so our output needs to be variable in length. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 344.74px; height: 194.13px;"><img alt="" src="images/image72.png" style="width: 344.74px; height: 194.13px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>We also might have </span><span class="c3 c12">many to one models</span><span class="c1">, </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>where our input could be variably sized. This might be something like a </span><span class="c3">piece of text,</span><span>&nbsp;and we want to say what is the </span><span class="c3">sentiment of that text</span><span>, whether it&#39;s positive or negative in sentiment. Or in a computer vision context, you might imagine </span><span class="c3">taking as input a video</span><span>, and that video might have a variable number of frames. And now we want to read this entire video of potentially variable length. And then at the end, </span><span class="c3">make a classification</span><span>&nbsp;decision about maybe what </span><span class="c3">kind of activity or action</span><span class="c1">&nbsp;is going on in that video. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.50px; height: 196.81px;"><img alt="" src="images/image78.png" style="width: 349.50px; height: 196.81px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>We also might also have problems where we want both the </span><span class="c3 c12">inputs and the output to be variable (many to many)</span><span class="c1">&nbsp;in length. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>We might see something like this in </span><span class="c3">machine translation</span><span>, where our input is some, maybe, sentence in English, which could have a variable length, and our output is maybe some sentence in French, which also could have a variable length. And crucially, the length of the English sentence might be different from the length of the French sentence. So we need some models that have the capacity to accept </span><span class="c3">both </span><span class="c25 c3">variable</span><span class="c3">&nbsp;length sequences on the input and on the output</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 335.50px; height: 188.93px;"><img alt="" src="images/image25.png" style="width: 335.50px; height: 188.93px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>Finally, we might also consider problems where our </span><span class="c3 c12">input is variably length</span><span>, like something like a video sequence with a variable number of </span><span class="c3">frames</span><span>. And now we want to </span><span class="c3 c12">make a decision for each element of that input sequence (many to many)</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>So in the context of videos, that might be making some </span><span class="c3">classification</span><span>&nbsp;decision along </span><span class="c3">every frame of the video</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span class="c1">And recurrent neural networks are this kind of general paradigm for handling variable sized sequence data that allow us to pretty naturally capture all of these different types of setups in our models. </span></li><li class="c5 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 264.50px; height: 260.58px;"><img alt="" src="images/image46.gif" style="width: 264.50px; height: 260.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span class="c3 c12">Sequential Processing of Non-Sequence Data: </span><span>So recurrent neural networks are actually important, even for some </span><span class="c3 c12">problems that have a fixed size input and a fixed size output</span><span class="c1">. Recurrent neural networks can still be pretty useful. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>So in </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1412.7755.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108725213&amp;usg=AOvVaw01RfVdTS75fDkpRUZieivI">this example</a></span><sup><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup><span>, we might want to do, for example, sequential processing of our input. So here, we&#39;re receiving a fixed size </span><span class="c3">input</span><span>&nbsp;like an </span><span class="c3">image</span><span>, and we want to </span><span class="c3">make a classification</span><span>&nbsp;decision about, like, </span><span class="c3">what number is being shown</span><span>&nbsp;in this image? But now, rather than just doing a single feed forward pass and making the decision all at once, this network is actually </span><span class="c3">looking around the image</span><span>&nbsp;and taking various </span><span class="c3 c12">glimpses</span><sup class="c3 c12"><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span>&nbsp;of different parts of the image. And then after making some </span><span class="c25 c3">series of</span><span class="c3">&nbsp;glimpses</span><span>, then it makes its </span><span class="c3">final decision</span><span>&nbsp;as to what </span><span class="c3">kind of number</span><span class="c1">&nbsp;is present. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span>So here, even though our </span><span class="c3">input was an image</span><span>, and our </span><span class="c3">output was a classification decision</span><span>, even in this context, this idea of being able to handle </span><span>variably</span><span class="c1">&nbsp;length processing with recurrent neural networks can lead to some really interesting types of models. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2"><li class="c10 c27 li-bullet-0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 382.51px; height: 174.73px;"><img alt="" src="images/image21.gif" style="width: 382.51px; height: 174.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c27 li-bullet-0"><span>There&#39;s a really </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1502.04623.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108727336&amp;usg=AOvVaw0mooZVrKhfhk0pBmu2Fe-u">cool paper</a></span><sup><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup><span>&nbsp;that I like that applied this same type of idea to generating new images. Where now, we want the model to </span><span class="c3 c12">synthesize brand new images</span><span class="c3">&nbsp;that look kind of like the images it saw in training</span><span class="c1">, and we can use a recurrent neural network architecture to actually paint these output images sort of one piece at a time in the output. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span>You can see that, even though our </span><span class="c3">output is this fixed size image</span><span>, we can have these models that are working over time to </span><span class="c3">compute parts of the output one at a time </span><span class="c25 c3">sequentially</span><span class="c1">. And we can use recurrent neural networks for that type of setup as well. </span></li></ul><h3 class="c0" id="h.vtek9pgw49x1"><span>RNN Basics</span></h3><h4 class="c14" id="h.oorizrx36x4r"><span>Interpretation 1[</span><span class="c22"><a class="c11" href="#id.d8x3gwmp52na">confusing</a></span><span class="c17">]: Hidden state that feeds back at itself, recurrently</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">So after this sort of cool pitch about all these cool things that RNNs can do, you might wonder, like what exactly are these things?</span></li><li class="c10 c15 li-bullet-0"><span>So in general, a recurrent neural network has this little </span><span class="c3 c12">recurrent core cell</span><span>&nbsp;and it will take some input x, feed that input into the RNN, and that RNN has some internal </span><span class="c3 c12">hidden state</span><span>, and that </span><span class="c3">internal hidden state will be updated every time that the RNN reads a new input</span><span class="c1">. And that internal hidden state will be then fed back to the model the next time it reads an input. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 370.50px; height: 209.65px;"><img alt="" src="images/image60.png" style="width: 370.50px; height: 209.65px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span>And frequently, we will want our RNN&quot;s to also </span><span class="c3">produce some output at every time step</span><span class="c1">, so we&#39;ll have this pattern where it will read an input, update its hidden state, and then produce an output. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 374.89px; height: 211.62px;"><img alt="" src="images/image66.png" style="width: 374.89px; height: 211.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span>So then the question is what is the </span><span class="c3">functional form of this recurrence relation</span><span class="c1">&nbsp;that we&#39;re computing? </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 379.50px; height: 213.71px;"><img alt="" src="images/image51.png" style="width: 379.50px; height: 213.71px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So inside this little green RNN block, we&#39;re computing some </span><span class="c3 c12">recurrence relation</span><span>, with a </span><span class="c3">function </span><img src="images/image7.png"><span>. So this function </span><img src="images/image7.png"><span>&nbsp;will depend on some </span><span class="c3">weights </span><img src="images/image8.png"><span>. It will accept the </span><span class="c3">previous hidden state </span><img src="images/image9.png"><span>, as well as the </span><span class="c3">input at the current state </span><img src="images/image10.png"><span>, and this will output the </span><span class="c3">next hidden state (updated hidden state) </span><img src="images/image11.png"><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And now, then as we read the next input, this hidden state, this new hidden state </span><img src="images/image11.png"><span>, will then just be passed into the same function as we read </span><span class="c3">the next input</span><span>&nbsp;</span><img src="images/image12.png"><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And now, if we wanted to </span><span class="c3">produce some output at every time step</span><span>&nbsp;of this network, we might attach some additional fully connected layers that read in this </span><img src="images/image11.png"><span class="c1">&nbsp;at every time step and make that decision based on the hidden state at every time step. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>And one thing to note is that we use the same function </span><img src="images/image13.png"><span>, and the same weights </span><img src="images/image8.png"><span class="c1">&nbsp;at every time step of the computation. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>[&ldquo;confusing picture&rdquo;, wie er selbst sagt, s. n&auml;chster Punkt] So then kind of the </span><span class="c3">simplest function form</span><span>&nbsp;that you can imagine is what we call this </span><span class="c3 c12">vanilla recurrent neural network</span><span class="c1">. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 372.50px; height: 209.76px;"><img alt="" src="images/image87.png" style="width: 372.50px; height: 209.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So here, we have this </span><span class="c3">same functional form from the previous slide</span><span class="c1">, where we&#39;re taking in our previous hidden state and our current input and we need to produce the next hidden state. </span></li><li class="c5 li-bullet-0"><span>And the kind of simplest thing you might imagine is that we have some weight matrix, </span><img src="images/image14.png"><span>, that we multiply against the input, </span><img src="images/image10.png"><span>, as well as another weight matrix, </span><img src="images/image15.png"><span class="c1">, that we multiply against the previous hidden state. </span></li><li class="c5 li-bullet-0"><span>So we make these </span><span class="c3">two multiplications</span><span>&nbsp;against our two states, </span><span class="c3">add them</span><span>&nbsp;together, and </span><span class="c3">squash them through a </span><img src="images/image16.png"><span class="c1">, so we get some kind of non-linearity in the system. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">You might be wondering why we use a tanh here and not some other type of non-linearity? After all that we&#39;ve said negative about tanh&#39;s in previous lectures </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">and I think we&#39;ll return a little bit to that later on when we talk about more advanced architectures, like lstm. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>So then, in addition in this architecture, if we wanted to </span><span class="c3">produce some </span><img src="images/image17.png"><span class="c3">&nbsp;at every time step</span><span>, you might have another weight matrix, </span><img src="images/image18.png"><span>, that accepts this hidden state and then transforms it to some </span><img src="images/image19.png"><span>&nbsp;to </span><span class="c3">produce maybe some class score predictions at every time step</span><span class="c1">. </span></li></ul><h4 class="c14" id="h.8znxq2q1muzu"><span>Interpretation 2: </span><span class="c17">Unrolling computational graph for multiple time steps</span></h4><a id="id.d8x3gwmp52na"></a><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>You can kind of think of recurrent neural networks </span><span class="c3">in two ways</span><span class="c1">: </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">One is this concept of having a hidden state that feeds back at itself, recurrently. But I find that picture a little bit confusing. </span></li><li class="c5 li-bullet-0"><span class="c1">And sometimes, I find it clearer to think about unrolling this computational graph for multiple time steps. And this makes the data flow of the hidden states and the inputs and the outputs and the weights maybe a little bit more clear. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So then at the first time step, we&#39;ll have some </span><span class="c3">initial hidden state h zero</span><span>. This is usually initialized to zeros for most contexts, in most contexts, and then we&#39;ll have some </span><span class="c3">input, x t</span><span>. This initial hidden state, </span><span class="c3">h zero</span><span>, and our </span><span class="c3">current input, x t</span><span>, will go into our </span><span class="c3">f w function</span><span>. This will produce our </span><span class="c3">next hidden state, h one</span><span class="c1">. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 410.21px; height: 231.00px;"><img alt="" src="images/image65.png" style="width: 410.21px; height: 231.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span>And then, we&#39;ll </span><span class="c3">repeat this process</span><span>&nbsp;when we receive </span><span class="c3">the next input</span><span>. So now our </span><span class="c3">current h one and our x two</span><span>, will go into that </span><span class="c3">same f w</span><span>, to produce our </span><span class="c3">next output, h two</span><span class="c1">. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 343.50px; height: 193.43px;"><img alt="" src="images/image41.png" style="width: 343.50px; height: 193.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span>And </span><span class="c3">this process will repeat</span><span>&nbsp;over and over again, as we </span><span class="c3">consume all of the input, x ts</span><span class="c1">, in our sequence of inputs. </span></li><li class="c10 c15 li-bullet-0"><span>And now, one thing to note is that we can actually make this even more explicit and write the </span><span class="c18 c3">w matrix</span><span>&nbsp;in our computational graph. And here you can see that we&#39;re re-using the </span><span class="c3">same w matrix at every time step</span><span>&nbsp;of the computation. So now every time that we have this little f w block, it&#39;s receiving a </span><span class="c3">unique h</span><span>&nbsp;and a </span><span class="c3">unique x</span><span>, but all of these blocks are taking the </span><span class="c3">same w</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>And if you remember, we talked about how </span><span class="c18 c3">gradient flows</span><span class="c3">&nbsp;in back propagation, when you re-use the same node multiple times in a computational graph</span><span>, then remember during the backward pass, you end up </span><span class="c3">summing the gradients into the w matrix</span><span>&nbsp;when you&#39;re computing a </span><img src="images/image20.png"><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>So, if you kind of think about the back propagation for this model, then you&#39;ll have a separate gradient for w flowing from each of those time steps, and then the </span><span class="c3">final gradient for w will be the sum of all of those individual per time step gradients</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>We can also write to this </span><span class="c18 c3">y t</span><span>&nbsp;explicitly in this computational graph. So then, this </span><span class="c3">output, h t</span><span>, at every time step might </span><span class="c3">feed into some other little neural network</span><span>&nbsp;that can </span><span class="c3">produce a y t</span><span>, which might be some </span><span class="c3">class scores</span><span class="c1">, or something like that, at every time step. </span></li><li class="c10 c15 li-bullet-0"><span>We can also make </span><span class="c18 c3">the loss</span><span>&nbsp;more explicit. So in many cases, you might imagine that you have some ground truth label at every time step of your sequence, and then you&#39;ll compute some </span><span class="c3">individual loss at every time step</span><span>&nbsp;of these outputs, y t&#39;s. And this loss will </span><span class="c3">frequently</span><span>&nbsp;be something like </span><span class="c3">softmax loss</span><span>, in the case where you have, maybe, a ground truth label at every time step of the sequence. And now the </span><span class="c3">final loss</span><span>&nbsp;for this entire training stop, </span><span class="c3">will be the sum of these individual losses</span><span class="c1">.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 383.40px; height: 215.83px;"><img alt="" src="images/image67.png" style="width: 383.40px; height: 215.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So now, we had a scalar loss at every time step. And we just summed them up to get our final scalar loss at the top of the network. And now, if you think about </span><span class="c3">backpropagation</span><span>&nbsp;through this thing, we need, in order to train the model, we need to compute the gradient of the loss with respect to w. So, we&#39;ll have </span><span class="c3 c42">loss flowing from that final loss into each of these time steps</span><span>. And then each of those time steps will compute a </span><span class="c3">local gradient</span><span>&nbsp;on the weights, w, which </span><span class="c3">will all then be summed</span><span>&nbsp;to give us our </span><span class="c3">final gradient</span><span class="c1">&nbsp;for the weights, w. </span></li></ul><h3 class="c0" id="h.qahuzr10hiap"><span class="c21">RNN model types [details]</span></h3><h4 class="c14" id="h.jcbbt2d1fsca"><span>many to one situation</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>Now if we have a </span><span class="c18 c3">many to one situation</span><span>, where maybe we want to do something like </span><span class="c3 c12">sentiment analysis</span><span>, then we would typically make that decision based on the final hidden state of this network. Because this </span><span class="c3">final hidden state kind of summarizes all of the context from the entire sequence</span><span class="c1">. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 343.50px; height: 193.43px;"><img alt="" src="images/image69.png" style="width: 343.50px; height: 193.43px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><h4 class="c14" id="h.ueqpi5was60p"><span>one to many situation</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>Also, if we have a </span><span class="c18 c3">one to many situation</span><span>, where we want to receive a fixed sized input and then produce a variably sized output, then you&#39;ll commonly </span><span class="c3">use that fixed size input to initialize the initial hidden state</span><span class="c1">&nbsp;of the model, and now the recurrent network will tick for each cell in the output. And now, as you produce your variably sized output, you&#39;ll unroll the graph for each element in the output. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 335.50px; height: 189.80px;"><img alt="" src="images/image73.png" style="width: 335.50px; height: 189.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><h4 class="c14" id="h.em5xbkwqt8bg"><span>sequence to sequence models</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>When we talk about the </span><span class="c18 c3">sequence to sequence models</span><span>&nbsp;where you might do something like </span><span class="c3 c12">machine translation</span><span>, where you take a variably sized input and a variably sized output. You can </span><span class="c3">think of this as a combination of the many to one, plus a one to many</span><span>. So, we&#39;ll kind of proceed in </span><span class="c3">two stages</span><span>, what we call an </span><span class="c3 c12">encoder and a decoder</span><span class="c1">. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 336.00px; height: 188.94px;"><img alt="" src="images/image71.png" style="width: 336.00px; height: 188.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So if you&#39;re </span><span class="c3">the encoder</span><span>, we&#39;ll receive the variably sized input, which might be your sentence in English, and then summarize that entire sentence using the final hidden state of the encoder network. And now we&#39;re in this </span><span class="c3">many to one situation</span><span class="c1">&nbsp;where we&#39;ve summarized this entire variably sized input in this single vector, </span></li><li class="c5 li-bullet-0"><span>and now, we have a second </span><span class="c3">decoder network</span><span>, which is a </span><span class="c3">one to many situation</span><span class="c1">, which will input that single vector summarizing the input sentence and now produce this variably sized output, which might be your sentence in another language. And now in this variably sized output, we might make some predictions at every time step, maybe about what word to use. And you can imagine training this entire thing by unrolling this computational graph summing the losses at the output sequence and just performing back propagation, as usual. </span></li></ul><h5 class="c68" id="h.2tv00yspfd5z"><span>Example: language model</span></h5><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>So as a bit of a concrete example, one thing that we frequently use recurrent neural networks for, is this problem called </span><span class="c3 c12">language modeling</span><span>. So in the language modeling problem, we want to have our network, sort of, </span><span class="c18 c3">understand how to produce natural language</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>So, this might happen at the </span><span class="c3">character level</span><span class="c1">&nbsp;where our model will produce characters one at a time. </span></li><li class="c10 c27 li-bullet-0"><span>This might also happen at the </span><span class="c3">word level</span><span class="c1">&nbsp;where our model will produce words one at a time. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 382.07px; height: 215.15px;"><img alt="" src="images/image32.png" style="width: 382.07px; height: 215.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>But in a very simple example, you can imagine this </span><span class="c3">character level language model</span><span class="c1">, where the network will </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">read some sequence of characters and </span></li><li class="c10 c27 li-bullet-0"><span class="c1">then it needs to predict, what will the next character be in this stream of text. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span class="c1">So in this example, we have this very small vocabulary of four letters, h, e, l, and o, and we have this example training sequence of the word hello, h, e, l, l, o. So during training, when we&#39;re training this language model, we will feed the characters of this training sequence as inputs, as x ts, to out input of our, we&#39;ll feed the characters of our training sequence, these will be the x ts that we feed in as the inputs to our recurrent neural network. And then, each of these inputs is a letter, and we need to figure out a way to represent letters in our network. So what we&#39;ll typically do is, figure out what is our total vocabulary. In this case, our vocabulary has four elements. And each letter will be represented by a vector that has zeros in every slot but one, and a 1 for the slot in the vocabulary corresponding to that letter. In this little example, since our vocab has the four letters, h, e, l, o, then in our input sequence, the h is represented by a four element vector with a 1 in the first slot and zeros in the other three slots. And we use the same sort of pattern to represent all the different letters in the input sequence. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 326.50px; height: 183.40px;"><img alt="" src="images/image28.png" style="width: 326.50px; height: 183.40px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c5 li-bullet-0"><span>Now, during this </span><span class="c18 c3">forward pass</span><span>&nbsp;of what this network is doing, at the </span><span class="c3">first time step</span><span class="c1">, it will receive the input letter h. That will go into the first RNN cell, and then we&#39;ll produce this output, y t, which is the network making predictions about, for each letter in the vocabulary, which letter does it think is most likely going to come next. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">In this example, the correct output letter was e because our training sequence was hello, but the model is actually predicting, I think it&#39;s actually predicting o as the most likely letter. So in this case, this prediction was wrong and we would use softmax loss to quantify our unhappiness with these predictions. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>The </span><span class="c3">next time step</span><span class="c1">, we would feed in the second letter in the training sequence, e, and this process will repeat. We&#39;ll now represent e as a vector. Use that input vector together with the previous hidden state to produce a new hidden state and now use the second hidden state to, again, make predictions over every letter in the vocabulary. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">In this case, because our training sequence was hello, after the letter e, we want our model to predict l. In this case, our model may have very low predictions for the letter l, so we would incur high loss. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>And you kind of </span><span class="c3">repeat this process</span><span class="c1">&nbsp;over and over, and if you train this model with many different sequences, then eventually it should learn how to predict the next character in a sequence based on the context of all the previous characters that it&#39;s seen before. </span></li><li class="c5 li-bullet-0"><span>And now, if you think about what happens </span><span class="c3 c18">at test time</span><span>, after we train this model, one thing that we might want to do with it is a </span><span class="c3">sample from the model</span><span>, and actually use this trained neural network model to </span><span class="c3">synthesize new text</span><span class="c1">&nbsp;that kind of looks similar in spirit to the text that it was trained on. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>The way that this will work is we&#39;ll typically see the model with some </span><span class="c3">input prefix of text</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">In this case, the prefix is just the single letter h, </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2"><li class="c10 c27 li-bullet-0"><span>and now we&#39;ll </span><span class="c3">feed that letter h through the first time step</span><span>&nbsp;of our recurrent neural network. It will produce this </span><span class="c3">distribution of scores over all the characters</span><span class="c1">&nbsp;in the vocabulary. </span></li><li class="c10 c27 li-bullet-0"><span>Now, at training time, we&#39;ll </span><span class="c3">use these scores to actually sample</span><span class="c1">&nbsp;from it. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span>So we&#39;ll </span><span class="c3">use a softmax function to convert those scores into a probability distribution</span><span class="c1">&nbsp;and </span></li><li class="c10 c35 li-bullet-0"><span>then we will </span><span class="c3">sample from that probability distribution</span><span>&nbsp;to actually </span><span class="c3">synthesize the second letter</span><span class="c1">&nbsp;in the sequence. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-4 start"><li class="c10 c39 li-bullet-0"><span class="c1">And in this case, even though the scores were pretty bad, maybe we got lucky and sampled the letter e from this probability distribution. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2"><li class="c10 c27 li-bullet-0"><span>And now, we&#39;ll take this letter e that was sampled from this distribution and </span><span class="c3">feed it back as input into the network at the next time step</span><span class="c1">. Now, we&#39;ll take this e, pull it down from the top, feed it back into the network as one of these, sort of, one hot vector representations, </span></li><li class="c10 c27 li-bullet-0"><span>and then </span><span class="c3">repeat the process</span><span>&nbsp;in order to </span><span class="c3">synthesize the second letter in the output</span><span class="c1">. </span></li><li class="c10 c27 li-bullet-0"><span>And we can </span><span class="c3">repeat this process</span><span>&nbsp;[with another input prefix or the same (we </span><span class="c18 c3">sample</span><span>&nbsp;the character from a probability distribution, ie. even the same input prefix might produce a different result !)] over and over again </span><span class="c3">to synthesize a new sequence using this trained model</span><span class="c1">, where we&#39;re synthesizing the sequence one character at a time using these predicted probability distributions at each time step. </span></li></ul><h3 class="c0" id="h.lms6rss1klhv"><span>Questions</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">Questions: </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">Why might we sample instead of just taking the character with the largest score? </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">In this case, because of the probability distribution that we had, it was impossible to get the right character, so we had the sample so the example could work out, and it would make sense. But in practice, sometimes you&#39;ll see both. So sometimes you&#39;ll just take the argmax probability, and that will sometimes be a little bit more stable, but one advantage of sampling, in general, is that it lets you get diversity from your models. Sometimes you might have the same input, maybe the same prefix, or in the case of image captioning, maybe the same image. But then if you sample rather than taking the argmax, then you&#39;ll see that sometimes these trained models are actually able to produce multiple different types of reasonable output sequences, depending on the kind, depending on which samples they take at the first time steps. It&#39;s actually kind of a benefit cause we can get now more diversity in our outputs. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span class="c1">At test time, could we feed in this whole softmax vector rather than a one hot vector? </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">There&#39;s kind of two problems with that. One is that that&#39;s very different from the data that it saw at training time. In general, if you ask your model to do something at test time, which is different from training time, then it&#39;ll usually blow up. It&#39;ll usually give you garbage and you&#39;ll usually be sad. The other problem is that in practice, our vocabularies might be very large. So maybe, in this simple example, our vocabulary is only four elements, so it&#39;s not a big problem. But if you&#39;re thinking about generating words one at a time, now your vocabulary is every word in the English language, which could be something like tens of thousands of elements. So in practice, this first element, this first operation that&#39;s taking in this one hot vector, is often performed using sparse vector operations rather than dense factors. It would be, sort of, computationally really bad if you wanted to have this load of 10,000 elements softmax vector. So that&#39;s usually why we use a one hot instead, even at test time. </span></li></ul><h3 class="c0" id="h.n9p64x93el07"><span>Truncated Backpropagation Through Time</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>This idea that we have a sequence and we produce an output at every time step of the sequence and then finally compute some loss, this is sometimes called </span><span class="c3 c12">backpropagation through time</span><span>&nbsp;because you&#39;re imagining that in the forward pass, you&#39;re kind of stepping forward through time and then </span><span class="c3">during the backward pass, you&#39;re sort of going backwards through time to compute all your gradients</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>This </span><span class="c3">can actually be kind of problematic</span><span class="c1">&nbsp;if you want to train the sequences that are very, very long. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>So if you imagine that we were kind of trying to train a neural network language model on maybe the </span><span class="c3">entire text of Wikipedia</span><span>, which is, by the way, something that people do pretty frequently, this would be super slow, and every time we made a gradient step, we would have to make a forward pass through the entire text of all of wikipedia, and then make a backward pass through all of wikipedia, and then make a single gradient update. And that would be super slow. </span><span class="c3">Your model would never converge</span><span>. It would also take a </span><span class="c3">ridiculous amount of memory</span><span class="c1">&nbsp;so this would be just really bad. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span>In practice, what people do is this, sort of, approximation called </span><span class="c3 c12">truncated backpropagation through time</span><span>. Here, the idea is that, even though our input sequence is very, very long, and even potentially infinite, what we&#39;ll do is that during, when we&#39;re training the model, we&#39;ll step forward for some number of steps, maybe like a hundred is kind of a ballpark number that people frequently use, and we&#39;ll step forward for maybe a hundred steps, </span><span class="c3">compute a loss only over this sub sequence of the data</span><span>, and then </span><span class="c3">back propagate through this sub sequence</span><span>, and now make a gradient step. And now, when we repeat, well, we still have these hidden states that we computed from the first batch, and now, </span><span class="c3">when we compute this next batch of data, we will carry those hidden states forward in time</span><span>, so the forward pass will be exactly the same. But now when we compute a gradient step for this next batch of data, we will </span><span class="c3">only backpropagate again through this second batch</span><span>. Now, we&#39;ll </span><span class="c3">make a gradient step based on this truncated backpropagation through time</span><span class="c1">. This process will continue, where now when we make the next batch, we&#39;ll again copy these hidden states forward, but then step forward and then step backward, but only for some small number of time steps. </span></li><li class="c10 c35 li-bullet-0"><span>You can kind of think of this as being </span><span class="c18 c3">analogous to stochastic gradient descent in the case of sequences</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-4 start"><li class="c10 c39 li-bullet-0"><span class="c1">Remember, when we talked about training our models on large data sets. It would be super expensive to compute the gradients over every element in the data set. So instead, we kind of take small samples, small mini batches instead, and use mini batches of data to compute gradient steps in the image classification case. </span></li></ul><h3 class="c0" id="h.pvbik77ywsdy"><span>Questions</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">Is this kind of making the Markov assumption? </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">No, not really. Because we&#39;re carrying this hidden state forward in time forever. It&#39;s making a Marcovian assumption in the sense that, conditioned on the hidden state, but the hidden state is all that we need to predict the entire future of the sequence. But that assumption is kind of built into the recurrent neural network formula from the start. And that&#39;s not really particular to backpropagation through time. Backpropagation through time, or sorry, truncated backprop through time is just the way to approximate these gradients without going making a backwards pass through your potentially very large sequence of data. </span></li></ul><h3 class="c0" id="h.mdzry3l3fy5b"><span class="c21">min-char-rnn (Karpathy)</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>This all sounds very complicated and confusing and it sounds like a lot of </span><span class="c18 c3">code</span><span>&nbsp;to write, but in fact, this can actually be pretty concise. Andrej has this example of what he calls </span><span class="c3 c12">min-char-rnn </span><span>(</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://gist.github.com/karpathy/d4dee566867f8291f086&amp;sa=D&amp;source=editors&amp;ust=1642006108755677&amp;usg=AOvVaw3vtyiF9PB4joxaCLGtSh9O">git</a></span><span class="c1">), that does all of this stuff in just 112 lines of Python. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">[What it does:]</span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>It handles </span><span class="c3">building the vocabulary</span><span class="c1">. </span></li><li class="c10 c27 li-bullet-0"><span>It </span><span class="c3">trains the model with truncated backpropagation through time</span><span class="c1">. </span></li><li class="c10 c27 li-bullet-0"><span>And then, it </span><span class="c3">can actually sample from that model</span><span class="c1">&nbsp;in actually not too much code. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span class="c1">So even though this sounds like kind of a big, scary process, it&#39;s actually not too difficult. I&#39;d encourage you, if you&#39;re confused, to maybe go check this out and step through the code on your own time, and see, kind of, all of these concrete steps happening in code. </span></li><li class="c5 li-bullet-0"><span>So this is all in just a </span><span class="c3">single file</span><span>, all </span><span class="c3">using numpy with no dependencies</span><span class="c1">. This was relatively easy to read. </span></li></ul><h3 class="c0" id="h.kyaj028nie3l"><span class="c21">Example RNN language models</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So then, once we have this idea of training a </span><span class="c3">recurrent neural network language model</span><span>, we can actually have a lot of fun with this. And </span><span class="c3">we can take in, sort of, any text that we want</span><span class="c1">. Take in, like, whatever random text you can think of from the internet, train our recurrent neural network language model on this text, and then generate new text. </span></li></ul><h4 class="c14" id="h.1k5565xonjex"><span>Shakespeare</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">So in this example, we took this entire text of all of Shakespeare&#39;s works, and then used that to train a recurrent neural network language model on all of Shakespeare. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>And you can see that </span><span class="c3">at the beginning of training</span><span class="c1">, it&#39;s kind of producing maybe random gibberish garbage, </span></li><li class="c5 li-bullet-0"><span>but </span><span class="c3">throughout the course of training</span><span class="c1">, it ends up producing things that seem relatively reasonable. </span></li><li class="c5 li-bullet-0"><span>And </span><span class="c3">after this model has been trained</span><span class="c1">&nbsp;pretty well, then it produces text that seems, kind of, Shakespeare-esque to me. &quot;Why do what that day,&quot; replied, whatever, right, you can read this. Like, it kind of looks like Shakespeare. </span></li><li class="c5 li-bullet-0"><span>And </span><span class="c3">if you actually train this model even more</span><span class="c1">, and let it converge even further, and then sample these even longer sequences, you can see that it learns all kinds of crazy cool stuff that really looks like a Shakespeare play. It knows that it uses, maybe, these headings to say who&#39;s speaking. Then it produces these bits of text that have crazy dialogue that sounds kind of Shakespeare-esque. It knows to put line breaks in between these different things. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">And this is all, like, really cool, all just sort of learned from the structure of the data. </span></li></ul><h4 class="c14" id="h.xdix8ue8bo1i"><span>LaTeX text</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">We can actually get even crazier than this. This was one of my favorite examples. I found online, there&#39;s this. Is anyone a mathematician in this room? Has anyone taken an algebraic topology course by any chance? Wow, a couple, that&#39;s impressive. So you probably know more algebraic topology than me, </span></li><li class="c10 c15 li-bullet-0"><span class="c1">but I found this open source algebraic topology textbook online. It&#39;s just a whole bunch of text files that are like this super dense mathematics. And LaTeX, cause LaTeX lets you write equations and diagrams and everything just using plain text. </span></li><li class="c10 c15 li-bullet-0"><span>We can actually </span><span class="c3">train our recurrent neural network language model on the raw LateX source code</span><span class="c1">&nbsp;of this algebraic topology textbook. And if we do that, then after we sample from the model, then we get something that seems like, kind of like algebraic topology. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">So it knows to like put equations. It puts all kinds of crazy stuff. It&#39;s like, to prove study, we see that F sub U is a covering of x prime, blah, blah, blah, blah, blah. It knows where to put unions. It knows to put squares at the end of proofs. It makes lemmas. It makes references to previous lemmas. Right, like we hear, like. It&#39;s namely a bi-lemma question. We see that R is geometrically something. So it&#39;s actually pretty crazy. </span></li><li class="c10 c15 li-bullet-0"><span>It also sometimes </span><span class="c3">tries to make diagrams</span><span class="c1">. For those of you that have taken algebraic topology, you know that these commutative diagrams are kind of a thing that you work with a lot. So it kind of got the general gist of how to make those diagrams, but they actually don&#39;t make any sense. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">And actually, one of my favorite examples here is that it sometimes omits proofs. So it&#39;ll sometimes say, it&#39;ll sometimes say something like theorem, blah, blah, blah, blah, blah, proof omitted. </span></li><li class="c10 c15 li-bullet-0"><span>This thing kind of </span><span class="c3">has gotten the gist of how some of these math textbooks look like</span><span class="c1">. We can have a lot of fun with this. </span></li></ul><h4 class="c14" id="h.7ll6oguzqdjz"><span>C code</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So we also tried </span><span class="c3">training</span><span>&nbsp;one of these models </span><span class="c3">on the entire source code of the Linux kernel</span><span>. &#39;Cause again, this character level stuff that we can train on, And then, when we sample this, it actually again </span><span class="c3">looks like C source code</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">It knows how to write if statements. It has, like, pretty good code formatting skills. It knows to indent after these if statements. It knows to put curly braces. It actually even makes comments about some things that are usually nonsense. </span></li><li class="c10 c15 li-bullet-0"><span>One </span><span class="c3">problem with this model</span><span class="c1">&nbsp;is that it knows how to declare variables. But </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>it </span><span class="c3">doesn&#39;t always use the variables that it declares</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>And sometimes it </span><span class="c3">tries to use variables that haven&#39;t been declared</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c3">This wouldn&#39;t compile</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">I would not recommend sending this as a pull request to Linux. </span></li><li class="c10 c15 li-bullet-0"><span>This thing also </span><span class="c3">figures out how to recite this GNU license</span><span class="c1">&nbsp;character by character. It kind of knows that you need to recite the GNU license and after the license comes some includes, then some other includes, then source code. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">This thing has actually learned quite a lot about the general structure of the data. Where, again, during training, all we asked this model to do was try to predict the next character in the sequence. We didn&#39;t tell it any of this structure, but somehow, just through the course of this training process, it learned a lot about the latent structure in the sequential data. Yeah, so it knows how to write code. It does a lot of cool stuff. </span></li></ul><h3 class="c0" id="h.lnt9o2eig4l8"><span>Visualizing and understanding RNNs</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>I had this </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1506.02078.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108764525&amp;usg=AOvVaw2S4XYVR0TKuFrkEgWRw6el">paper</a></span><sup><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup><span>&nbsp;with Andrej a couple years ago where we trained a bunch of these models and then we wanted to try to poke into the brains of these models and figure </span><span>out like </span><span class="c18 c3">what</span><span class="c18 c3">&nbsp;</span><span class="c18 c3">are they</span><span class="c18 c3">&nbsp;doing</span><span>&nbsp;and </span><span class="c18 c3">why </span><span class="c18 c3">are they</span><span class="c18 c3">&nbsp;working</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So we saw, these recurrent neural networks have this </span><span class="c3 c12">hidden vector</span><span>&nbsp;which is, maybe, </span><span class="c3">some vector that&#39;s updated over every time step</span><span>. And then what we wanted to try to figure out is, could we find </span><span class="c3">some elements of this vector</span><span>&nbsp;that have some </span><span class="c3">semantic interpretable meaning</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span class="c1">So what we did is </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c3">we trained</span><span>&nbsp;a neural network language model, one of these </span><span class="c3">character level models</span><span class="c1">&nbsp;on one of these data sets, and then </span></li><li class="c10 c27 li-bullet-0"><span class="c3">we picked one of the elements in that hidden vector</span><span class="c1">&nbsp;and now </span></li><li class="c10 c27 li-bullet-0"><span class="c3">we look at what is the value of that hidden vector over the course of a sequence</span><span class="c1">&nbsp;to try to get some sense of maybe what these different hidden states are looking for. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span class="c1">When you do this, a lot of them end up looking kind of like random gibberish garbage. </span></li><li class="c5 li-bullet-0"><span class="c1">So here again, what we&#39;ve done, is </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">we&#39;ve picked one element of that vector, and now </span></li><li class="c10 c27 li-bullet-0"><span class="c1">we run the sequence forward through the trained model, and now </span></li><li class="c10 c27 li-bullet-0"><span>the </span><span class="c3">color of each character</span><span>&nbsp;corresponds to the </span><span class="c3">magnitude of that single scalar element of the hidden vector</span><span class="c1">&nbsp;at every time step when it&#39;s reading the sequence. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>So you can see that a lot of the </span><span class="c18 c3">vectors in these hidden states</span><span>&nbsp;are kind of </span><span class="c18 c3">not very interpretable</span><span class="c1">. It seems like they&#39;re kind of doing some of this low level language modeling to figure out what character should come next. </span></li><li class="c5 li-bullet-0"><span class="c3 c12">But</span><span class="c1">&nbsp;some of them end up quite nice: </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>So here </span><span class="c3">we found this vector that is looking for </span><span class="c18 c3">quotes</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">You can see that there&#39;s this one hidden element, this one element in the vector, that is off, off, off, off, off blue and then once it hits a quote, it turns on and remains on for the duration of this quote. And now when we hit the second quotation mark, then that cell turns off. </span></li><li class="c10 c35 li-bullet-0"><span class="c1">So somehow, even though this model was only trained to predict the next character in a sequence, it somehow learned that a useful thing, in order to do this, might be to have some cell that&#39;s trying to detect quotes. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2"><li class="c10 c27 li-bullet-0"><span class="c3">We also found this other cell that looks like it&#39;s </span><span class="c18 c3">counting the number of characters since a line break</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span class="c1">So you can see that at the beginning of each line, this element starts off at zero. Throughout the course of the line, it&#39;s gradually more red, so that value increases. And then after the new line character, it resets to zero. </span></li><li class="c10 c35 li-bullet-0"><span>So you can imagine that maybe this cell is letting the network </span><span class="c3">keep track of when it needs to produce these new line characters</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2"><li class="c10 c27 li-bullet-0"><span>When we trained on the linux source code, </span><span class="c3">we found some examples that are turning on </span><span class="c18 c3">inside the conditions of if statements</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span>So this </span><span>maybe</span><span>&nbsp;</span><span>allows</span><span class="c1">&nbsp;the network to differentiate whether it&#39;s outside an if statement or inside that condition, which might help it model these sequences better. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2"><li class="c10 c27 li-bullet-0"><span class="c3">We also found some that turn on </span><span class="c18 c3">in comments</span><span class="c1">, </span></li><li class="c10 c27 li-bullet-0"><span>or </span><span class="c3">some that seem like they&#39;re </span><span class="c18 c3">counting the number of indentation levels</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>This is all just really cool stuff because </span><span class="c3">it&#39;s saying that even though we are only trying to train this model to predict next characters, it somehow ends up learning a lot of useful structure about the input data</span><span class="c1">. </span></li></ul><h3 class="c0" id="h.1cnnp53tybop"><span class="c21">Image Captioning</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">One kind of thing that we often use, so this has not really been computer vision so far, and we need to pull this back to computer vision since this is a vision class. </span></li><li class="c10 c15 li-bullet-0"><span>We&#39;ve alluded many times to this </span><span class="c3 c12">image captioning model</span><span>&nbsp;where we want to build models that can </span><span class="c3">input an image</span><span>&nbsp;and then </span><span class="c3">output a caption</span><span class="c1">&nbsp;in natural language. </span></li><li class="c10 c15 li-bullet-0"><span>There were a bunch of papers</span><sup><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup><span>&nbsp;a couple years ago that all had relatively similar approaches. But I&#39;m showing the figure from the </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1412.2306.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108772460&amp;usg=AOvVaw1fqYzaPLl3QbYSvPZ_Nivs">paper from our lab</a></span><span class="c1">&nbsp;in a totally unbiased way. </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 352.50px; height: 198.50px;"><img alt="" src="images/image81.png" style="width: 352.50px; height: 198.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></li><li class="c10 c15 li-bullet-0"><span class="c1">But, the idea here is that the caption is this variable length sequence, that is, the sequence might have different numbers of words for different captions. So this is a totally natural fit for a recurrent neural network language model. </span></li><li class="c10 c15 li-bullet-0"><span>So then what this model looks like is we have some </span><span class="c3">convolutional network</span><span>&nbsp;which will input the, which will take as input the image, and we&#39;ve seen a lot about how convolution networks work at this point, and that convolutional network will produce a summary vector of the image which will then </span><span class="c3">feed into the first time step of one of these recurrent neural network language models</span><span class="c1">&nbsp;which will then produce words of the caption one at a time. </span></li><li class="c10 c15 li-bullet-0"><span>So the way that this kind of works </span><span class="c18 c3">at test time</span><span>&nbsp;after the model is trained looks almost exactly the </span><span class="c3">same as these character level language models</span><span class="c1">&nbsp;that we saw a little bit ago. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">We&#39;ll take our input image, feed it through our convolutional network. </span></li><li class="c5 li-bullet-0"><span class="c1">But now instead of taking the softmax scores from an image net model, we&#39;ll instead take this 4,096 dimensional vector from the end of the model, and we&#39;ll take that vector and use it to summarize the whole content of the image. </span></li><li class="c5 li-bullet-0"><span>Now, remember when we talked about RNN language models, we said that we need to see the language model with that first initial input to tell it to start generating text. So in this case, we&#39;ll give it some </span><span class="c3">special start token</span><span>, which is just saying, hey, this is the </span><span class="c3">start of a sentence</span><span class="c1">. Please start generating some text conditioned on this image information. </span></li><li class="c5 li-bullet-0"><span>So now previously, we saw that in this RNN language model, we had these </span><span class="c3">matrices</span><span>&nbsp;that were taking the previous, the input at the current time step and the hidden state of the previous time step and combining those to get the next hidden state. Well now, we also need to </span><span class="c3">add in this image information</span><span>. So one way, people play around with exactly different ways to incorporate this image information, but one simple way is just to </span><span class="c3">add a third weight matrix that is adding in this image information at every time step</span><span class="c1">&nbsp;to compute the next hidden state. </span></li><li class="c5 li-bullet-0"><span>So now, we&#39;ll compute this </span><span class="c3">distribution over all scores in our vocabulary</span><span class="c1">&nbsp;and here, our vocabulary is something like all English words, so it could be pretty large. </span></li><li class="c5 li-bullet-0"><span>We&#39;ll </span><span class="c3">sample from that distribution</span><span>&nbsp;and now </span><span class="c3">pass that word back as input at the next time step</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span class="c1">And that will then feed that word in, again get a distribution over all words in the vocab, and again sample to produce the next word. </span></li><li class="c5 li-bullet-0"><span>So then, after that thing is all done, we&#39;ll maybe generate, we&#39;ll </span><span class="c3">generate this complete sentence</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span>We </span><span class="c3">stop generation once we sample the special end token</span><span class="c1">, which kind of corresponds to the period at the end of the sentence. Then once the network samples this end token, we stop generation and we&#39;re done and we&#39;ve gotten our caption for this image. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">And now, during training, we trained this thing to put an end token at the end of every caption, so that the network kind of learned during training that end tokens come at the end of sequences. So then, during test time, it tends to sample these end tokens once it&#39;s done generating. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So we trained this model </span><span>in kind of a</span><span>&nbsp;completely supervised way. You can find </span><span class="c18 c3">data sets</span><span class="c3">&nbsp;that have images together with natural language captions</span><span>. </span><span class="c3 c12">Microsoft COCO</span><span class="c1">&nbsp;is probably the biggest and most widely used for this task. But you can just train this model in a purely supervised way. And then backpropagate through to jointly train both this recurrent neural network language model and then also pass gradients back into this final layer of the CNN and additionally update the weights of the CNN to jointly tune all parts of the model to perform this task. </span></li><li class="c10 c15 li-bullet-0"><span>Once you train these models, they actually do some pretty reasonable things. These are </span><span class="c3">some real results</span><span class="c1">&nbsp;from a model, from one of these trained models, and it says things like </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">&ldquo;a cat sitting on a suitcase on the floor&rdquo;, which is pretty impressive. </span></li><li class="c5 li-bullet-0"><span class="c1">It knows about &ldquo;cats sitting on a tree branch&rdquo;, which is also pretty cool. </span></li><li class="c5 li-bullet-0"><span>It </span><span>knows</span><span class="c1">&nbsp;about &ldquo;two people walking on the beach with surfboards&rdquo;. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So these models are actually pretty powerful and </span><span class="c3">can produce relatively complex captions</span><span class="c1">&nbsp;to describe the image. </span></li></ul><h4 class="c14" id="h.vvsqh1bmqm19"><span class="c17">Caveats</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>But that being said, these models are really </span><span class="c3">not perfect</span><span>. They&#39;re not magical. Just like any machine learning model, if you try to run them on </span><span class="c3">data that is very different from the training data</span><span class="c1">, they don&#39;t work very well. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So for </span><span class="c3">example</span><span class="c1">, this example, it says a &ldquo;woman is holding a cat in her hand&rdquo;. There&#39;s clearly no cat in the image. But she is wearing a fur coat, and maybe the texture of that coat kind of looked like a cat to the model. </span></li><li class="c5 li-bullet-0"><span class="c1">Over here, we see a &ldquo;woman standing on a beach holding a surfboard&rdquo;. Well, she&#39;s definitely not holding a surfboard and she&#39;s doing a handstand, which is maybe the interesting part of that image, and the model totally missed that. </span></li><li class="c5 li-bullet-0"><span>Also, over here, we see this example where there&#39;s this picture of a spider web in the tree branch, and it says something like &ldquo;a bird sitting on a tree branch&rdquo;. So it totally missed the spider, but during training, it never really saw examples of spiders. It just knows that birds sit on tree branches during training. So it kind of </span><span class="c3">makes these reasonable mistakes</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span class="c1">Or here at the bottom, it can&#39;t really tell the difference between this guy throwing and catching the ball, but it does know that it&#39;s a baseball player and there&#39;s balls and things involved. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So again, I </span><span>just</span><span>&nbsp;want to say that </span><span class="c3">these models are not perfect</span><span>. They work pretty well when you ask them to caption images that </span><span>were</span><span>&nbsp;</span><span class="c3">similar to the training data</span><span class="c1">, but they definitely have a hard time generalizing far beyond that. </span></li></ul><h3 class="c0" id="h.nvgcn5ud57x"><span>Visual </span><span class="c21">Attention model</span></h3><p class="c10"><span>[</span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1502.03044.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108780954&amp;usg=AOvVaw0JKPRK9SmJE5dmmDQ27I5H">Xu 2016, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></span><span class="c1">]</span></p><h4 class="c14" id="h.dkdsxtdrmso0"><span class="c17">Image Captioning</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So another thing you&#39;ll sometimes see is this slightly more advanced model called </span><span class="c3 c12">Attention</span><span>, where now when we&#39;re generating the words of this caption, we can allow the model to </span><span class="c3">steer it&#39;s attention to different parts of the image</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">And I don&#39;t want to spend too much time on this. But the general way that this works is that </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>now our </span><span class="c3">convolutional network</span><span>, rather than producing a single vector summarizing the entire image, </span><span>now it </span><span class="c3">produces</span><span class="c3">&nbsp;some grid of vectors that summarize the, that give maybe one vector for each spatial location in the image</span><span class="c1">. </span></li><li class="c5 li-bullet-0"><span class="c1">And now, when we, when this model runs forward, </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>1. in addition to </span><span class="c3">sampling the vocabulary at every time step</span><span class="c1">, </span></li><li class="c10 c27 li-bullet-0"><span>2. it also produces a </span><span class="c3">distribution over the locations in the image where it wants to look</span><span>. And now this distribution over image locations can be seen as a kind of </span><span class="c3 c12">attention</span><span class="c1">&nbsp;of where the model should look during training. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span class="c1">So now that first hidden state computes this distribution over image locations, which then goes back to the set of vectors to give a single summary vector that maybe focuses the attention on one part of that image. And now that summary vector gets fed, as an additional input, at the next time step of the neural network. And now again, it will produce two outputs. One is our distribution over vocabulary words. And the other is a distribution over image locations. This whole process will continue, and it will sort of do these two different things at every time step. And after you train the model, then you can see that it kind of will shift it&#39;s attention around the image for every word that it generates in the caption. Here you can see that it produced the caption, a bird is flying over, I can&#39;t see that far. But you can see that its attention is shifting around different parts of the image for each word in the caption that it generates. </span></li><li class="c5 li-bullet-0"><span>There&#39;s this notion of </span><span class="c3 c12">hard attention</span><span>&nbsp;versus </span><span class="c3 c12">soft attention</span><span class="c1">, which I don&#39;t really want to get into too much, </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>but with this idea of </span><span class="c3">soft attention</span><span class="c1">, we&#39;re kind of taking a weighted combination of all features from all image locations, </span></li><li class="c10 c27 li-bullet-0"><span>whereas in the </span><span class="c3">hard attention</span><span>&nbsp;case, we&#39;re forcing the model to select exactly one location to look at in the image at each time step. So the hard attention case where we&#39;re selecting exactly one image location is a little bit tricky because that is not really a differentiable function, so you need to do something slightly fancier than vanilla backpropagation in order to just train the model in that scenario. And I think we&#39;ll talk about that a little bit later in the lecture on </span><span class="c3 c12">reinforcement learning</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>Now, when you look at after you train one of these attention models and then run it on to generate captions, you can see that </span><span class="c3">it tends to focus its attention on maybe the salient or semantically meaningful part of the image when generating captions</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">You can see that the caption was a woman is throwing a frisbee in a park and you can see that this attention mask, when it generated the word, when the model generated the word frisbee, at the same time, it was focusing it&#39;s attention on this image region that actually contains the frisbee. This is actually really cool. We did not tell the model where it should be looking at every time step. It sort of figured all that out for itself during the training process. Because somehow, it figured out that looking at that image region was the right thing to do for this image. And because everything in this model is differentiable, because we can backpropagate through all these soft attention steps, all of this soft attention stuff just comes out through the training process. So that&#39;s really, really cool. </span></li></ul><h4 class="c14" id="h.y2bg2khv23d9"><span>Visual question answering</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>By the way, this idea of </span><span class="c3">recurrent neural networks and attention</span><span>&nbsp;actually gets used </span><span class="c18 c3">in other tasks beyond image captioning</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>One recent example is this idea of </span><span class="c3 c12">visual question answering</span><span>. So here, our model is going to take two things as input. It&#39;s going to take an image and it will also take a natural language question that&#39;s asking some </span><span class="c3">question about the image</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">Here, we might see this image on the left and we might ask the question, &ldquo;what endangered animal is featured on the truck&rdquo;? And now the model needs to select from one of these four natural language answers about which of these answers correctly answers that question in the context of the image. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1"><li class="c5 li-bullet-0"><span>So you can </span><span>imagine kind of stitching</span><span>&nbsp;this model together using CNNs and RNNs </span><span>in kind of a</span><span class="c1">&nbsp;natural way. </span></li><li class="c5 li-bullet-0"><span>Now, we&#39;re in this </span><span class="c3">many to one scenario</span><span class="c1">, where now our model needs to take as input this natural language sequence, so we can imagine running a recurrent neural network over each element of that input question, to now summarize the input question in a single vector. And then we can have a CNN to again summarize the image, and now combine both the vector from the CNN and the vector from the question and coding RNN to then predict a distribution over answers. </span></li><li class="c5 li-bullet-0"><span>We also sometimes, you&#39;ll also sometimes see this idea of </span><span class="c3">soft spatial attention</span><span>&nbsp;being </span><span class="c3">incorporated into things like visual question answering</span><span class="c1">. So you can see that here, this model is also having the spatial attention over the image when it&#39;s trying to determine answers to the questions. </span></li></ul><h4 class="c14" id="h.9iol93yu1f4p"><span>Questions</span></h4><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">Questions: </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">How are the different inputs combined? Do you mean like the encoded question vector and the encoded image vector? Yeah, so the question is how are the encoded image and the encoded question vector combined? </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">Kind of the simplest thing to do is just to concatenate them and stick them into fully connected layers. That&#39;s probably the most common and that&#39;s probably the first thing to try. Sometimes people do slightly fancier things where they might try to have multiplicative interactions between those two vectors to allow a more powerful function. But generally, concatenation is kind of a good first thing to try. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>Okay, so now we&#39;ve talked about a bunch of scenarios where RNNs are used for different kinds of problems. And I think it&#39;s super cool because it allows you to start tackling really complicated problems combining images and computer vision with natural language processing. And you can see that we can kind of stitch together these models like Lego blocks and attack really complicated things, like image captioning or visual question answering </span><span class="c3">just by stitching together these relatively simple types of neural network modules</span><span class="c1">. </span></li></ul><h3 class="c0" id="h.4wygwgd6dt9h"><span>Multilayer RNN</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>But I&#39;d also like to mention that so far, we&#39;ve talked about this idea of a single recurrent network layer, where we have sort of one hidden state, and another thing that you&#39;ll see pretty commonly is this idea of a </span><span class="c3 c12">multilayer recurrent neural network</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">Here, this is a three layer recurrent neural network, </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>so now our input goes in, goes into, goes in and produces a sequence of hidden states from the </span><span class="c3">first recurrent neural network layer</span><span class="c1">. And now, after we run kind of one recurrent neural network layer, then we have this whole sequence of hidden states. </span></li><li class="c10 c15 li-bullet-0"><span>And now, we can </span><span class="c3">use the sequence of hidden states as an input sequence to another recurrent neural network layer</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span>And then you can just imagine, which will then produce another sequence of hidden states from the </span><span class="c3">second RNN layer</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span>And then you can just imagine stacking these things on top of each other, </span><span>cause</span><span>&nbsp;we know that we&#39;ve seen in other contexts that </span><span class="c3">deeper models tend to perform better</span><span class="c1">&nbsp;for various problems. And the same kind of holds in RNNs as well. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>For many problems, you&#39;ll see maybe a </span><span class="c3">two or three layer</span><span>&nbsp;recurrent neural network model is </span><span class="c3">pretty commonly</span><span class="c1">&nbsp;used. </span></li><li class="c5 li-bullet-0"><span>You typically </span><span class="c3">don&#39;t</span><span class="c1">&nbsp;see super deep models in RNNs. </span></li><li class="c5 li-bullet-0"><span>So generally, like </span><span class="c3">two, three, four layer</span><span>&nbsp;RNNs is maybe as deep as you&#39;ll </span><span class="c3">typically</span><span class="c1">&nbsp;go. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>Then, I think it&#39;s also really interesting and important to think about, now we&#39;ve </span><span>seen kind of what</span><span>&nbsp;kinds of problems these RNNs can be used for, but then you need to think a little bit more carefully about exactly </span><span class="c3">what happens to these models when we try to train them</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>So here, I&#39;ve drawn this little </span><span class="c18 c3">vanilla RNN</span><span class="c1">&nbsp;cell that we&#39;ve talked about so far. </span></li><li class="c5 li-bullet-0"><span class="c1">So here, we&#39;re taking our current input, x t, and our previous hidden state, h t minus one, and then we stack, those are two vectors. So we can just stack them together. And then perform this matrix multiplication with our weight matrix, to give our, and then squash that output through a tanh, and that will give us our next hidden state. And that&#39;s kind of the basic functional form of this vanilla recurrent neural network. </span></li><li class="c5 li-bullet-0"><span>But then, we need to think about </span><span class="c3">what happens</span><span>&nbsp;in this architecture </span><span class="c18 c3">during the backward pass</span><span>&nbsp;</span><span class="c3">when we try to compute gradients</span><span class="c1">? </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span>So then if we think about trying to compute, </span><span>so then</span><span>&nbsp;during the backwards pass, we&#39;ll receive the derivative of our h t, we&#39;ll </span><span>receive derivative</span><span>&nbsp;of loss with respect to h t. And during the backward pass through the cell, we&#39;ll need to </span><span>compute derivative</span><span class="c1">&nbsp;of loss to the respect of h t minus one. Then, when we compute this backward pass, we see that the gradient flows backward through this red path. So first, that gradient will flow backwards through this tanh gate, and then it will flow backwards through this matrix multiplication gate. </span></li><li class="c10 c27 li-bullet-0"><span>And then, as we&#39;ve seen in the homework and when implementing these matrix multiplication layers, </span><span class="c3">when you backpropagate through this matrix multiplication gate, you end up multiplying by the transpose of that weight matrix</span><span class="c1">. So that means that every time we backpropagate through one of these vanilla RNN cells, we end up multiplying by some part of the weight matrix. </span></li><li class="c10 c27 li-bullet-0"><span class="c1">So now if you imagine that we are sticking many of these recurrent neural network cells in sequence, because again this is an RNN. We want to model sequences. </span></li><li class="c10 c27 li-bullet-0"><span>Now if you imagine </span><span class="c3">what happens to the gradient flow</span><span class="c1">&nbsp;through a sequence of these layers, then something kind of fishy starts to happen: </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-3 start"><li class="c10 c35 li-bullet-0"><span>Because now, when we want to compute the gradient of the loss with respect to h zero, we need to backpropagate through every one of these RNN cells. And every time you backpropagate through one cell, you&#39;ll pick up one of these w transpose factors. So that means that the </span><span class="c3">final expression for the gradient on h zero will involve many, many factors of this weight matrix, which could be kind of bad</span><span>. Maybe don&#39;t think about the weight, the matrix case, but imagine a scalar case. If we end up, if we have some scalar and we multiply by that same number over and over and over again, maybe not for four examples, but for something like a hundred or several hundred time steps, then multiplying by the same number over and over again is really bad. In the scalar case, it&#39;s either going to </span><span class="c3 c12">explode</span><span>&nbsp;in the case that that number is greater than one or it&#39;s going to </span><span class="c3 c12">vanish</span><span>&nbsp;towards zero in the case that number is less than one in absolute value. (And the only way in which this will not happen is if that number is exactly one, which is actually very rare to happen in practice.) That leaves us to, </span><span>that same</span><span>&nbsp;intuition extends to the matrix case, but now, rather than the absolute value of a scalar number, you instead need to look at the largest, the largest singular value of this weight matrix. Now if that largest singular value is greater than one, then during this backward pass, when we multiply by the weight matrix over and over, that gradient on h w, on h zero, sorry, will become very, very large, when that matrix is too large. And that&#39;s something we call the </span><span class="c3 c12">exploding gradient problem</span><span>. Where now this gradient will explode exponentially in depth with the number of time steps that we backpropagate through. And if the largest singular value is less than one, then we get the opposite problem, where now our gradients will shrink and shrink and shrink exponentially, as we backpropagate and pick up more and more factors of this weight matrix. That&#39;s called the </span><span class="c3 c12">vanishing gradient problem</span><span>. THere&#39;s a bit of a hack that people sometimes do to fix the exploding gradient problem called </span><span class="c3 c12">gradient clipping</span><span class="c1">, which is just this simple heuristic saying that after we compute our gradient, if that gradient, if it&#39;s L2 norm is above some threshold, then just clamp it down and divide, just clamp it down so it has this maximum threshold. This is kind of a nasty hack, but it actually gets used in practice quite a lot when training recurrent neural networks. And it&#39;s a relatively useful tool for attacking this exploding gradient problem. But now for the vanishing gradient problem, what we typically do is we might need to move to a more complicated RNN architecture. </span></li></ul><h3 class="c0" id="h.ilkrrdb778cq"><span class="c21">LSTM</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So that motivates this idea of an </span><span class="c3 c12">LSTM</span><span>. An LSTM, which stands for </span><span class="c3 c12">Long Short Term Memory</span><span>, is this slightly </span><span class="c3">fancier recurrence relation</span><span class="c1">&nbsp;for these recurrent neural networks. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>It&#39;s really </span><span class="c3">designed to help alleviate this problem of vanishing and exploding gradients</span><span class="c1">. So that rather than kind of hacking on top of it, we just kind of design the architecture to have better gradient flow properties. Kind of an analogy to those fancier CNN architectures that we saw at the top of the lecture. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>Another thing to point out is that the LSTM cell actually comes from </span><span class="c3">1997</span><span>. So this idea of an LSTM has been around for quite a while, and these </span><span>folks were</span><span class="c1">&nbsp;working on these ideas way back in the 90s, were definitely ahead of the curve. Because these models are kind of used everywhere now 20 years later. </span></li><li class="c10 c15 li-bullet-0"><span>And LSTMs kind of have this funny </span><span class="c3">functional form</span><span class="c1">. So remember when we had this vanilla recurrent neural network, it had this hidden state. And we used this recurrence relation to update the hidden state at every time step. Well, now in an LSTM, we actually have two, we maintain two hidden states at every time step. One is this h t, which is called the hidden state, which is kind of an analogy to the hidden state that we had in the vanilla RNN. But an LSTM also maintains the second vector, c t, called the cell state. And the cell state is this vector which is kind of internal, kept inside the LSTM, and it does not really get exposed to the outside world. And we&#39;ll see, and you can kind of see that through this update equation, where you can see that when we, first when we compute these, we take our two inputs, we use them to compute these four gates called i, f, o, n, g. We use those gates to update our cell states, c t, and then we expose part of our cell state as the hidden state at the next time step. This is kind of a funny functional form, and I want to walk through for a couple slides exactly why do we use this architecture and why does it make sense, especially in the context of vanishing or exploding gradients. This first thing that we do in an LSTM is that we&#39;re given this previous hidden state, h t, and we&#39;re given our current input vector, x t, and just like the vanilla RNN. In the vanilla RNN, remember, we took those two input vectors. We concatenated them. Then we did a matrix multiply to directly compute the next hidden state in the RNN. Now, the LSTM does something a little bit different. We&#39;re going to take our previous hidden state and our current input, stack them, and now multiply by a very big weight matrix, w, to compute four different gates, Which all have the same size as the hidden state. Sometimes, you&#39;ll see this written in different ways. Some authors will write a different weight matrix for each gate. Some authors will combine them all into one big weight matrix. But it&#39;s all really the same thing. The ideas is that we take our hidden state, our current input, and then we use those to compute these four gates. These four gates are the, you often see this written as i, f, o, g, ifog, which makes it pretty easy to remember what they are. I is the input gate. It says how much do we want to input into our cell. F is the forget gate. How much do we want to forget the cell memory at the previous, from the previous time step. O is the output gate, which is how much do we want to reveal ourself to the outside world. And G really doesn&#39;t have a nice name, so I usually call it the gate gate. G, it tells us how much do we want to write into our input cell. And then you notice that each of these four gates are using a different non linearity. The input, forget and output gate are all using sigmoids, which means that their values will be between zero and one. Whereas the gate gate uses a tanh, which means it&#39;s output will be between minus one and one. So, these are kind of weird, but it makes a little bit more sense if you imagine them all as binary values. Right, like what happens at the extremes of these two values? It&#39;s kind of what happens, if you look after we compute these gates if you look at this next equation, you can see that our cell state is being multiplied element wise by the forget gate. Sorry, our cell state from the previous time step is being multiplied element wise by this forget gate. And now if this forget gate, you can think of it as being a vector of zeros and ones, that&#39;s telling us for each element in the cell state, do we want to forget that element of the cell in the case if the forget gate was zero? Or do we want to remember that element of the cell in the case if the forget gate was one. Now, once we&#39;ve used the forget gate to gate off the part of the cell state, then we have the second term, which is the element wise product of i and g. So now, i is this vector of zeros and ones, cause it&#39;s coming through a sigmoid, telling us for each element of the cell state, do we want to write to that element of the cell state in the case that i is one, or do we not want to write to that element of the cell state at this time step in the case that i is zero. And now the gate gate, because it&#39;s coming through a tanh, will be either one or minus one. So that is the value that we want, the candidate value that we might consider writing to each element of the cell state at this time step. Then if you look at the cell state equation, you can see that at every time step, the cell state has these kind of these different, independent scaler values, and they&#39;re all being incremented or decremented by one. So there&#39;s kind of like, inside the cell state, we can either remember or forget our previous state, and then we can either increment or decrement each element of that cell state by up to one at each time step. So you can kind of think of these elements of the cell state as being little scaler integer counters that can be incremented and decremented at each time step. And now, after we&#39;ve computed our cell state, then we use our now updated cell state to compute a hidden state, which we will reveal to the outside world. So because this cell state has this interpretation of being counters, and sort of counting up by one or minus one at each time step, we want to squash that counter value into a nice zero to one range using a tanh. And now, we multiply element wise, by this output gate. And the output gate is again coming through a sigmoid, so you can think of it as being mostly zeros and ones, and the output gate tells us for each element of our cell state, do we want to reveal or not reveal that element of our cell state when we&#39;re computing the external hidden state for this time step. And then, I think there&#39;s kind of a tradition in people trying to explain LSTMs, that everyone needs to come up with their own potentially confusing LSTM diagram. So here&#39;s my attempt. Here, we can see what&#39;s going on inside this LSTM cell, is that we take our, we&#39;re taking as input on the left our previous cell state and the previous hidden state, as well as our current input, x t. Now we&#39;re going to take our current, our previous hidden state, as well as our current input, stack them, and then multiply with this weight matrix, w, to produce our four gates. And here, I&#39;ve left out the non linearities because we saw those on a previous slide. And now the forget gate multiplies element wise with the cell state. The input and gate gate are multiplied element wise and added to the cell state. And that gives us our next cell. The next cell gets squashed through a tanh, and multiplied element wise with this output gate to produce our next hidden state. Question? No, So they&#39;re coming through this, they&#39;re coming from different parts of this weight matrix. So if our hidden, if our x and our h all have this dimension h, then after we stack them, they&#39;ll be a vector size two h, and now our weight matrix will be this matrix of size four h times two h. So you can think of that as sort of having four chunks of this weight matrix. And each of these four chunks of the weight matrix is going to compute a different one of these gates. You&#39;ll often see this written for clarity, kind of combining all four of those different weight matrices into a single large matrix, w, just for notational convenience. But they&#39;re all computed using different parts of the weight matrix. But you&#39;re correct in that they&#39;re all computed using the same functional form of just stacking the two things and taking the matrix multiplication. Now that we have this picture, we can think about what happens to an LSTM cell during the backwards pass? We saw, in the context of vanilla recurrent neural network, that some bad things happened during the backwards pass, where we were continually multiplying by that weight matrix, w. But now, the situation looks much, quite a bit different in the LSTM. If you imagine this path backwards of computing the gradients of the cell state, we get quite a nice picture. Now, when we have our upstream gradient from the cell coming in, then once we backpropagate backwards through this addition operation, remember that this addition just copies that upstream gradient into the two branches, so our upstream gradient gets copied directly and passed directly to backpropagating through this element wise multiply. So then our upstream gradient ends up getting multiplied element wise by the forget gate. As we backpropagate backwards through this cell state, the only thing that happens to our upstream cell state gradient is that it ends up getting multiplied element wise by the forget gate. This is really a lot nicer than the vanilla RNN for two reasons. One is that this forget gate is now an element wise multiplication rather than a full matrix multiplication. So element wise multiplication is going to be a little bit nicer than full matrix multiplication. Second is that element wise multiplication will potentially be multiplying by a different forget gate at every time step. So remember, in the vanilla RNN, we were continually multiplying by that same weight matrix over and over again, which led very explicitly to these exploding or vanishing gradients. But now in the LSTM case, this forget gate can vary from each time step. Now, it&#39;s much easier for the model to avoid these problems of exploding and vanishing gradients. Finally, because this forget gate is coming out from a sigmoid, this element wise multiply is guaranteed to be between zero and one, which again, leads to sort of nicer numerical properties if you imagine multiplying by these things over and over again. Another thing to notice is that in the context of the vanilla recurrent neural network, we saw that during the backward pass, our gradients were flowing through also a tanh at every time step. But now, in an LSTM, our outputs are, in an LSTM, our hidden state is used to compute those outputs, y t, so now, each hidden state, if you imagine backpropagating from the final hidden state back to the first cell state, then through that backward path, we only backpropagate through a single tanh non linearity rather than through a separate tanh at every time step. So kind of when you put all these things together, you can see this backwards pass backpropagating through the cell state is kind of a gradient super highway that lets gradients pass relatively unimpeded from the loss at the very end of the model all the way back to the initial cell state at the beginning of the model. Was there a question? Yeah, what about the gradient in respect to w? &#39;Cause that&#39;s ultimately the thing that we care about. So, the gradient with respect to w will come through, at every time step, will take our current cell state as well as our current hidden state and that will give us an element, that will give us our local gradient on w for that time step. So because our cell state, and just in the vanilla RNN case, we&#39;ll end up adding those first time step w gradients to compute our final gradient on w. But now, if you imagine the situation where we have a very long sequence, and we&#39;re only getting gradients to the very end of the sequence. Now, as you backpropagate through, we&#39;ll get a local gradient on w for each time step, and that local gradient on w will be coming through these gradients on c and h. So because we&#39;re maintaining the gradients on c much more nicely in the LSTM case, those local gradients on w at each time step will also be carried forward and backward through time much more cleanly. Another question? Yeah, so the question is due to the non linearities, could this still be susceptible to vanishing gradients? And that could be the case. Actually, so one problem you might imagine is that maybe if these forget gates are always less than zero, or always less than one, you might get vanishing gradients as you continually go through these forget gates. Well, one sort of trick that people do in practice is that they will, sometimes, initialize the biases of the forget gate to be somewhat positive. So that at the beginning of training, those forget gates are always very close to one. So that at least at the beginning of training, then we have not so, relatively clean gradient flow through these forget gates, since they&#39;re all initialized to be near one. And then throughout the course of training, then the model can learn those biases and kind of learn to forget where it needs to. You&#39;re right that there still could be some potential for vanishing gradients here. But it&#39;s much less extreme than the vanilla RNN case, both because those fs can vary at each time step, and also because we&#39;re doing this element wise multiplication rather than a full matrix multiplication. So you can see that this LSTM actually looks quite similar to ResNet. In this residual network, we had this path of identity connections going backward through the network and that gave, sort of a gradient super highway for gradients to flow backward in ResNet. And now it&#39;s kind of the same intuition in LSTM where these additive and element wise multiplicative interactions of the cell state can give a similar gradient super highway for gradients to flow backwards through the cell state in an LSTM. </span></li></ul><h3 class="c0" id="h.ud6tlxev5yut"><span>&ldquo;Highway Networks&rdquo; (Srivastava 2015)</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>And by the way, there&#39;s this other kind of nice </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1505.00387.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108800328&amp;usg=AOvVaw3MATCO89VNQwDav7Cp7D60">paper</a></span><span>&nbsp;called </span><span class="c3 c12">highway networks</span><span>, which is kind of </span><span class="c3">in between this idea of this LSTM cell and these residual networks</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span class="c1">So these highway networks actually came before residual networks, and they had this idea where at every layer of the highway network, we&#39;re going to compute sort of a candidate activation, as well as a gating function that tells us that interprelates between our previous input at that layer, and that candidate activation that came through our convolutions or what not. So there&#39;s actually a lot of architectural similarities between these things, and people take a lot of inspiration from training very deep CNNs and very deep RNNs and there&#39;s a lot of crossover here. </span></li></ul><h3 class="c0" id="h.fw0nqo5ht9eu"><span>GRU</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>Very briefly, you&#39;ll see a lot of </span><span class="c3">other types of variants of recurrent neural network</span><span>&nbsp;architectures out there in the wild. Probably the most common, apart from the LSTM, is this </span><span class="c3 c12">GRU</span><span>, called the </span><span class="c3 c12">gated recurrent unit</span><span class="c1">. </span></li><li class="c10 c15 li-bullet-0"><span>And you can see those update equations here, and it kind of has this </span><span class="c3">similar flavor of the LSTM</span><span class="c1">, where it uses these multiplicative element wise gates together with these additive interactions to avoid this vanishing gradient problem. </span></li></ul><h3 class="c0" id="h.60uuk8qzz8y2"><span class="c21">&ldquo;LSTM: a search space odyssey&rdquo; (Greff, Srivastava 2017)</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>There&#39;s also this cool </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1503.04069.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108802738&amp;usg=AOvVaw3ZkD7D4VWnglkicj-spDdg">paper</a></span><span class="c1">&nbsp;called &ldquo;LSTM: a search space odyssey&rdquo;, very inventive title, </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span class="c1">where they tried to play around with the LSTM equations and swap out the non linearities at one point, like do we really need that tanh for exposing the output gate, </span></li><li class="c5 li-bullet-0"><span class="c1">and they tried to answer a lot of these different questions about each of those non linearities, each of those pieces of the LSTM update equations. What happens if we change the model and tweak those LSTM equations a little bit. </span></li><li class="c5 li-bullet-0"><span>And kind of the </span><span class="c3">conclusion</span><span class="c1">&nbsp;is that </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-2 start"><li class="c10 c27 li-bullet-0"><span class="c1">they all work about the same. Some of them work a little bit better than others for one problem or another. </span></li><li class="c10 c27 li-bullet-0"><span>But generally, none of the things, </span><span class="c3">none of the tweaks of LSTM that they tried were significantly better than the original LSTM for all problems</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span class="c1">So that gives you a little bit more faith that the LSTM update equations seem kind of magical but they&#39;re useful anyway. You should probably consider them for your problem. </span></li></ul><h3 class="c0" id="h.5tth4ca345bv"><span>&ldquo;</span><span>An Empirical Exploration of Recurrent Network Architectures&rdquo; (Google, Sutskever </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://proceedings.mlr.press/v37/jozefowicz15.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108805094&amp;usg=AOvVaw1zrDt-os7gIUH5wnG7hACP">paper</a></span><span class="c21">&nbsp;2015)</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>There&#39;s also this cool </span><span class="c22"><a class="c11" href="https://www.google.com/url?q=https://proceedings.mlr.press/v37/jozefowicz15.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108805784&amp;usg=AOvVaw27c-JrpLUbqIw_vYiL-t-K">paper</a></span><span>&nbsp;from Google a couple years ago where they tried to use, where they did kind of an </span><span class="c3">evolutionary search</span><span class="c1">&nbsp;and did a search over many, over a very large number of random RNN architectures [10000], they kind of randomly permute these update equations and try putting the additions and the multiplications and the gates and the non linearities in different kinds of combinations. They blasted this out over their huge Google cluster and just tried a whole bunch of these different weight updates in various flavors. </span></li><li class="c10 c15 li-bullet-0"><span>And again, it was the same story that </span><span class="c3">they didn&#39;t really find anything that was significantly better than these existing GRU or LSTM styles</span><span class="c1">. </span></li></ul><ul class="c2 lst-kix_btugr32bfvpf-1 start"><li class="c5 li-bullet-0"><span>Although there were </span><span class="c3">some variations that worked maybe slightly better</span><span>&nbsp;or worse </span><span class="c3">for certain problems</span><span class="c1">. </span></li></ul><h3 class="c0" id="h.kvgr86oi80l5"><span>Summary LSTM, GRU</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>But kind of the </span><span class="c3">takeaway</span><span>&nbsp;is that there is not so much magic in those LSTM or GRU equations, but this idea of </span><span class="c3">managing gradient flow properly through these additive connections and these multiplicative gates is super useful</span><span class="c1">. </span></li></ul><h3 class="c0" id="h.9akhfzyskzao"><span>Summary</span></h3><ul class="c2 lst-kix_btugr32bfvpf-0"><li class="c10 c15 li-bullet-0"><span>So yeah, the summary is that RNNs are super cool. They can allow you to attack tons of new types of problems. They sometimes are susceptible to vanishing or exploding gradients. But we can address that with weight clipping and with fancier architectures. And there&#39;s a lot of cool overlap between CNN architectures and RNN architectures. </span></li></ul><hr class="c56"><div><p class="c16"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c19">&nbsp;aus RNN lecture sp&auml;ter, aber passt hier besser hin</span></p></div><div><p class="c16"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c28">&nbsp;PASCAL dataset ist nur &lt;10GB, aber wenn </span><span class="c3 c28">die berechneten features der 2000 crops / image</span><span class="c28">&nbsp;gespeichert werden, werden ~200GB ben&ouml;tigt (s. </span><span class="c22 c28"><a class="c11" href="#id.fnzd99e9kkj2">slide</a></span><span class="c19">) ! &nbsp;</span></p></div><div><p class="c16"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c28">&nbsp;post-hoc hei&szlig;t: &ldquo;CNN features </span><span class="c3 c28">not updated in response to SVMs</span><span class="c28">&nbsp;and regressors&rdquo; (s. </span><span class="c22 c28"><a class="c11" href="#id.62hwt07x920b">slide</a></span><span class="c28">), Johnson 2016: &ldquo;SVMs and regressors are </span><span class="c3 c28">trained sort of offline</span><span class="c19">&nbsp;using libsvm [sklearn&rsquo;s .SVC is based on this code] and LinearRegression. So, the weights of the CNN did not have a chance to update in response to what those objectives [of the SVMs] wanted to do&rdquo;</span></p></div><div><p class="c16"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c19">&nbsp;es f&uuml;r jede class ein separates binary SVM (e.g. &ldquo;Katze/keine Katze&rdquo;), weil &ldquo;sometimes you might wanna have one region have multiple positives, be able to output &ldquo;yes&rdquo; on multiple classes for the same image region. And one way they do that is by training separate binary SVMs for each class&rdquo; Johnson 2016 CS231n</span></p></div><div><p class="c16"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c28">&nbsp;woher wei&szlig; das zu trainierende binary &ldquo;cat&rdquo; SVM, ob ein bestimmtes region feature ein positive oder negative feature ist? In anderen Worten, welche der (berechneten) features sind positive und welche negative ? - Die &ldquo;positve samples (for cat) SVM&rdquo; sind die </span><span class="c3 c28">berechneten features der in den training images (f&uuml;r Katzen) positiv gelabelten ROIs</span><span class="c19">.</span></p></div><div><p class="c16"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c28">&nbsp;s. </span><span class="c25 c75"><a class="c11" href="https://www.google.com/url?q=https://youtu.be/GxZrEKZfW2o?t%3D2587&amp;sa=D&amp;source=editors&amp;ust=1642006108810021&amp;usg=AOvVaw05CHvY6KwHhk9rL6cHPBUO">CS231n Winter 2016: Lecture 8: Localization and Detection</a></span><span class="c19">&nbsp;zu ROI pooling</span></p></div><div><p class="c16"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c28">&nbsp;s. </span><span class="c22 c28"><a class="c11" href="#id.5jaq8zvvgan2">slide</a></span><span class="c28">&nbsp;geschweifte Klammer mit &ldquo;Trainable&rdquo;</span></p></div><div><p class="c16"><a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c28">&nbsp;The confidence score indicates 1. how sure the model is that the box contains an object [= Pr(object)] </span><span class="c3 c28">and also</span><span class="c19">&nbsp;2. how accurate it thinks the box is that [the model] predicts [= IoU]. The confidence score can be calculated using the formula:</span></p><p class="c47"><span class="c33 c28">C = Pr(object) * IoU &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c28">[Formel stimmt s. </span><span class="c22 c28"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.05511.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108811088&amp;usg=AOvVaw23Ui1op3mng9E-JUEb0LSf">paper</a></span><span class="c19">]</span></p><p class="c47"><span class="c19">IoU: Intersection over Union between the predicted box and the ground truth.</span></p><p class="c47"><span class="c19">If no object exists in a cell, its confidence score should be zero.</span></p><p class="c47"><span class="c28">(</span><span class="c22 c28"><a class="c11" href="https://www.google.com/url?q=https://towardsdatascience.com/iou-a-better-detection-evaluation-metric-45a511185be1&amp;sa=D&amp;source=editors&amp;ust=1642006108811843&amp;usg=AOvVaw0F3zBzGXN4bOnKByA8u0w8">source</a></span><span class="c19">)</span></p></div><div><p class="c16"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c28">&nbsp;a certain confidence (or only the IoU) will be used as threshold for &ldquo;Non-max suppression&rdquo; (see </span><span class="c22 c28"><a class="c11" href="https://www.google.com/url?q=https://towardsdatascience.com/object-detection-part1-4dbe5147ad0a&amp;sa=D&amp;source=editors&amp;ust=1642006108812390&amp;usg=AOvVaw0LiaZ4noMvlKoTnc_O9Wrm">here</a></span><span class="c19">) </span></p></div><div><p class="c16"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c28">&nbsp;bezogen auf: &ldquo;Given the VGG16 backbone architecture, SSD adds several feature layers to the end of the network, which are responsible for </span><span class="c3 c28">predicting the offsets to default boxes</span><span class="c28">&nbsp;with different scales and aspect ratios&rdquo; </span><span class="c28">[YOLO predicted offsets auch, aber benutzt einen grid am Anfang: &ldquo;Each grid cell predicts B bounding boxes and their corresponding confidence scores.&rdquo;] aus </span><span class="c22 c28"><a class="c11" href="https://www.google.com/url?q=https://arxiv.org/pdf/1807.05511.pdf&amp;sa=D&amp;source=editors&amp;ust=1642006108813044&amp;usg=AOvVaw19mb6bIlMQBUfuFoiWZLT8">paper</a></span></p></div><div><p class="c16"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c19">&nbsp;similar to faster R-CNN</span></p></div><div><p class="c16"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c28">&nbsp;In all experiments we choose B, P, and K in such a way that 3B [</span><span class="c25 c3 c28">classical</span><span class="c28">&nbsp;batch size] is close to PK [</span><span class="c25 c3 c28">Leibe</span><span class="c19">&nbsp;batch size], e.g. 3 &middot;42 &asymp;32 &middot;4.</span></p></div><div><p class="c16"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c19">&nbsp;Because PK&asymp;3B, see footnote 11</span></p></div><div><p class="c16"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c19">&nbsp;Ba, Mnih 2015, &ldquo;MULTIPLE OBJECT RECOGNITION WITH VISUAL ATTENTION&rdquo;</span></p></div><div><p class="c16"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span class="c28">&nbsp;see paragraph &ldquo;Glimpse Sensor&rdquo; </span><span class="c22 c28"><a class="c11" href="https://www.google.com/url?q=https://towardsdatascience.com/visual-attention-model-in-deep-learning-708813c2912c&amp;sa=D&amp;source=editors&amp;ust=1642006108814562&amp;usg=AOvVaw0rvYTWIaaJNPsjU5iYPIzc">here</a></span><span class="c28">: &ldquo;</span><span class="c33 c28">The idea is to allow our network to &ldquo;take a glance&rdquo; at the image around a given location, called a glimpse, then extract and resize this glimpse into various scales of image crops, but each scale is using the same resolution. For example, the glimpse in the above example contains 3 different scales, each scale has the same resolution (a.k.a. sensor bandwidth), e.g. 12x12. Therefore, the smallest scale of crop in the centre is most detailed, whereas the largest crop in the outer ring is most blurred. In summary, Glimpse Sensor takes a full-sized image and a location, outputs the &ldquo;Retina-like&rdquo; representation of the image around the given location.</span><span class="c28">&rdquo;</span></p></div><div><p class="c16"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span class="c19">&nbsp;Gregor 2015, &ldquo;DRAW: A Recurrent Neural Network For Image Generation&rdquo;</span></p></div><div><p class="c16"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span class="c19">&nbsp;Karpathy, Johnson, Fei-Fei - VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS</span></p></div><div><p class="c16"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span class="c19">&nbsp;s. slide oben dr&uuml;ber</span></p></div></body></html>